{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel  # pip install peft\n",
    "from LatentCOMP_cleaned.infer_lopa_pure import lopa_generate, ensure_mistral_special_token, _get_inner_model\n",
    "\n",
    "repo_id = \"jeongseokoh/Llama-3.1-8B-Instruct-LOPA-partial4-0specials\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if (device==\"cuda\" and torch.cuda.is_bf16_supported()) else (torch.float16 if device==\"cuda\" else torch.float32)\n",
    "\n",
    "# Tokenizer (saved at repo root)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Base (under subfolder=base)\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id, subfolder=\"base\", trust_remote_code=False, torch_dtype=dtype)\n",
    "ensure_mistral_special_token(tokenizer, model)\n",
    "\n",
    "# Merge LoRA if present (under subfolder=lora)\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, repo_id, subfolder=\"lora\").merge_and_unload()\n",
    "except Exception as e:\n",
    "    print(f\"[warn] LoRA merge failed or missing, using base only: {e}\")\n",
    "\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Force eager attention (stability)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try:\n",
    "        setattr(model.config, k, \"eager\")\n",
    "        setattr(_get_inner_model(model).config, k, \"eager\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Fill these\n",
    "system = \"You are a helpful assistant that answers questions based on the given document. \"\n",
    "document = \"Replace with your full document text here.\"\n",
    "question = \"Replace with your question here.\"\n",
    "K = 4  # same as training (partial4)\n",
    "\n",
    "# Generate with LoPA\n",
    "text = lopa_generate(\n",
    "    model, tokenizer,\n",
    "    system=system, document=document, question=question,\n",
    "    K=K, device=device,\n",
    "    max_new_tokens=256, min_length=16,\n",
    "    temperature=0.7, top_p=0.9, top_k=None,\n",
    "    do_sample=True, debug=True,\n",
    ")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edcbf962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Transformers version: 4.56.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be99633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f61e08d60147ad9d30d2f3b7ccdcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 20:39:19,291] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::substr(unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_stringbuf_init(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_dispose(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::swap(std::__detail::_List_node_base&, std::__detail::_List_node_base&)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_mutate(unsigned long, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_next_bkt(unsigned long) const@GLIBCXX_3.4.18'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::end()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::flush<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::string const&, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::runtime_error(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(std::string const&) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 20:39:20,573] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "Rewrote clean base to: LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/base\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# 1) 경로와 원본 모델 ID 지정\n",
    "best_dir = \"LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\"\n",
    "base_dir = f\"{best_dir}/base\"\n",
    "orig_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # 학습에 쓴 원본 모델 이름\n",
    "\n",
    "# 2) 원본 베이스 모델 로드(캐시 사용), 저장\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    orig_model_id,\n",
    "    trust_remote_code=True,          # Llama3 계열이면 True 괜찮습니다\n",
    "    cache_dir=\"/data2/jeongseokoh/hub/model\"  # 학습 때 쓰던 캐시 경로가 있으면 지정\n",
    ")\n",
    "# base/config.json에 remote code 흔적이 섞이지 않도록 auto_map 클리어(있다면)\n",
    "try:\n",
    "    setattr(m.config, \"auto_map\", None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "m.save_pretrained(base_dir, safe_serialization=True)\n",
    "del m\n",
    "\n",
    "print(\"Rewrote clean base to:\", base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c22a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26170ccbacaf4356b8fe1991f271731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready to help. What would you like to know? Please go ahead and ask your question.\n"
     ]
    }
   ],
   "source": [
    "# LoPA (low-only prefill) scratch pipeline without trust_remote_code\n",
    "# Torch 2.5.1, Transformers 4.56.1, GPU+SDPA\n",
    "# - Phase1: lower-K only prefill (no assistant header)\n",
    "# - Phase2: generation (first feed assistant header tokens step-by-step)\n",
    "# - Positions unchanged; no remapping\n",
    "# - LoRA: best/lora attach → merge\n",
    "# - MinLength 수동 적용(4.56.1 디바이스 불일치 회피)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "K = 4  # prefill layers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (device == \"cuda\" and torch.cuda.is_bf16_supported())\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "cache_dir_model = \"/data2/jeongseokoh/hub/model\"\n",
    "cache_dir_tok   = \"/data2/jeongseokoh/hub/tokenizer\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (best_dir 우선)\n",
    "# -----------------------------\n",
    "tok_src = str(best_dir) if (best_dir / \"tokenizer.json\").is_file() else backbone_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir_tok, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone + LoRA (merge)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir=cache_dir_model,\n",
    ")\n",
    "base_model.to(device).eval()\n",
    "\n",
    "from peft import PeftModel\n",
    "assert lora_dir.is_dir(), f\"LoRA folder not found: {lora_dir}\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "model = peft_model.merge_and_unload().to(device).eval()\n",
    "del peft_model\n",
    "\n",
    "# SDPA 고정 (필요시 eager로 비교)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try: setattr(model.config, k, \"sdpa\")\n",
    "    except: pass\n",
    "try:\n",
    "    for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "        setattr(model.model.config, k, \"sdpa\")\n",
    "except: pass\n",
    "\n",
    "# -----------------------------\n",
    "# Cache helpers\n",
    "# -----------------------------\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    try: return len(pkv)\n",
    "    except: return 0\n",
    "\n",
    "def pkv_get(pkv, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)  # layer index 유지\n",
    "    return dc\n",
    "\n",
    "def cache_slice_doc_only(k: torch.Tensor, v: torch.Tensor, doc_len: int):\n",
    "    return k[:, :, -doc_len:, :], v[:, :, -doc_len:, :]\n",
    "\n",
    "def build_mask(length: int, batch: int = 1):\n",
    "    return torch.ones(batch, length, device=device, dtype=torch.long)\n",
    "\n",
    "def build_pos_ids(start: int, length: int, batch: int = 1):\n",
    "    return torch.arange(start, start + length, device=device, dtype=torch.long).unsqueeze(0).expand(batch, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt builders (Gen3)\n",
    "# -----------------------------\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    s = apply_chat_template(messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# LoPA generate (low-only prefill; no upper continue)\n",
    "# -----------------------------\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    system: str,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    K: int = 4,\n",
    "    max_new_tokens: int = 256,\n",
    "    min_length: int = 16,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: Optional[int] = None,\n",
    "    do_sample: bool = True,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    start_with_assistant_header: bool = True, # Phase2 첫 토큰: header를 단계적으로 투입\n",
    "    force_attn_impl: Optional[str] = None,   # \"sdpa\" | \"eager\"\n",
    "):\n",
    "    if force_attn_impl:\n",
    "        for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "            try:\n",
    "                setattr(model.config, k, force_attn_impl)\n",
    "                setattr(model.model.config, k, force_attn_impl)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Phase-1 ids\n",
    "    msgs = build_messages(system, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(msgs, add_generation_prompt=False)  # [1, L_sys+doc]\n",
    "    ids_hdr    = tokens_from_messages(msgs, add_generation_prompt=True)   # [1, L_sys+doc + header]\n",
    "    sys_only   = tokens_from_messages([{\"role\":\"system\",\"content\":system}], add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Phase-1 must include document tokens.\"\n",
    "\n",
    "    # 1) System-only prefill (base model)\n",
    "    out_sys = model.model(\n",
    "        input_ids=sys_only,\n",
    "        attention_mask=build_mask(L_sys),\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    n_layers = pkv_len(pkv_sys)\n",
    "\n",
    "    # 2) Lower-K doc pass (base model) — no upper continue\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "    full_layers: nn.ModuleList = model.model.layers\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "\n",
    "    model.model.layers = lower_layers\n",
    "    dc_low_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    attn_doc_full = torch.cat([build_mask(L_sys), build_mask(L_doc)], dim=1)\n",
    "\n",
    "    out_low = model.model(\n",
    "        input_ids=ids_phase1[:, L_sys:],  # doc only\n",
    "        past_key_values=dc_low_in,\n",
    "        attention_mask=attn_doc_full,     # sys+doc 길이\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # 복원\n",
    "    model.model.layers = full_layers\n",
    "\n",
    "    # 3) Combine caches\n",
    "    # lower(0..K-1): sys + doc\n",
    "    # upper(K..):    sys only  (doc 미포함; generation 시작 시점부터 쌓임)\n",
    "    combined = DynamicCache()\n",
    "    # 헤드/차원 shape 확보\n",
    "    k0_sys, v0_sys = pkv_get(pkv_sys, 0)\n",
    "    for li in range(n_layers):\n",
    "        k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "        k_sys = k_sys[:, :, :L_sys, :]\n",
    "        v_sys = v_sys[:, :, :L_sys, :]\n",
    "        if li < K_eff:\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_doc, v_doc = cache_slice_doc_only(k_low, v_low, L_doc)\n",
    "            k_cat = torch.cat([k_sys, k_doc], dim=2).contiguous()\n",
    "            v_cat = torch.cat([v_sys, v_doc], dim=2).contiguous()\n",
    "            combined.update(k_cat, v_cat, li)\n",
    "        else:\n",
    "            # upper: sys only\n",
    "            combined.update(k_sys.contiguous(), v_sys.contiguous(), li)\n",
    "\n",
    "    # 4) Phase-2: Generation\n",
    "    # seed: assistant header를 한 토큰씩 밀어 넣어 upper past가 L_sys → L_sys+H로 자라도록 함\n",
    "    if start_with_assistant_header:\n",
    "        hdr_tail = ids_hdr[:, L_all:]  # header-only tokens (len H)\n",
    "        H = int(hdr_tail.size(1))\n",
    "        for j in range(H):\n",
    "            past_len = pkv_get(combined, 0)[0].shape[2]  # lower 기준: L_sys+L_doc+grown\n",
    "            attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "            step_tok = hdr_tail[:, j:j+1]\n",
    "            out_seed = model(\n",
    "                input_ids=step_tok,\n",
    "                past_key_values=combined,\n",
    "                attention_mask=attn_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            combined = out_seed.past_key_values\n",
    "\n",
    "    # 5) Decoding (CausalLM) with safe processors\n",
    "    from transformers.generation import LogitsProcessorList\n",
    "    from transformers.generation.logits_process import (\n",
    "        TemperatureLogitsWarper, TopPLogitsWarper, TopKLogitsWarper,\n",
    "        RepetitionPenaltyLogitsProcessor,\n",
    "    )\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    generated_ids = torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    # 첫 생성 스텝의 입력은 \"직전 토큰\" (header 마지막 또는 마지막 doc)\n",
    "    last = (ids_hdr[:, -1:] if start_with_assistant_header else ids_phase1[:, -1:])\n",
    "\n",
    "    processors = LogitsProcessorList()\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        processors.append(RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty)))\n",
    "    if temperature and temperature != 1.0:\n",
    "        processors.append(TemperatureLogitsWarper(temperature=float(temperature)))\n",
    "    if top_k is not None and top_k > 0:\n",
    "        processors.append(TopKLogitsWarper(top_k=int(top_k), filter_value=-float(\"inf\")))\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        processors.append(TopPLogitsWarper(top_p=float(top_p), min_tokens_to_keep=1))\n",
    "\n",
    "    cur = 0\n",
    "    while cur < max_new_tokens:\n",
    "        # lower past_len은 L_sys+L_doc + grown; upper past_len은 L_sys + grown\n",
    "        past_len = pkv_get(combined, 0)[0].shape[2]\n",
    "        attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=last,\n",
    "            past_key_values=combined,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        combined = out.past_key_values\n",
    "        logits = out.logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "        # 즉시 EOS 방지 (min_length 수동 적용)\n",
    "        if eos_id is not None and cur < min_length:\n",
    "            logits[:, eos_id] = -float(\"inf\")\n",
    "\n",
    "        if not torch.isfinite(logits).all():\n",
    "            logits = torch.nan_to_num(logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        inp = generated_ids if generated_ids.numel() > 0 else last.new_zeros((1, 0), dtype=torch.long, device=device)\n",
    "        inp = inp.to(logits.device)\n",
    "        logits = processors(inp, logits)\n",
    "\n",
    "        # Fallback-safe sampling\n",
    "        if not torch.isfinite(logits).any():\n",
    "            next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if do_sample:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                if (not torch.isfinite(probs).all()) or (probs.sum(dim=-1) <= 0).any():\n",
    "                    next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if eos_id is not None and int(next_tok.item()) == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_tok], dim=1)\n",
    "        last = next_tok\n",
    "        cur += 1\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "answer = lopa_generate(\n",
    "    system, document, question,\n",
    "    K=K,\n",
    "    max_new_tokens=256,\n",
    "    min_length=16,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    start_with_assistant_header=True,  # Phase2 처음에 header를 단계적으로 투입\n",
    "    # force_attn_impl=\"eager\",         # 필요시 비교\n",
    ")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ff3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcead6b3d56471c87aadf12086bbd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lens] L_sys=40, L_doc=35, L_all=75, header_len=4\n",
      "[Layers] n_layers=32, K_eff=4\n",
      "[sys] n_layers=32 | L0:K(1, 8, 40, 128) | L1:K(1, 8, 40, 128) | L31:K(1, 8, 40, 128)\n",
      "[OK]   system_prefill: all present layers seq_len == 40\n",
      "[low] h_low=(1, 35, 4096)\n",
      "[low] n_layers=4 | L0:K(1, 8, 75, 128) | L1:K(1, 8, 75, 128) | L3:K(1, 8, 75, 128)\n",
      "[up] n_layers=32 | L4:K(1, 8, 75, 128) | L5:K(1, 8, 75, 128) | L31:K(1, 8, 75, 128)\n",
      "[combined(before_hdr)] n_layers=32 | L0:K(1, 8, 75, 128) | L1:K(1, 8, 75, 128) | L31:K(1, 8, 75, 128)\n",
      "[OK]   combined_prefill: all present layers seq_len == 75\n",
      "[hdr] past_len before=75, after=79, delta=4\n",
      "[combined(after_hdr)] n_layers=32 | L0:K(1, 8, 79, 128) | L1:K(1, 8, 79, 128) | L31:K(1, 8, 79, 128)\n",
      "[decode step 0] past_len 79 -> 80 | next_id=40\n",
      "[decode step 1] past_len 80 -> 81 | next_id=2846\n",
      "[decode step 2] past_len 81 -> 82 | next_id=539\n",
      "[debug answer] I'm not able to answer your question as I don't have any information about a document. Can you please provide the document or more context about the question you are trying to ask?\n",
      "I'm not able to answer your question since you didn't provide a document. Please provide the document, and I'll be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "# LoPA (low-only prefill) scratch pipeline without trust_remote_code\n",
    "# Torch 2.5.1, Transformers 4.56.1, GPU+SDPA\n",
    "# - Phase1: lower-K only prefill (no assistant header)\n",
    "# - Phase2: generation (first feed assistant header tokens step-by-step)\n",
    "# - Positions unchanged; no remapping\n",
    "# - LoRA: best/lora attach → merge\n",
    "# - MinLength 수동 적용(4.56.1 디바이스 불일치 회피)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "K = 4  # prefill layers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (device == \"cuda\" and torch.cuda.is_bf16_supported())\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "cache_dir_model = \"/data2/jeongseokoh/hub/model\"\n",
    "cache_dir_tok   = \"/data2/jeongseokoh/hub/tokenizer\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (best_dir 우선)\n",
    "# -----------------------------\n",
    "tok_src = str(best_dir) if (best_dir / \"tokenizer.json\").is_file() else backbone_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir_tok, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone + LoRA (merge)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir=cache_dir_model,\n",
    ")\n",
    "base_model.to(device).eval()\n",
    "\n",
    "from peft import PeftModel\n",
    "assert lora_dir.is_dir(), f\"LoRA folder not found: {lora_dir}\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "model = peft_model.merge_and_unload().to(device).eval()\n",
    "del peft_model\n",
    "\n",
    "# SDPA 고정 (필요시 eager로 비교)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try: setattr(model.config, k, \"sdpa\")\n",
    "    except: pass\n",
    "try:\n",
    "    for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "        setattr(model.model.config, k, \"sdpa\")\n",
    "except: pass\n",
    "\n",
    "# -----------------------------\n",
    "# Cache helpers\n",
    "# -----------------------------\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    try: return len(pkv)\n",
    "    except: return 0\n",
    "\n",
    "def pkv_get(pkv, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)  # layer index 유지\n",
    "    return dc\n",
    "\n",
    "def cache_slice_doc_only(k: torch.Tensor, v: torch.Tensor, doc_len: int):\n",
    "    return k[:, :, -doc_len:, :], v[:, :, -doc_len:, :]\n",
    "\n",
    "def build_mask(length: int, batch: int = 1):\n",
    "    return torch.ones(batch, length, device=device, dtype=torch.long)\n",
    "\n",
    "def build_pos_ids(start: int, length: int, batch: int = 1):\n",
    "    return torch.arange(start, start + length, device=device, dtype=torch.long).unsqueeze(0).expand(batch, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt builders (Gen3)\n",
    "# -----------------------------\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    s = apply_chat_template(messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# LoPA generate (low-only prefill; no upper continue)\n",
    "# -----------------------------\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    system: str,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    K: int = 4,\n",
    "    max_new_tokens: int = 256,\n",
    "    min_length: int = 16,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: Optional[int] = None,\n",
    "    do_sample: bool = True,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    start_with_assistant_header: bool = True, # Phase2 첫 토큰: header를 단계적으로 투입\n",
    "    force_attn_impl: Optional[str] = None,   # \"sdpa\" | \"eager\"\n",
    "):\n",
    "    if force_attn_impl:\n",
    "        for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "            try:\n",
    "                setattr(model.config, k, force_attn_impl)\n",
    "                setattr(model.model.config, k, force_attn_impl)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Phase-1 ids\n",
    "    msgs = build_messages(system, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(msgs, add_generation_prompt=False)  # [1, L_sys+doc]\n",
    "    ids_hdr    = tokens_from_messages(msgs, add_generation_prompt=True)   # [1, L_sys+doc + header]\n",
    "    sys_only   = tokens_from_messages([{\"role\":\"system\",\"content\":system}], add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Phase-1 must include document tokens.\"\n",
    "\n",
    "    # 1) System-only prefill (base model)\n",
    "    out_sys = model.model(\n",
    "        input_ids=sys_only,\n",
    "        attention_mask=build_mask(L_sys),\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    n_layers = pkv_len(pkv_sys)\n",
    "\n",
    "    # 2) Lower-K doc pass (base model) — no upper continue\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "    full_layers: nn.ModuleList = model.model.layers\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "\n",
    "    model.model.layers = lower_layers\n",
    "    dc_low_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    attn_doc_full = torch.cat([build_mask(L_sys), build_mask(L_doc)], dim=1)\n",
    "\n",
    "    out_low = model.model(\n",
    "        input_ids=ids_phase1[:, L_sys:],  # doc only\n",
    "        past_key_values=dc_low_in,\n",
    "        attention_mask=attn_doc_full,     # sys+doc 길이\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # 복원\n",
    "    model.model.layers = full_layers\n",
    "\n",
    "    # 3) Combine caches\n",
    "    # lower(0..K-1): sys + doc\n",
    "    # upper(K..):    sys only  (doc 미포함; generation 시작 시점부터 쌓임)\n",
    "    combined = DynamicCache()\n",
    "    # 헤드/차원 shape 확보\n",
    "    k0_sys, v0_sys = pkv_get(pkv_sys, 0)\n",
    "    for li in range(n_layers):\n",
    "        k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "        k_sys = k_sys[:, :, :L_sys, :]\n",
    "        v_sys = v_sys[:, :, :L_sys, :]\n",
    "        if li < K_eff:\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_doc, v_doc = cache_slice_doc_only(k_low, v_low, L_doc)\n",
    "            k_cat = torch.cat([k_sys, k_doc], dim=2).contiguous()\n",
    "            v_cat = torch.cat([v_sys, v_doc], dim=2).contiguous()\n",
    "            combined.update(k_cat, v_cat, li)\n",
    "        else:\n",
    "            # upper: sys only\n",
    "            combined.update(k_sys.contiguous(), v_sys.contiguous(), li)\n",
    "\n",
    "    # 4) Phase-2: Generation\n",
    "    # seed: assistant header를 한 토큰씩 밀어 넣어 upper past가 L_sys → L_sys+H로 자라도록 함\n",
    "    if start_with_assistant_header:\n",
    "        hdr_tail = ids_hdr[:, L_all:]  # header-only tokens (len H)\n",
    "        H = int(hdr_tail.size(1))\n",
    "        for j in range(H):\n",
    "            past_len = pkv_get(combined, 0)[0].shape[2]  # lower 기준: L_sys+L_doc+grown\n",
    "            attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "            step_tok = hdr_tail[:, j:j+1]\n",
    "            out_seed = model(\n",
    "                input_ids=step_tok,\n",
    "                past_key_values=combined,\n",
    "                attention_mask=attn_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            combined = out_seed.past_key_values\n",
    "\n",
    "    # 5) Decoding (CausalLM) with safe processors\n",
    "    from transformers.generation import LogitsProcessorList\n",
    "    from transformers.generation.logits_process import (\n",
    "        TemperatureLogitsWarper, TopPLogitsWarper, TopKLogitsWarper,\n",
    "        RepetitionPenaltyLogitsProcessor,\n",
    "    )\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    generated_ids = torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    # 첫 생성 스텝의 입력은 \"직전 토큰\" (header 마지막 또는 마지막 doc)\n",
    "    last = (ids_hdr[:, -1:] if start_with_assistant_header else ids_phase1[:, -1:])\n",
    "\n",
    "    processors = LogitsProcessorList()\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        processors.append(RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty)))\n",
    "    if temperature and temperature != 1.0:\n",
    "        processors.append(TemperatureLogitsWarper(temperature=float(temperature)))\n",
    "    if top_k is not None and top_k > 0:\n",
    "        processors.append(TopKLogitsWarper(top_k=int(top_k), filter_value=-float(\"inf\")))\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        processors.append(TopPLogitsWarper(top_p=float(top_p), min_tokens_to_keep=1))\n",
    "\n",
    "    cur = 0\n",
    "    while cur < max_new_tokens:\n",
    "        # lower past_len은 L_sys+L_doc + grown; upper past_len은 L_sys + grown\n",
    "        past_len = pkv_get(combined, 0)[0].shape[2]\n",
    "        attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=last,\n",
    "            past_key_values=combined,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        combined = out.past_key_values\n",
    "        logits = out.logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "        # 즉시 EOS 방지 (min_length 수동 적용)\n",
    "        if eos_id is not None and cur < min_length:\n",
    "            logits[:, eos_id] = -float(\"inf\")\n",
    "\n",
    "        if not torch.isfinite(logits).all():\n",
    "            logits = torch.nan_to_num(logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        inp = generated_ids if generated_ids.numel() > 0 else last.new_zeros((1, 0), dtype=torch.long, device=device)\n",
    "        inp = inp.to(logits.device)\n",
    "        logits = processors(inp, logits)\n",
    "\n",
    "        # Fallback-safe sampling\n",
    "        if not torch.isfinite(logits).any():\n",
    "            next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if do_sample:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                if (not torch.isfinite(probs).all()) or (probs.sum(dim=-1) <= 0).any():\n",
    "                    next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if eos_id is not None and int(next_tok.item()) == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_tok], dim=1)\n",
    "        last = next_tok\n",
    "        cur += 1\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "answer = lopa_generate(\n",
    "    system, document, question,\n",
    "    K=K,\n",
    "    max_new_tokens=256,\n",
    "    min_length=16,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    start_with_assistant_header=True,  # Phase2 처음에 header를 단계적으로 투입\n",
    "    # force_attn_impl=\"eager\",         # 필요시 비교\n",
    ")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7acf95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1346d627656d4138b818cc49145abfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant that answers questions based on the given document.user\\n\\nDocument:\\nThe Nile is the longest river in Africa...\\n\\nQuestion: Which continent is the Nile the longest river in?assistant\\n\\nThe Nile is the longest river in Africa.']\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_dir = \"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\"\n",
    "prefill_layers = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sys.path.insert(0, model_dir)  # best/ 를 import 경로에 추가\n",
    "from modeling_partial_layer import LlamaForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir, device_map=\"cuda:0\", dtype=torch.bfloat16, trust_remote_code=True).to(device).eval()\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    setattr(model.config, k, \"sdpa\")\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "out = model.generate(\n",
    "    system=system, document=document, query=question,\n",
    "    compress=False, tokenizer=tok, prefill_layers=prefill_layers,\n",
    "    max_new_tokens=256, do_sample=False, temperature=0.7, top_p=0.9\n",
    ")\n",
    "print(tok.batch_decode(out, skip_special_tokens=True))  # 문자열 또는 문자열 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786d4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_available=True | attached=True | merged=False | is_sharded=True | source=/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best/lora | error=None | last_used_lora=True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LATENTRAG_DEBUG\"]=\"1\"\n",
    "print(model.lora_debug_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89bed7b41e444fab4a660167f1660c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da2aecd742744a68944483b961a633b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant<|end_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Africa.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 경로 설정 (혼동 방지를 위해 절대경로 권장)\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "prefill_layers = 4  # 학습 시 사용한 값과 동일하게\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# best/의 remote-code(LOPA 구현)를 import할 수 있게 경로 추가\n",
    "sys.path.insert(0, str(best_dir))\n",
    "from modeling_partial_layer import LlamaForCausalLM  # Llama 3.1 계열용\n",
    "\n",
    "# 토크나이저: chat template 포함\n",
    "tok = AutoTokenizer.from_pretrained(str(best_dir))\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# 1) 백본을 허깅페이스에서 ‘깨끗하게’ 로드 (백본=llama3.1 8B instruct)\n",
    "#    주의: trust_remote_code=False → 표준 HF 클래스 경로를 통해 가중치만 로드\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=False,  # 가중치만 가져오고 동작은 our remote-code 클래스가 담당\n",
    "    cache_dir=\"/data2/jeongseokoh/hub/model\",\n",
    ").to(device).eval()\n",
    "\n",
    "# 2) LoRA 어댑터 attach 후 merge (단일 모델로)\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "merged = peft_model.merge_and_unload().to(device).eval()\n",
    "\n",
    "# 3) LOPA 추론 (compress=True → partial prefill)\n",
    "#    이미 merge된 모델이므로 추가 attach를 막기 위해 use_lora=False로 고정\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "out = merged.generate(\n",
    "    system=system,\n",
    "    document=document,\n",
    "    query=question,\n",
    "    compress=True,                 # LOPA 경로\n",
    "    tokenizer=tok,                 # 필수\n",
    "    prefill_layers=prefill_layers, # 학습과 동일\n",
    "    use_lora=False,                # 이미 merge됐으므로 재-부착 방지\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(out)  # 문자열 또는 문자열 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913061c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
