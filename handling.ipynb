{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536499ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device=cuda, DTYPE=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Notebook 1 — Env & Paths\n",
    "from pathlib import Path\n",
    "import os, torch\n",
    "\n",
    "# 안전: tokenizer 멀티스레드 이슈 방지\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# 수정된 모델링 파일 경로 (이미 앞서 저장/교체했다고 가정)\n",
    "LOPA_MODELING_PATH = Path(\"lopa_llama_modeling.py\").resolve()\n",
    "assert LOPA_MODELING_PATH.exists(), f\"❌ modeling file not found: {LOPA_MODELING_PATH}\"\n",
    "\n",
    "# 학습 산출물 경로 (예시)\n",
    "BEST_DIR  = Path(\"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\")\n",
    "BASE_DIR  = BEST_DIR / \"base\"   # base 가중치\n",
    "LORA_DIR  = BEST_DIR / \"lora\"   # LoRA 어댑터(있으면 사용)\n",
    "assert BASE_DIR.exists(), f\"❌ Base dir not found: {BASE_DIR}\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    major_cc = torch.cuda.get_device_capability(0)[0]\n",
    "    DTYPE = torch.bfloat16 if major_cc >= 8 else torch.float16\n",
    "else:\n",
    "    DTYPE = torch.float32\n",
    "\n",
    "print(f\"Device={DEVICE}, DTYPE={DTYPE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de428193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded custom LoPA modeling into transformers.models.llama.modeling_llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057ab207b1304b048c9ddd352a42434f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base weights.\n",
      "✅ LoRA merged for inference\n",
      "Model dtype: torch.bfloat16 | device: cuda:0\n",
      "TOTAL_LAYERS: 32\n"
     ]
    }
   ],
   "source": [
    "# Notebook 2 — Load custom modeling, tokenizer, model\n",
    "import importlib.util, sys, transformers, transformers.models.llama\n",
    "\n",
    "def load_custom_llama_modeling(modeling_path: Path):\n",
    "    target_name = \"transformers.models.llama.modeling_llama\"\n",
    "    if target_name in sys.modules:\n",
    "        del sys.modules[target_name]\n",
    "    spec = importlib.util.spec_from_file_location(target_name, str(modeling_path))\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise RuntimeError(f\"Failed to load spec for {modeling_path}\")\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[target_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    # 간단 검증\n",
    "    for klass in (\"LlamaModel\", \"LlamaForCausalLM\"):\n",
    "        assert hasattr(module, klass), f\"{klass} missing in {modeling_path}\"\n",
    "    return module\n",
    "\n",
    "_ = load_custom_llama_modeling(LOPA_MODELING_PATH)\n",
    "print(\"✅ Loaded custom LoPA modeling into transformers.models.llama.modeling_llama\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "# tokenizer: best 루트(없으면 base)\n",
    "tok_src = BEST_DIR if (BEST_DIR / \"tokenizer_config.json\").exists() else BASE_DIR\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, use_fast=True)\n",
    "if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# base 모델 로드\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_DIR,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"Loaded base weights.\")\n",
    "\n",
    "# LoRA 병합(있으면)\n",
    "if LORA_DIR.exists():\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, LORA_DIR)\n",
    "        model = model.merge_and_unload()\n",
    "        print(\"✅ LoRA merged for inference\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LoRA merge failed, using wrapped model: {e}\")\n",
    "\n",
    "model.eval()\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(\"Model dtype:\", next(model.parameters()).dtype, \"| device:\", DEVICE)\n",
    "\n",
    "# 참조용: 전체 레이어 수\n",
    "TOTAL_LAYERS = model.config.num_hidden_layers\n",
    "print(\"TOTAL_LAYERS:\", TOTAL_LAYERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c124863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 3 — Helpers\n",
    "import torch\n",
    "\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Question: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "def apply_chat_template(tokenizer, messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        tmpl = getattr(tokenizer, \"chat_template\", \"\") or \"\"\n",
    "        if add_generation_prompt and \"<|start_header_id|>\" in tmpl:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(tokenizer, messages, device, add_generation_prompt=False):\n",
    "    s = apply_chat_template(tokenizer, messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "def sample_from_logits(logits, *, do_sample=True, temperature=0.7, top_p=0.9,\n",
    "                       repetition_penalty=1.15, generated_ids=None):\n",
    "    \"\"\" nucleus + repetition penalty + temperature \"\"\"\n",
    "    if repetition_penalty != 1.0 and generated_ids is not None and generated_ids.numel() > 0:\n",
    "        uniq = torch.unique(generated_ids)\n",
    "        # (logit>0)/penalty, (logit<0)*penalty\n",
    "        gathered = logits.index_select(dim=-1, index=uniq)\n",
    "        gathered = torch.where(gathered > 0, gathered / repetition_penalty, gathered * repetition_penalty)\n",
    "        logits.scatter_(dim=-1, index=uniq.unsqueeze(0), src=gathered)\n",
    "\n",
    "    if not do_sample:\n",
    "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "    logits = logits / max(1e-6, temperature)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cumsum > top_p\n",
    "        mask[..., 1:] = mask[..., :-1].clone()\n",
    "        mask[..., 0] = False\n",
    "        sorted_probs[mask] = 0.0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "        next_local = torch.multinomial(sorted_probs, num_samples=1)\n",
    "        next_id = sorted_idx.gather(-1, next_local)\n",
    "    else:\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "    return next_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c8d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── helper: rope_mode를 임시로 바꿨다가 복원\n",
    "import contextlib, torch\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _tmp_rope_mode(model, mode: str):\n",
    "    inner = getattr(model, \"model\", model)\n",
    "    prev = getattr(inner, \"lopa_rope_mode\", \"local\")\n",
    "    try:\n",
    "        inner.lopa_rope_mode = str(mode)\n",
    "        yield\n",
    "    finally:\n",
    "        inner.lopa_rope_mode = prev\n",
    "\n",
    "\n",
    "# ── patched lopa_generate: \"fast_global\" 지원\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    system_prompt: str = \"You are a helpful assistant that answers questions based on the given document. \",\n",
    "    K: int = 8,\n",
    "    max_new_tokens: int = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = True,\n",
    "    stop_on_eos: bool = True,\n",
    "    repetition_penalty: float = 1.15,\n",
    "    seed_debug: bool = False,\n",
    "    step_debug: bool = False,\n",
    "    explicit_empty_upper: bool = False,\n",
    "    rope_mode: str = \"local\",       # ← \"local\" | \"global\" | \"fast_global\"\n",
    "    zero_pad_prefix: bool = False,  # ← global에서만 의미 있음; fast_global에서는 무시\n",
    "):\n",
    "    if rope_mode not in (\"local\", \"global\", \"fast_global\"):\n",
    "        raise ValueError('rope_mode must be \"local\", \"global\", or \"fast_global\"')\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    inner = getattr(model, \"model\", model)\n",
    "    cfg = getattr(model, \"config\", None)\n",
    "\n",
    "    # 1) 메시지 토크나이즈\n",
    "    msgs = build_messages(system_prompt, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(tokenizer, msgs, device, add_generation_prompt=False)\n",
    "    ids_hdr    = tokens_from_messages(tokenizer, msgs, device, add_generation_prompt=True)\n",
    "\n",
    "    # 2) 하층 K 레이어만 prefill\n",
    "    pref = inner.lopa_prefill_lower_k(input_ids=ids_phase1, lower_k=K, use_cache=True)\n",
    "    lower_cache = pref.past_key_values\n",
    "    L_all = lower_cache.get_seq_length()\n",
    "\n",
    "    # 3) upper 캐시 합성\n",
    "    if rope_mode == \"global\":\n",
    "        if zero_pad_prefix:\n",
    "            # 정확한 zero-pad 등가: upper에 L_all 길이 0-KV\n",
    "            combined = inner.lopa_build_zero_padded_cache(\n",
    "                lower_cache, lower_k=K, batch_size=ids_phase1.size(0),\n",
    "                device=ids_phase1.device, zero_len=L_all\n",
    "            )\n",
    "        elif explicit_empty_upper:\n",
    "            combined = inner.lopa_build_combined_cache(\n",
    "                lower_cache, lower_k=K, batch_size=ids_phase1.size(0), device=ids_phase1.device\n",
    "            )\n",
    "        else:\n",
    "            combined = lower_cache\n",
    "    elif rope_mode == \"fast_global\":\n",
    "        if zero_pad_prefix:\n",
    "            print(\"[warn] zero_pad_prefix=True 는 fast_global에서 무시됩니다 (virtual zero로 동치 보정).\")\n",
    "        # fast_global은 zero-pad를 만들 필요 없음\n",
    "        if explicit_empty_upper:\n",
    "            # 디버깅 가독성용으로 원하면 빈 KV를 명시할 수 있음(필수 아님)\n",
    "            combined = inner.lopa_build_combined_cache(\n",
    "                lower_cache, lower_k=K, batch_size=ids_phase1.size(0), device=ids_phase1.device\n",
    "            )\n",
    "        else:\n",
    "            combined = lower_cache\n",
    "    else:  # local\n",
    "        if explicit_empty_upper:\n",
    "            combined = inner.lopa_build_combined_cache(\n",
    "                lower_cache, lower_k=K, batch_size=ids_phase1.size(0), device=ids_phase1.device\n",
    "            )\n",
    "        else:\n",
    "            combined = lower_cache\n",
    "\n",
    "    # 4) seed → 첫 로짓\n",
    "    seed_ids = ids_hdr[:, L_all:] if ids_hdr.size(1) > L_all else ids_phase1[:, -1:]\n",
    "\n",
    "    if seed_debug:\n",
    "        print(f\"[seed] L_all={L_all}, seed_len={seed_ids.size(1)}\")\n",
    "\n",
    "    with _tmp_rope_mode(model, rope_mode):\n",
    "        seed_out = model.lopa_step_logits(\n",
    "            input_ids=seed_ids,\n",
    "            prefix_len=L_all,\n",
    "            past_key_values=combined,\n",
    "            attention_mask_total_len=L_all + seed_ids.size(1),\n",
    "            logits_to_keep=1,\n",
    "            labels=None,\n",
    "        )\n",
    "    logits = seed_out.logits[:, -1, :]\n",
    "    pkv = seed_out.past_key_values\n",
    "    total_len = L_all + seed_ids.size(1)\n",
    "\n",
    "    # 5) 첫 토큰\n",
    "    generated = []\n",
    "    next_id = sample_from_logits(\n",
    "        logits, do_sample=do_sample, temperature=temperature, top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty, generated_ids=None\n",
    "    )\n",
    "    generated.append(next_id)\n",
    "\n",
    "    eos_ids = set(t for t in [tokenizer.eos_token_id, getattr(tokenizer, \"eot_token_id\", None)] if t is not None)\n",
    "    if stop_on_eos and int(next_id[0, 0]) in eos_ids:\n",
    "        gen_ids = torch.cat(generated, dim=1)\n",
    "        return tokenizer.decode(gen_ids[0], skip_special_tokens=True), gen_ids\n",
    "\n",
    "    last = next_id\n",
    "\n",
    "    # 6) 디코딩 루프\n",
    "    for _ in range(max_new_tokens - 1):\n",
    "        if step_debug:\n",
    "            print(\"total_len(before):\", total_len)\n",
    "\n",
    "        with _tmp_rope_mode(model, rope_mode):\n",
    "            step_out = model.lopa_step_logits(\n",
    "                input_ids=last,\n",
    "                prefix_len=total_len,                      # 이번 토큰의 절대 시작 위치\n",
    "                past_key_values=pkv,\n",
    "                attention_mask_total_len=total_len + 1,    # 한 토큰 추가 후 총 길이\n",
    "                logits_to_keep=1,\n",
    "                labels=None,\n",
    "            )\n",
    "        logits = step_out.logits[:, -1, :]\n",
    "        pkv = step_out.past_key_values\n",
    "        total_len += 1\n",
    "\n",
    "        next_id = sample_from_logits(\n",
    "            logits, do_sample=do_sample, temperature=temperature, top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            generated_ids=torch.cat(generated, dim=1)\n",
    "        )\n",
    "        generated.append(next_id)\n",
    "        last = next_id\n",
    "        if stop_on_eos and int(next_id[0, 0]) in eos_ids:\n",
    "            break\n",
    "\n",
    "    gen_ids = torch.cat(generated, dim=1) if generated else torch.zeros((1, 0), dtype=torch.long, device=device)\n",
    "    text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return text, gen_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7bd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] zero_pad_prefix=True 는 fast_global에서 무시됩니다 (virtual zero로 동치 보정).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how much she earned, we need to look at the information provided in the document. However, since you didn't provide any specific details about her earnings, I'll have to make an assumption.\n",
      "\n",
      "Let's assume that the question is asking for a general idea of what she might earn based on the context given. In this case, the document doesn't mention anything about her salary or income directly. \n",
      "\n",
      "However, if we were to consider a more hypothetical scenario where she was working as a writer or editor (given the context), here are some rough estimates:\n",
      "\n",
      "- According to the Bureau of Labor Statistics, the median annual wage for writers and authors in 2020 was around $67,120.\n",
      "- For editors, it was around $61,370 per year.\n",
      "\n",
      "Please note these figures are just examples and may not be relevant to your specific situation. If you could provide more context or clarify which \"she\" refers to, I'd be happy to try and help further!\n"
     ]
    }
   ],
   "source": [
    "# Notebook 5 — Run simple example\n",
    "doc = \"\"\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting.\"\"\"\n",
    "q   = \"How much did she earn? Let's think step by step. \"\n",
    "txt, _ = lopa_generate(model, tokenizer, document=doc, question=q,\n",
    "                       K=8, do_sample=False, rope_mode=\"fast_global\", zero_pad_prefix=True)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd317d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[seed] L_all=88, seed_len=4\n",
      "li | past | global_start | off | local_start\n",
      "[00]   88 |   92 |   +4 |   88\n",
      "[01]   88 |   92 |   +4 |   88\n",
      "[02]   88 |   92 |   +4 |   88\n",
      "[03]   88 |   92 |   +4 |   88\n",
      "[04]   88 |   92 |   +4 |   88\n",
      "[05]   88 |   92 |   +4 |   88\n",
      "[06]   88 |   92 |   +4 |   88\n",
      "[07]   88 |   92 |   +4 |   88\n",
      "[28]    0 |   92 |  +92 |    0\n",
      "[29]    0 |   92 |  +92 |    0\n",
      "[30]    0 |   92 |  +92 |    0\n",
      "[31]    0 |   92 |  +92 |    0\n",
      "Greedy Answer:\n",
      " The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The\n",
      "\n",
      "Sampled Answer:\n",
      " To answer this question, To find the answer to the given information provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided\n"
     ]
    }
   ],
   "source": [
    "# Notebook 5 — Run simple example\n",
    "doc = \"\"\"The Nile is a major north-flowing river in northeastern Africa, widely regarded as the longest river in the world...\"\"\"\n",
    "q   = \"Which continent is the Nile river located in? Let's think step by step. \"\n",
    "\n",
    "# 재현성(탐욕은 의미 없음, 샘플링시만 영향)\n",
    "torch.manual_seed(0)\n",
    "model.model.lopa_rope_mode = \"global\"\n",
    "# Greedy (반복 붕괴 진단에 좋음)\n",
    "answer_greedy, _ = lopa_generate(\n",
    "    model, tokenizer, document=doc, question=q,\n",
    "    K=8, max_new_tokens=64,\n",
    "    do_sample=False, stop_on_eos=True,\n",
    "    seed_debug=True, step_debug=False,   # 필요시 True로\n",
    "    explicit_empty_upper=False\n",
    ")\n",
    "print(\"Greedy Answer:\\n\", answer_greedy)\n",
    "\n",
    "# Sampling\n",
    "answer_sample, _ = lopa_generate(\n",
    "    model, tokenizer, document=doc, question=q,\n",
    "    K=8, max_new_tokens=64,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    do_sample=True, stop_on_eos=True,\n",
    "    repetition_penalty=1.15,\n",
    "    seed_debug=False, step_debug=False\n",
    ")\n",
    "print(\"\\nSampled Answer:\\n\", answer_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a48de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== K=8 ===\n",
      "The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The\n",
      "\n",
      "=== K=32 ===\n",
      "To answer the question, we need to look at the given information. \n",
      "\n",
      "Step 1: The Nile river is described as being in northeastern Africa.\n",
      "\n",
      "Step 2: The question asks for the continent where the Nile river is located.\n",
      "\n",
      "Step 3: Since Africa is a continent and the Nile river is in northeastern Africa\n"
     ]
    }
   ],
   "source": [
    "# Notebook 6 — Compare different K\n",
    "def run_compare_K(doc, q, Ks=(8, None)):\n",
    "    outs = {}\n",
    "    for K in Ks:\n",
    "        k_val = TOTAL_LAYERS if (K is None or K == \"full\") else int(K)\n",
    "        txt, _ = lopa_generate(\n",
    "            model, tokenizer, document=doc, question=q,\n",
    "            K=k_val, max_new_tokens=64,\n",
    "            do_sample=False, stop_on_eos=True,\n",
    "            repetition_penalty=1.0,\n",
    "            seed_debug=False, step_debug=False\n",
    "        )\n",
    "        outs[k_val] = txt\n",
    "    return outs\n",
    "\n",
    "outs = run_compare_K(doc, q, Ks=(8, \"full\"))\n",
    "for k, txt in outs.items():\n",
    "    print(f\"\\n=== K={k} ===\\n{txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — 기본 설정\n",
    "import os, sys, math, contextlib\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "MISTRAL_ASSIST_START = \"<Mistral_start>\"\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 🔧 체크포인트 루트 (trainer가 저장한 _best_ckpt)\n",
    "CKPT_ROOT = \"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\"   # <-- 필요시 절대경로로 바꾸세요. 구조: base/, (옵션) lora/, tokenizer 파일\n",
    "PREFILL_LAYERS = 8                         # 하위 K 레이어만 prefill\n",
    "DTYPE = \"auto\"               # \"auto\" | \"bf16\" | \"fp16\" | \"fp32\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228dc53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48255f449f44723a31bf1ff3c208c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] LoRA adapters loaded.\n",
      "Loaded model dtype: torch.bfloat16 | device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — 로드 유틸\n",
    "\n",
    "def _get_inner_model(m):\n",
    "    if hasattr(m, \"module\"): m = m.module\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        if isinstance(m, PeftModel):\n",
    "            try: m = m.get_base_model()\n",
    "            except Exception: m = getattr(m, \"base_model\", m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for attr in (\"model\",\"transformer\",\"backbone\",\"base_model\",\"language_model\"):\n",
    "        if hasattr(m, attr):\n",
    "            cand = getattr(m, attr)\n",
    "            if hasattr(cand, \"layers\") and isinstance(getattr(cand, \"layers\", None), nn.ModuleList):\n",
    "                return cand\n",
    "            if hasattr(cand, \"decoder\") and hasattr(cand.decoder, \"layers\") and isinstance(cand.decoder.layers, nn.ModuleList):\n",
    "                return cand.decoder\n",
    "    if hasattr(m, \"layers\") and isinstance(getattr(m, \"layers\", None), nn.ModuleList):\n",
    "        return m\n",
    "    for child in m.modules():\n",
    "        if child is m: continue\n",
    "        if hasattr(child, \"layers\") and isinstance(getattr(child, \"layers\", None), nn.ModuleList):\n",
    "            return child\n",
    "    raise AttributeError(\"Could not locate inner base model with a .layers attribute\")\n",
    "\n",
    "def _is_mistral_template(tokenizer) -> bool:\n",
    "    tmpl = getattr(tokenizer, \"chat_template\", \"\") or \"\"\n",
    "    name = getattr(getattr(tokenizer, \"init_kwargs\", {}), \"get\", lambda k, d=None: d)(\"name_or_path\", \"\")\n",
    "    return (\"[INST]\" in tmpl) or (\"mistral\" in str(name).lower()) or (\"mistral\" in tmpl.lower())\n",
    "\n",
    "def ensure_mistral_special_token(tokenizer, model=None):\n",
    "    if not _is_mistral_template(tokenizer):\n",
    "        return False\n",
    "    add_tok = []\n",
    "    cur = set(tokenizer.get_vocab().keys())\n",
    "    if MISTRAL_ASSIST_START not in cur:\n",
    "        add_tok.append(MISTRAL_ASSIST_START)\n",
    "    if add_tok:\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": tokenizer.special_tokens_map_extended.get(\"additional_special_tokens\", []) + add_tok})\n",
    "        if model is not None:\n",
    "            try: model.resize_token_embeddings(len(tokenizer))\n",
    "            except Exception: pass\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "def apply_chat_template(tokenizer, messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        tmpl = getattr(tokenizer, \"chat_template\", \"\") or \"\"\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt and \"<|start_header_id|>\" in tmpl:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(tokenizer, messages, device, add_generation_prompt=False):\n",
    "    s = apply_chat_template(tokenizer, messages, add_generation_prompt)\n",
    "    print(s)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    return len(pkv)\n",
    "\n",
    "def pkv_get(pkv, idx: int):\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)\n",
    "    return dc\n",
    "\n",
    "def _kv_meta_from_model(model_like):\n",
    "    try: cfg = getattr(model_like, \"config\", None) or getattr(_get_inner_model(model_like), \"config\", None)\n",
    "    except Exception: cfg = getattr(_get_inner_model(model_like), \"config\", None)\n",
    "    num_heads = getattr(cfg, \"num_attention_heads\", None)\n",
    "    num_kv    = getattr(cfg, \"num_key_value_heads\", None) or num_heads\n",
    "    hidden    = getattr(cfg, \"hidden_size\", None)\n",
    "    head_dim  = (hidden // num_heads) if (hidden and num_heads) else None\n",
    "    try: dtype = next(_get_inner_model(model_like).parameters()).dtype\n",
    "    except Exception: dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    return int(num_kv), int(head_dim), dtype\n",
    "\n",
    "def _make_empty_kv(batch: int, num_kv: int, head_dim: int, device, dtype):\n",
    "    shape = (batch, num_kv, 0, head_dim)\n",
    "    k = torch.empty(shape, device=device, dtype=dtype)\n",
    "    v = torch.empty(shape, device=device, dtype=dtype)\n",
    "    return k.contiguous(), v.contiguous()\n",
    "\n",
    "# dtype 선택\n",
    "if DTYPE == \"fp32\": TORCH_DTYPE = torch.float32\n",
    "elif DTYPE == \"bf16\": TORCH_DTYPE = torch.bfloat16\n",
    "elif DTYPE == \"fp16\": TORCH_DTYPE = torch.float16\n",
    "else:\n",
    "    TORCH_DTYPE = (torch.bfloat16 if (DEVICE==\"cuda\" and torch.cuda.is_bf16_supported())\n",
    "                   else (torch.float16 if DEVICE==\"cuda\" else torch.float32))\n",
    "\n",
    "# 토크나이저 & 모델 로드\n",
    "tok = AutoTokenizer.from_pretrained(CKPT_ROOT, use_fast=True)\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "base_dir = Path(CKPT_ROOT) / \"base\"\n",
    "assert base_dir.is_dir(), f\"Base backbone not found at {base_dir}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(str(base_dir), trust_remote_code=False, torch_dtype=TORCH_DTYPE).to(DEVICE)\n",
    "\n",
    "lora_dir = Path(CKPT_ROOT) / \"lora\"\n",
    "if lora_dir.is_dir():\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, str(lora_dir))\n",
    "        print(\"[Info] LoRA adapters loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Warn] LoRA load failed: {e}\")\n",
    "\n",
    "# Mistral 템플릿이면 start special 보장\n",
    "_ = ensure_mistral_special_token(tok, model)\n",
    "\n",
    "# 안정성: eager 강제 (훈련과 동일)\n",
    "for k in (\"attn_implementation\",\"_attn_implementation\"):\n",
    "    try:\n",
    "        setattr(model.config, k, \"eager\")\n",
    "        setattr(_get_inner_model(model).config, k, \"eager\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded model dtype:\", next(model.parameters()).dtype, \"| device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9dda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def lopa_cache_position_patch_global(model, past_key_values, doc_len: int):\n",
    "    inner = _get_inner_model(model)\n",
    "\n",
    "    def _pkv_past_len(li: int) -> int:\n",
    "        if hasattr(past_key_values, \"key_cache\"):\n",
    "            return int(past_key_values.key_cache[li].shape[2])\n",
    "        if hasattr(past_key_values, \"layers\"):\n",
    "            return int(past_key_values.layers[li].keys.shape[2])\n",
    "        return int(past_key_values[li][0].shape[2])\n",
    "\n",
    "    nL = len(inner.layers)\n",
    "    past_lens = [ _pkv_past_len(li) for li in range(nL) ]\n",
    "\n",
    "    handles = []\n",
    "    for li, layer in enumerate(inner.layers):\n",
    "        layer._lopa_past = past_lens[li]\n",
    "        layer._lopa_li = li\n",
    "\n",
    "        def _pre_hook(module, args, kwargs):\n",
    "            li_local = getattr(module, \"_lopa_li\", 0)\n",
    "            past_len = getattr(module, \"_lopa_past\", 0)\n",
    "\n",
    "            cp = kwargs.get(\"cache_position\", None)\n",
    "            pi = kwargs.get(\"position_ids\", None)\n",
    "            start_val = None\n",
    "            if isinstance(cp, torch.Tensor) and cp.numel() > 0:\n",
    "                start_val = int(cp.view(-1)[0].item())\n",
    "            elif isinstance(pi, torch.Tensor) and pi.numel() > 0:\n",
    "                start_val = int(pi.view(-1)[0].item())\n",
    "            if start_val is None:\n",
    "                return args, kwargs\n",
    "\n",
    "            # 전 레이어가 '전역' 절대 위치(=past_len + doc_len 기준)로 보이도록 보정\n",
    "            # 하층(K): past_len 약 L_sys+L_doc → doc_len 더해도 큰 변화 없음\n",
    "            # 상층  : past_len=seed_len → doc_len을 더해 절대 프레임을 하층과 맞춤\n",
    "            desired_start = past_len + doc_len\n",
    "            off = start_val - desired_start\n",
    "            if off != 0:\n",
    "                if isinstance(cp, torch.Tensor): kwargs[\"cache_position\"] = cp - off\n",
    "                if isinstance(pi, torch.Tensor): kwargs[\"position_ids\"] = pi - off\n",
    "            return args, kwargs\n",
    "\n",
    "        handles.append(layer.register_forward_pre_hook(_pre_hook, with_kwargs=True))\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles: h.remove()\n",
    "        for layer in inner.layers:\n",
    "            for a in (\"_lopa_past\",\"_lopa_li\"):\n",
    "                if hasattr(layer,a): delattr(layer,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4e9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — prefill 캐시 구성 (하위 K만 sys+doc, 상위는 빈 KV)\n",
    "\n",
    "def build_combined_prefill_cache(model, tokenizer, system_prompt: str, document: str, question: str, K: int, device):\n",
    "    msgs = build_messages(system_prompt, document, question, include_query=True)\n",
    "    print(\"=== Phase1 ===\")\n",
    "    ids_phase1 = tokens_from_messages(tokenizer, msgs, device, add_generation_prompt=False)\n",
    "    print(\"=== Header ===\\n\")\n",
    "    ids_hdr    = tokens_from_messages(tokenizer, msgs, device, add_generation_prompt=True)\n",
    "    sys_only   = tokens_from_messages(tokenizer, [{\"role\":\"system\",\"content\":system_prompt}], device, add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Document tokens must be > 0\"\n",
    "\n",
    "    inner = _get_inner_model(model)\n",
    "    full_layers: nn.ModuleList = inner.layers\n",
    "    n_layers = len(full_layers)\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "\n",
    "    # lower K: [system] prefill\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "    inner.layers = lower_layers\n",
    "    out_sys_low = inner(input_ids=sys_only, attention_mask=torch.ones_like(sys_only), use_cache=True, return_dict=True)\n",
    "    pkv_sys_low = out_sys_low.past_key_values\n",
    "\n",
    "    # then [document] prefill (still lower K)\n",
    "    dc_low_in = dc_from_subset(pkv_sys_low, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    out_low = inner(input_ids=ids_phase1[:, L_sys:], past_key_values=dc_low_in,\n",
    "                    attention_mask=None, use_cache=True, return_dict=True)\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # restore all layers\n",
    "    inner.layers = full_layers\n",
    "\n",
    "    # combined cache: lower K = sys+doc, upper = empty\n",
    "    combined = DynamicCache()\n",
    "    num_kv, head_dim, kv_dtype = _kv_meta_from_model(model)\n",
    "    for li in range(n_layers):\n",
    "        if li < K_eff:\n",
    "            k_sys, v_sys = pkv_get(pkv_sys_low, li)\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_sys_slice = k_sys[:, :, :L_sys, :]\n",
    "            v_sys_slice = v_sys[:, :, :L_sys, :]\n",
    "            k_doc = k_low[:, :, -L_doc:, :]\n",
    "            v_doc = v_low[:, :, -L_doc:, :]\n",
    "            combined.update(torch.cat([k_sys_slice, k_doc], dim=2).contiguous(),\n",
    "                            torch.cat([v_sys_slice, v_doc], dim=2).contiguous(), li)\n",
    "        else:\n",
    "            k_empty, v_empty = _make_empty_kv(1, num_kv, head_dim, DEVICE, kv_dtype)\n",
    "            combined.update(k_empty, v_empty, li)\n",
    "\n",
    "    # header tail as seed\n",
    "    hdr_tail = ids_hdr[:, L_all:]\n",
    "    seed_default = hdr_tail if hdr_tail.numel() > 0 else ids_phase1[:, -1:]\n",
    "\n",
    "    meta = dict(L_sys=int(L_sys), L_doc=int(L_doc), L_all=int(L_all),\n",
    "                n_layers=int(n_layers), K_eff=int(K_eff))\n",
    "    return combined, seed_default, ids_phase1, ids_hdr, meta\n",
    "\n",
    "def print_cache_lengths(cache, tag: str):\n",
    "    print(f\"\\n[{tag}] per-layer past lengths\")\n",
    "    n = pkv_len(cache)\n",
    "    for li in range(n):\n",
    "        k, _ = pkv_get(cache, li)\n",
    "        print(f\"  layer {li:02d}: past_seq = {int(k.shape[2])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407dede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase1 ===\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant that answers questions based on the given document.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Document:\n",
      "France is a country in Western Europe. Its capital city is Paris. It is known for art, fashion, and cuisine.\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Finally, provide your answer in '\\boxed{answer}' at the end of your explanation.<|eot_id|>\n",
      "=== Header ===\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant that answers questions based on the given document.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Document:\n",
      "France is a country in Western Europe. Its capital city is Paris. It is known for art, fashion, and cuisine.\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Finally, provide your answer in '\\boxed{answer}' at the end of your explanation.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant that answers questions based on the given document.<|eot_id|>\n",
      "Layers total= 32 | K_eff= 8 | L_sys= 40 | L_doc= 59 | L_all= 99\n",
      "\n",
      "[prefill] per-layer past lengths\n",
      "  layer 00: past_seq = 99\n",
      "  layer 01: past_seq = 99\n",
      "  layer 02: past_seq = 99\n",
      "  layer 03: past_seq = 99\n",
      "  layer 04: past_seq = 99\n",
      "  layer 05: past_seq = 99\n",
      "  layer 06: past_seq = 99\n",
      "  layer 07: past_seq = 99\n",
      "  layer 08: past_seq = 0\n",
      "  layer 09: past_seq = 0\n",
      "  layer 10: past_seq = 0\n",
      "  layer 11: past_seq = 0\n",
      "  layer 12: past_seq = 0\n",
      "  layer 13: past_seq = 0\n",
      "  layer 14: past_seq = 0\n",
      "  layer 15: past_seq = 0\n",
      "  layer 16: past_seq = 0\n",
      "  layer 17: past_seq = 0\n",
      "  layer 18: past_seq = 0\n",
      "  layer 19: past_seq = 0\n",
      "  layer 20: past_seq = 0\n",
      "  layer 21: past_seq = 0\n",
      "  layer 22: past_seq = 0\n",
      "  layer 23: past_seq = 0\n",
      "  layer 24: past_seq = 0\n",
      "  layer 25: past_seq = 0\n",
      "  layer 26: past_seq = 0\n",
      "  layer 27: past_seq = 0\n",
      "  layer 28: past_seq = 0\n",
      "  layer 29: past_seq = 0\n",
      "  layer 30: past_seq = 0\n",
      "  layer 31: past_seq = 0\n",
      "Prefill KV check: OK\n",
      "\n",
      "[Seed]\n",
      "  seed length = 11 | preview: <|start_header_id|>assistant<|end_header_id|>⏎⏎To find the capital of France,\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — 입력과 prefill 실행/검사\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant that answers questions based on the given document. \"\n",
    "QUESTION = \"What is the capital of France?\\nFinally, provide your answer in '\\\\boxed{answer}' at the end of your explanation.\"\n",
    "DOCUMENT = \"France is a country in Western Europe. Its capital city is Paris. It is known for art, fashion, and cuisine.\"\n",
    "\n",
    "combined, seed_default, ids_phase1, ids_hdr, meta = build_combined_prefill_cache(\n",
    "    model, tok, SYSTEM_PROMPT, DOCUMENT, QUESTION, PREFILL_LAYERS, DEVICE\n",
    ")\n",
    "\n",
    "L_sys, L_doc, L_all, n_layers, K_eff = meta[\"L_sys\"], meta[\"L_doc\"], meta[\"L_all\"], meta[\"n_layers\"], meta[\"K_eff\"]\n",
    "\n",
    "print(\"Layers total=\", n_layers, \"| K_eff=\", K_eff, \"| L_sys=\", L_sys, \"| L_doc=\", L_doc, \"| L_all=\", L_all)\n",
    "print_cache_lengths(combined, tag=\"prefill\")\n",
    "\n",
    "# 검증: 하위 K는 L_sys+L_doc, 상위는 0\n",
    "ok = True\n",
    "for li in range(n_layers):\n",
    "    k, _ = pkv_get(combined, li)\n",
    "    expect = (L_sys+L_doc) if li < K_eff else 0\n",
    "    if int(k.shape[2]) != expect:\n",
    "        ok = False\n",
    "print(\"Prefill KV check:\", \"OK\" if ok else \"MISMATCH\")\n",
    "\n",
    "# Seed 확정 (header tail 우선, 그 다음 Mistral start, 그 다음 fallback)\n",
    "seed = seed_default\n",
    "if seed.numel() == 0:\n",
    "    if _is_mistral_template(tok):\n",
    "        tid = tok.convert_tokens_to_ids(MISTRAL_ASSIST_START)\n",
    "        if tid is not None and tid >= 0:\n",
    "            seed = torch.tensor([[int(tid)]], device=DEVICE, dtype=ids_hdr.dtype)\n",
    "    if seed.numel() == 0:\n",
    "        seed = ids_phase1[:, -1:]\n",
    "# 시드 뒤에 'Answer: ' 같은 오프너를 1회만 덧붙인다.\n",
    "SEED_PREFIX = \"To find the capital of France,\"  # 또는 \"답변: \", \"Final answer: \" 등 태스크에 맞게\n",
    "prefix_ids = tok(SEED_PREFIX, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "seed = torch.cat([seed, prefix_ids], dim=1)\n",
    "\n",
    "# 길이 기대치 검증 위해 meta에 반영(선택)\n",
    "meta[\"seed_len\"] = int(seed.size(1))\n",
    "def decode_piece(ids: torch.Tensor, limit=120):\n",
    "    return tok.decode(ids[0].tolist(), skip_special_tokens=False)[:limit].replace(\"\\n\",\"⏎\")\n",
    "\n",
    "print(\"\\n[Seed]\")\n",
    "print(\"  seed length =\", int(seed.size(1)), \"| preview:\", decode_piece(seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9af6580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Next-token distribution | after SEED]\n",
      "   1. id=  2057  p= 0.98828  tok=' To'\n",
      "   2. id=   311  p= 0.01099  tok=' to'\n",
      "   3. id=    11  p= 0.00004  tok=','\n",
      "   4. id=  1271  p= 0.00000  tok='To'\n",
      "   5. id=  5257  p= 0.00000  tok=' TO'\n",
      "   6. id=   315  p= 0.00000  tok=' of'\n",
      "   7. id=   320  p= 0.00000  tok=' ('\n",
      "   8. id=  4194  p= 0.00000  tok='\\xa0'\n",
      "   9. id=  9822  p= 0.00000  tok=' France'\n",
      "  10. id=   264  p= 0.00000  tok=' a'\n",
      "\n",
      "[after-seed] per-layer past lengths\n",
      "  layer 00: past_seq = 110\n",
      "  layer 01: past_seq = 110\n",
      "  layer 02: past_seq = 110\n",
      "  layer 03: past_seq = 110\n",
      "  layer 04: past_seq = 110\n",
      "  layer 05: past_seq = 110\n",
      "  layer 06: past_seq = 110\n",
      "  layer 07: past_seq = 110\n",
      "  layer 08: past_seq = 11\n",
      "  layer 09: past_seq = 11\n",
      "  layer 10: past_seq = 11\n",
      "  layer 11: past_seq = 11\n",
      "  layer 12: past_seq = 11\n",
      "  layer 13: past_seq = 11\n",
      "  layer 14: past_seq = 11\n",
      "  layer 15: past_seq = 11\n",
      "  layer 16: past_seq = 11\n",
      "  layer 17: past_seq = 11\n",
      "  layer 18: past_seq = 11\n",
      "  layer 19: past_seq = 11\n",
      "  layer 20: past_seq = 11\n",
      "  layer 21: past_seq = 11\n",
      "  layer 22: past_seq = 11\n",
      "  layer 23: past_seq = 11\n",
      "  layer 24: past_seq = 11\n",
      "  layer 25: past_seq = 11\n",
      "  layer 26: past_seq = 11\n",
      "  layer 27: past_seq = 11\n",
      "  layer 28: past_seq = 11\n",
      "  layer 29: past_seq = 11\n",
      "  layer 30: past_seq = 11\n",
      "  layer 31: past_seq = 11\n",
      "After-seed KV check: OK\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — seed만 넣어 logits/Top-K 보기\n",
    "\n",
    "@torch.no_grad()\n",
    "def step_once(model, cache, input_ids, patch=True):\n",
    "    if patch:\n",
    "        doc_len = L_sys + L_doc\n",
    "        with lopa_cache_position_patch_global(model, cache, doc_len):\n",
    "            out = model(input_ids=input_ids, past_key_values=cache, use_cache=True, return_dict=True)\n",
    "    else:\n",
    "        out = model(input_ids=input_ids, past_key_values=cache, use_cache=True, return_dict=True)\n",
    "    return out.logits, out.past_key_values\n",
    "\n",
    "def topk_from_logits(logits, tokenizer, k=10, temperature=0.0):\n",
    "    last = logits[:, -1, :]\n",
    "    if temperature and temperature > 0.0:\n",
    "        last = last / float(temperature)\n",
    "    probs = last.softmax(dim=-1)\n",
    "    top_p, top_i = torch.topk(probs, k, dim=-1)\n",
    "    toks = [tokenizer.decode([int(t.item())], skip_special_tokens=False) for t in top_i[0]]\n",
    "    return [(int(i.item()), float(p.item()), t) for i, p, t in zip(top_i[0], top_p[0], toks)]\n",
    "\n",
    "# seed → 한 번 forward\n",
    "logits, combined = step_once(model, combined, input_ids=seed, patch=True)\n",
    "topk = topk_from_logits(logits, tok, k=10, temperature=0.0)\n",
    "\n",
    "print(\"[Next-token distribution | after SEED]\")\n",
    "for r,(tid,prob,txt) in enumerate(topk, 1):\n",
    "    print(f\"  {r:2d}. id={tid:6d}  p={prob:8.5f}  tok={repr(txt)}\")\n",
    "\n",
    "print_cache_lengths(combined, tag=\"after-seed\")\n",
    "\n",
    "# 기대 길이: 하위K = L_sys+L_doc+len(seed), 상위 = len(seed)\n",
    "seed_len = int(seed.size(1))\n",
    "ok2 = True\n",
    "for li in range(n_layers):\n",
    "    k, _ = pkv_get(combined, li)\n",
    "    expect = (L_sys+L_doc+seed_len) if li < K_eff else seed_len\n",
    "    if int(k.shape[2]) != expect:\n",
    "        ok2 = False\n",
    "print(\"After-seed KV check:\", \"OK\" if ok2 else \"MISMATCH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4169608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7b — 시드를 이미 한 번 적용한 상태(after-seed)에서 바로 스텝 시작\n",
    "@torch.no_grad()\n",
    "def generate_stepwise_from_after_seed(model, tokenizer, cache, first_logits, meta, max_new_tokens=20, temperature=0.0, topk=10):\n",
    "    logits = first_logits  # Cell 6에서 얻은 logits을 그대로 사용 (seed를 다시 넣지 않음)\n",
    "    gen = []\n",
    "    for step in range(max_new_tokens):\n",
    "        # 1) 직전 logits에서 다음 토큰 결정\n",
    "        if temperature and temperature > 0.0:\n",
    "            dist = torch.distributions.Categorical(logits=logits[:, -1, :]/float(temperature))\n",
    "            next_id = dist.sample().unsqueeze(0)\n",
    "        else:\n",
    "            next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        # if step == 0:\n",
    "        #     next_id = [[11439]]\n",
    "        print(next_id)\n",
    "        # 2) 그 토큰을 입력으로 한 스텝 전진\n",
    "        logits, cache = step_once(model, cache, input_ids=next_id, patch=True)\n",
    "\n",
    "        # 3) 리포트\n",
    "        report = topk_from_logits(logits, tokenizer, k=topk, temperature=temperature)\n",
    "        gen.append(int(next_id[0,0].item()))\n",
    "        print(f\"\\n[Step {step:02d}] input id={int(next_id[0,0])} '{tokenizer.decode([int(next_id[0,0])], skip_special_tokens=False)}'\")\n",
    "        for r,(tid,prob,txt) in enumerate(report, 1):\n",
    "            print(f\"  {r:2d}. id={tid:6d}  p={prob:8.5f}  tok={repr(txt)}'\")\n",
    "\n",
    "        # 4) 길이 검증 (expect: 하위K = L_sys+L_doc+seed_len+(step+1), 상위 = seed_len+(step+1))\n",
    "        seed_len =  meta[\"L_all\"] - meta[\"L_sys\"]  # = L_doc?  ← 이미 after-seed라면 seed_len은 Cell 5에서 별도로 보관해둔 값을 쓰는 게 깔끔\n",
    "        # 권장: Cell 5에서 seed_len을 meta에 넣어두세요. 아래는 예시로 seed_len을 별도로 넘겨 받는 쪽이 안전.\n",
    "        # 여기서는 설명 단순화를 위해 meta에 seed_len이 저장돼 있다고 가정:\n",
    "        seed_len = meta.get(\"seed_len\", 4)\n",
    "        ok_step = True\n",
    "        for li in range(meta[\"n_layers\"]):\n",
    "            k, _ = pkv_get(cache, li)\n",
    "            expect = (meta[\"L_sys\"]+meta[\"L_doc\"]+seed_len+(step+1)) if li < meta[\"K_eff\"] else (seed_len+(step+1))\n",
    "            if int(k.shape[2]) != expect:\n",
    "                ok_step = False\n",
    "        print(\"  KV check:\", \"OK\" if ok_step else \"MISMATCH\")\n",
    "        print(\"  partial:\", tokenizer.decode(gen, skip_special_tokens=False).replace(\"\\n\",\"⏎\")[:200])\n",
    "\n",
    "    return tokenizer.decode(gen, skip_special_tokens=False), cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da085920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2057]], device='cuda:0')\n",
      "\n",
      "[Step 00] input id=2057 ' To'\n",
      "   1. id=  1505  p= 1.00000  tok=' find''\n",
      "   2. id=  1766  p= 0.00000  tok=' found''\n",
      "   3. id=  2733  p= 0.00000  tok=' feel''\n",
      "   4. id=  2725  p= 0.00000  tok='.find''\n",
      "   5. id= 14035  p= 0.00000  tok=' finds''\n",
      "   6. id=  3990  p= 0.00000  tok='find''\n",
      "   7. id=  1833  p= 0.00000  tok=' follow''\n",
      "   8. id=  9455  p= 0.00000  tok=' finding''\n",
      "   9. id=  7531  p= 0.00000  tok=' Find''\n",
      "  10. id=  5266  p= 0.00000  tok=' fill''\n",
      "  KV check: OK\n",
      "  partial:  To\n",
      "tensor([[1505]], device='cuda:0')\n",
      "\n",
      "[Step 01] input id=1505 ' find'\n",
      "   1. id=   279  p= 1.00000  tok=' the''\n",
      "   2. id=  1820  p= 0.00000  tok='the''\n",
      "   3. id=   264  p= 0.00000  tok=' a''\n",
      "   4. id=   578  p= 0.00000  tok=' The''\n",
      "   5. id=    11  p= 0.00000  tok=',''\n",
      "   6. id=   315  p= 0.00000  tok=' of''\n",
      "   7. id=  1505  p= 0.00000  tok=' find''\n",
      "   8. id=  1766  p= 0.00000  tok=' found''\n",
      "   9. id=  2057  p= 0.00000  tok=' To''\n",
      "  10. id=  3247  p= 0.00000  tok=' THE''\n",
      "  KV check: OK\n",
      "  partial:  To find\n",
      "tensor([[279]], device='cuda:0')\n",
      "\n",
      "[Step 02] input id=279 ' the'\n",
      "   1. id=  6864  p= 1.00000  tok=' capital''\n",
      "   2. id= 66163  p= 0.00000  tok='capital''\n",
      "   3. id= 24862  p= 0.00000  tok=' captain''\n",
      "   4. id= 18880  p= 0.00000  tok=' Capital''\n",
      "   5. id= 17703  p= 0.00000  tok=' caption''\n",
      "   6. id= 53155  p= 0.00000  tok=' capita''\n",
      "   7. id= 41067  p= 0.00000  tok=' capitalist''\n",
      "   8. id= 98231  p= 0.00000  tok='-capital''\n",
      "   9. id= 53825  p= 0.00000  tok=' capitalize''\n",
      "  10. id=  2107  p= 0.00000  tok=' cap''\n",
      "  KV check: OK\n",
      "  partial:  To find the\n",
      "tensor([[6864]], device='cuda:0')\n",
      "\n",
      "[Step 03] input id=6864 ' capital'\n",
      "   1. id=   315  p= 1.00000  tok=' of''\n",
      "   2. id= 60835  p= 0.00000  tok=' của''\n",
      "   3. id=    11  p= 0.00000  tok=',''\n",
      "   4. id=   279  p= 0.00000  tok=' the''\n",
      "   5. id=   297  p= 0.00000  tok=' o''\n",
      "   6. id=   304  p= 0.00000  tok=' in''\n",
      "   7. id=   320  p= 0.00000  tok=' (''\n",
      "   8. id=   264  p= 0.00000  tok=' a''\n",
      "   9. id=   369  p= 0.00000  tok=' for''\n",
      "  10. id=   323  p= 0.00000  tok=' and''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital\n",
      "tensor([[315]], device='cuda:0')\n",
      "\n",
      "[Step 04] input id=315 ' of'\n",
      "   1. id=  9822  p= 1.00000  tok=' France''\n",
      "   2. id= 50100  p= 0.00000  tok='France''\n",
      "   3. id=  9635  p= 0.00000  tok=' England''\n",
      "   4. id= 48687  p= 0.00000  tok=' france''\n",
      "   5. id=  8753  p= 0.00000  tok=' French''\n",
      "   6. id= 10057  p= 0.00000  tok=' Germany''\n",
      "   7. id=  9893  p= 0.00000  tok=' Franc''\n",
      "   8. id=  4606  p= 0.00000  tok=' Europe''\n",
      "   9. id= 26184  p= 0.00000  tok=' Francis''\n",
      "  10. id= 18157  p= 0.00000  tok=' Spain''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of\n",
      "tensor([[9822]], device='cuda:0')\n",
      "\n",
      "[Step 05] input id=9822 ' France'\n",
      "   1. id=    11  p= 1.00000  tok=',''\n",
      "   2. id=  3638  p= 0.00000  tok=',\\n\\n''\n",
      "   3. id=   320  p= 0.00000  tok=' (''\n",
      "   4. id= 10856  p= 0.00000  tok=',,''\n",
      "   5. id=   345  p= 0.00000  tok=',\\n''\n",
      "   6. id=  1174  p= 0.00000  tok=',''\n",
      "   7. id=    25  p= 0.00000  tok=':''\n",
      "   8. id=    26  p= 0.00000  tok=';''\n",
      "   9. id=   705  p= 0.00000  tok='),''\n",
      "  10. id=  9822  p= 0.00000  tok=' France''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France\n",
      "tensor([[11]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 06] input id=11 ','\n",
      "   1. id=  2057  p= 1.00000  tok=' To''\n",
      "   2. id=  5257  p= 0.00000  tok=' TO''\n",
      "   3. id=   350  p= 0.00000  tok=' T''\n",
      "   4. id=  1271  p= 0.00000  tok='To''\n",
      "   5. id=   311  p= 0.00000  tok=' to''\n",
      "   6. id=  3354  p= 0.00000  tok='.To''\n",
      "   7. id=   578  p= 0.00000  tok=' The''\n",
      "   8. id=  1183  p= 0.00000  tok=' Tr''\n",
      "   9. id=  7054  p= 0.00000  tok=' Top''\n",
      "  10. id=   666  p= 0.00000  tok=' Th''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France,\n",
      "tensor([[2057]], device='cuda:0')\n",
      "\n",
      "[Step 07] input id=2057 ' To'\n",
      "   1. id=  1505  p= 1.00000  tok=' find''\n",
      "   2. id=  1766  p= 0.00000  tok=' found''\n",
      "   3. id=  2733  p= 0.00000  tok=' feel''\n",
      "   4. id=  2725  p= 0.00000  tok='.find''\n",
      "   5. id= 14035  p= 0.00000  tok=' finds''\n",
      "   6. id=  5266  p= 0.00000  tok=' fill''\n",
      "   7. id=  9455  p= 0.00000  tok=' finding''\n",
      "   8. id=  1833  p= 0.00000  tok=' follow''\n",
      "   9. id=  3887  p= 0.00000  tok=' fund''\n",
      "  10. id=  3990  p= 0.00000  tok='find''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To\n",
      "tensor([[1505]], device='cuda:0')\n",
      "\n",
      "[Step 08] input id=1505 ' find'\n",
      "   1. id=   279  p= 1.00000  tok=' the''\n",
      "   2. id=  1820  p= 0.00000  tok='the''\n",
      "   3. id=    11  p= 0.00000  tok=',''\n",
      "   4. id=   264  p= 0.00000  tok=' a''\n",
      "   5. id=   315  p= 0.00000  tok=' of''\n",
      "   6. id=   323  p= 0.00000  tok=' and''\n",
      "   7. id=   420  p= 0.00000  tok=' this''\n",
      "   8. id=  1505  p= 0.00000  tok=' find''\n",
      "   9. id=   578  p= 0.00000  tok=' The''\n",
      "  10. id=   430  p= 0.00000  tok=' that''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find\n",
      "tensor([[279]], device='cuda:0')\n",
      "\n",
      "[Step 09] input id=279 ' the'\n",
      "   1. id=  6864  p= 1.00000  tok=' capital''\n",
      "   2. id= 66163  p= 0.00000  tok='capital''\n",
      "   3. id= 24862  p= 0.00000  tok=' captain''\n",
      "   4. id= 18880  p= 0.00000  tok=' Capital''\n",
      "   5. id= 53155  p= 0.00000  tok=' capita''\n",
      "   6. id= 17703  p= 0.00000  tok=' caption''\n",
      "   7. id= 53825  p= 0.00000  tok=' capitalize''\n",
      "   8. id= 41067  p= 0.00000  tok=' capitalist''\n",
      "   9. id= 98231  p= 0.00000  tok='-capital''\n",
      "  10. id= 32682  p= 0.00000  tok=' capitalism''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the\n",
      "tensor([[6864]], device='cuda:0')\n",
      "\n",
      "[Step 10] input id=6864 ' capital'\n",
      "   1. id=   315  p= 1.00000  tok=' of''\n",
      "   2. id=    11  p= 0.00000  tok=',''\n",
      "   3. id= 60835  p= 0.00000  tok=' của''\n",
      "   4. id=   279  p= 0.00000  tok=' the''\n",
      "   5. id=   297  p= 0.00000  tok=' o''\n",
      "   6. id=   304  p= 0.00000  tok=' in''\n",
      "   7. id=   389  p= 0.00000  tok=' on''\n",
      "   8. id=   369  p= 0.00000  tok=' for''\n",
      "   9. id=   323  p= 0.00000  tok=' and''\n",
      "  10. id=   311  p= 0.00000  tok=' to''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital\n",
      "tensor([[315]], device='cuda:0')\n",
      "\n",
      "[Step 11] input id=315 ' of'\n",
      "   1. id=  9822  p= 1.00000  tok=' France''\n",
      "   2. id= 50100  p= 0.00000  tok='France''\n",
      "   3. id= 26184  p= 0.00000  tok=' Francis''\n",
      "   4. id=  9893  p= 0.00000  tok=' Franc''\n",
      "   5. id=  8753  p= 0.00000  tok=' French''\n",
      "   6. id= 48687  p= 0.00000  tok=' france''\n",
      "   7. id=  9635  p= 0.00000  tok=' England''\n",
      "   8. id= 43833  p= 0.00000  tok=' Frances''\n",
      "   9. id= 10057  p= 0.00000  tok=' Germany''\n",
      "  10. id= 44943  p= 0.00000  tok=' Franco''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of\n",
      "tensor([[9822]], device='cuda:0')\n",
      "\n",
      "[Step 12] input id=9822 ' France'\n",
      "   1. id=    11  p= 1.00000  tok=',''\n",
      "   2. id=  3638  p= 0.00000  tok=',\\n\\n''\n",
      "   3. id=   320  p= 0.00000  tok=' (''\n",
      "   4. id= 10856  p= 0.00000  tok=',,''\n",
      "   5. id=    25  p= 0.00000  tok=':''\n",
      "   6. id=   345  p= 0.00000  tok=',\\n''\n",
      "   7. id=  1174  p= 0.00000  tok=',''\n",
      "   8. id=    26  p= 0.00000  tok=';''\n",
      "   9. id=   705  p= 0.00000  tok='),''\n",
      "  10. id=   315  p= 0.00000  tok=' of''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France\n",
      "tensor([[11]], device='cuda:0')\n",
      "\n",
      "[Step 13] input id=11 ','\n",
      "   1. id=  2057  p= 1.00000  tok=' To''\n",
      "   2. id=   350  p= 0.00000  tok=' T''\n",
      "   3. id=  1271  p= 0.00000  tok='To''\n",
      "   4. id=  5257  p= 0.00000  tok=' TO''\n",
      "   5. id=  3354  p= 0.00000  tok='.To''\n",
      "   6. id=   311  p= 0.00000  tok=' to''\n",
      "   7. id=   578  p= 0.00000  tok=' The''\n",
      "   8. id=   666  p= 0.00000  tok=' Th''\n",
      "   9. id=  1183  p= 0.00000  tok=' Tr''\n",
      "  10. id=  2722  p= 0.00000  tok=' Te''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France,\n",
      "tensor([[2057]], device='cuda:0')\n",
      "\n",
      "[Step 14] input id=2057 ' To'\n",
      "   1. id=  1505  p= 1.00000  tok=' find''\n",
      "   2. id=  1766  p= 0.00000  tok=' found''\n",
      "   3. id=  2733  p= 0.00000  tok=' feel''\n",
      "   4. id=  2725  p= 0.00000  tok='.find''\n",
      "   5. id= 14035  p= 0.00000  tok=' finds''\n",
      "   6. id=  9455  p= 0.00000  tok=' finding''\n",
      "   7. id=  5266  p= 0.00000  tok=' fill''\n",
      "   8. id=  3990  p= 0.00000  tok='find''\n",
      "   9. id=  1833  p= 0.00000  tok=' follow''\n",
      "  10. id=  3887  p= 0.00000  tok=' fund''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To\n",
      "tensor([[1505]], device='cuda:0')\n",
      "\n",
      "[Step 15] input id=1505 ' find'\n",
      "   1. id=   279  p= 1.00000  tok=' the''\n",
      "   2. id=  1820  p= 0.00000  tok='the''\n",
      "   3. id=   315  p= 0.00000  tok=' of''\n",
      "   4. id=    11  p= 0.00000  tok=',''\n",
      "   5. id=   264  p= 0.00000  tok=' a''\n",
      "   6. id=   578  p= 0.00000  tok=' The''\n",
      "   7. id=   430  p= 0.00000  tok=' that''\n",
      "   8. id=   323  p= 0.00000  tok=' and''\n",
      "   9. id=   420  p= 0.00000  tok=' this''\n",
      "  10. id=  1505  p= 0.00000  tok=' find''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find\n",
      "tensor([[279]], device='cuda:0')\n",
      "\n",
      "[Step 16] input id=279 ' the'\n",
      "   1. id=  6864  p= 1.00000  tok=' capital''\n",
      "   2. id= 66163  p= 0.00000  tok='capital''\n",
      "   3. id= 18880  p= 0.00000  tok=' Capital''\n",
      "   4. id= 24862  p= 0.00000  tok=' captain''\n",
      "   5. id= 53155  p= 0.00000  tok=' capita''\n",
      "   6. id= 53825  p= 0.00000  tok=' capitalize''\n",
      "   7. id= 41067  p= 0.00000  tok=' capitalist''\n",
      "   8. id= 98231  p= 0.00000  tok='-capital''\n",
      "   9. id= 17703  p= 0.00000  tok=' caption''\n",
      "  10. id= 32682  p= 0.00000  tok=' capitalism''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find the\n",
      "tensor([[6864]], device='cuda:0')\n",
      "\n",
      "[Step 17] input id=6864 ' capital'\n",
      "   1. id=   315  p= 1.00000  tok=' of''\n",
      "   2. id= 60835  p= 0.00000  tok=' của''\n",
      "   3. id=    11  p= 0.00000  tok=',''\n",
      "   4. id=   279  p= 0.00000  tok=' the''\n",
      "   5. id=   297  p= 0.00000  tok=' o''\n",
      "   6. id=   304  p= 0.00000  tok=' in''\n",
      "   7. id=   323  p= 0.00000  tok=' and''\n",
      "   8. id=   389  p= 0.00000  tok=' on''\n",
      "   9. id=   369  p= 0.00000  tok=' for''\n",
      "  10. id=   311  p= 0.00000  tok=' to''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find the capital\n",
      "tensor([[315]], device='cuda:0')\n",
      "\n",
      "[Step 18] input id=315 ' of'\n",
      "   1. id=  9822  p= 1.00000  tok=' France''\n",
      "   2. id= 50100  p= 0.00000  tok='France''\n",
      "   3. id=  9893  p= 0.00000  tok=' Franc''\n",
      "   4. id= 26184  p= 0.00000  tok=' Francis''\n",
      "   5. id= 48687  p= 0.00000  tok=' france''\n",
      "   6. id=  8753  p= 0.00000  tok=' French''\n",
      "   7. id=  9635  p= 0.00000  tok=' England''\n",
      "   8. id= 10057  p= 0.00000  tok=' Germany''\n",
      "   9. id= 43833  p= 0.00000  tok=' Frances''\n",
      "  10. id=  4606  p= 0.00000  tok=' Europe''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find the capital of\n",
      "tensor([[9822]], device='cuda:0')\n",
      "\n",
      "[Step 19] input id=9822 ' France'\n",
      "   1. id=    11  p= 1.00000  tok=',''\n",
      "   2. id=  3638  p= 0.00000  tok=',\\n\\n''\n",
      "   3. id=   320  p= 0.00000  tok=' (''\n",
      "   4. id=    25  p= 0.00000  tok=':''\n",
      "   5. id=    26  p= 0.00000  tok=';''\n",
      "   6. id= 10856  p= 0.00000  tok=',,''\n",
      "   7. id=   705  p= 0.00000  tok='),''\n",
      "   8. id=  1174  p= 0.00000  tok=',''\n",
      "   9. id=   345  p= 0.00000  tok=',\\n''\n",
      "  10. id=    13  p= 0.00000  tok='.''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find the capital of France\n",
      "\n",
      "=== [Final output (raw)] ===\n",
      " To find the capital of France, To find the capital of France, To find the capital of France\n"
     ]
    }
   ],
   "source": [
    "# Cell 6을 실행한 직후에:\n",
    "first_logits = logits  # Cell 6에서 나온 logits\n",
    "# seed_len을 meta에 기록해두면 위 함수의 기대 길이 계산이 정확해집니다.\n",
    "meta[\"seed_len\"] = int(seed.size(1))  # 여기서는 4\n",
    "\n",
    "FINAL, combined = generate_stepwise_from_after_seed(\n",
    "    model, tok, combined, first_logits, meta,\n",
    "    max_new_tokens=20, temperature=0.0, topk=10\n",
    ")\n",
    "print(\"\\n=== [Final output (raw)] ===\")\n",
    "print(FINAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bcb0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 11439]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tok.encode(\"According\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964dceee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatistics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 학습 스크립트에서 쓰던 유틸 alias 가져오기 (이미 전 셀에서 wired 했다면 생략)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# from train_lopa_pure import build_messages, tokens_from_messages, _get_inner_model, pkv_get, _kv_meta_from_model, _make_empty_kv, dc_from_subset, lopa_cache_position_patch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m DEVICE = \u001b[38;5;28mnext\u001b[39m(\u001b[43mmodel\u001b[49m.parameters()).device\n\u001b[32m      8\u001b[39m SYSTEM = \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant that answers questions based on the given document. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_combined\u001b[39m(tokenizer, model, system_prompt, d, q, K):\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import json, random, torch\n",
    "from statistics import mean\n",
    "\n",
    "# 학습 스크립트에서 쓰던 유틸 alias 가져오기 (이미 전 셀에서 wired 했다면 생략)\n",
    "# from train_lopa_pure import build_messages, tokens_from_messages, _get_inner_model, pkv_get, _kv_meta_from_model, _make_empty_kv, dc_from_subset, lopa_cache_position_patch\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "SYSTEM = \"You are a helpful assistant that answers questions based on the given document. \"\n",
    "\n",
    "def make_combined(tokenizer, model, system_prompt, d, q, K):\n",
    "    msgs = build_messages(system_prompt, d, q, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(tokenizer, msgs, DEVICE, add_generation_prompt=False)\n",
    "    ids_hdr    = tokens_from_messages(tokenizer, msgs, DEVICE, add_generation_prompt=True)\n",
    "    sys_only   = tokens_from_messages(tokenizer, [{\"role\":\"system\",\"content\":system_prompt}], DEVICE, add_generation_prompt=False)\n",
    "\n",
    "    L_sys, L_all = sys_only.size(1), ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0\n",
    "    inner = _get_inner_model(model)\n",
    "    full_layers = inner.layers\n",
    "    n_layers = len(full_layers)\n",
    "    K_eff = max(0, min(int(K), n_layers))\n",
    "\n",
    "    # lower-K prefill (model(...) 경유로도 가능하지만 여기선 빠른 진단 목적: inner 사용, no_grad)\n",
    "    lower = torch.nn.ModuleList([full_layers[i] for i in range(K_eff)])\n",
    "    inner.layers = lower\n",
    "    with torch.no_grad():\n",
    "        out_sys = inner(input_ids=sys_only, attention_mask=torch.ones_like(sys_only),\n",
    "                        use_cache=True, return_dict=True)\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    dc_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff>0 else DynamicCache()\n",
    "    with torch.no_grad():\n",
    "        out_low = inner(input_ids=ids_phase1[:, L_sys:], past_key_values=dc_in,\n",
    "                        attention_mask=None, use_cache=True, return_dict=True)\n",
    "    pkv_low = out_low.past_key_values\n",
    "    inner.layers = full_layers\n",
    "\n",
    "    combined = DynamicCache()\n",
    "    num_kv, head_dim, kv_dtype = _kv_meta_from_model(model)\n",
    "    for li in range(n_layers):\n",
    "        if li < K_eff:\n",
    "            k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_cat = torch.cat([k_sys[:, :, :L_sys, :], k_low[:, :, -L_doc:, :]], dim=2)\n",
    "            v_cat = torch.cat([v_sys[:, :, :L_sys, :], v_low[:, :, -L_doc:, :]], dim=2)\n",
    "        else:\n",
    "            k_cat, v_cat = _make_empty_kv(1, num_kv, head_dim, DEVICE, kv_dtype)\n",
    "        combined.update(k_cat.contiguous(), v_cat.contiguous(), li)\n",
    "\n",
    "    hdr_tail = tokens_from_messages(tokenizer, msgs, DEVICE, add_generation_prompt=True)[:, L_all:]\n",
    "    seed = hdr_tail if hdr_tail.numel() > 0 else ids_phase1[:, -1:]\n",
    "    return combined, seed, ids_hdr\n",
    "\n",
    "@torch.inference_mode()\n",
    "def ce_for_pair(tokenizer, model, q, d, resp, K):\n",
    "    combined, seed, ids_hdr = make_combined(tokenizer, model, SYSTEM, d, q, K)\n",
    "    msgs = build_messages(SYSTEM, d, q, include_query=True)\n",
    "    msgs_ass = msgs + [{\"role\":\"assistant\",\"content\":resp}]\n",
    "    full_ids = tokens_from_messages(tokenizer, msgs_ass, DEVICE, add_generation_prompt=False)\n",
    "    a = full_ids[:, ids_hdr.size(1):]\n",
    "    if a.numel() == 0:\n",
    "        return None\n",
    "    inp = torch.cat([seed, a], dim=1)\n",
    "    lab = inp.clone(); lab[:, :seed.size(1)] = -100\n",
    "    doc_len = L_sys + L_doc\n",
    "    with lopa_cache_position_patch_global(model, combined, doc_len):\n",
    "        out = model(input_ids=inp, past_key_values=combined, labels=lab, use_cache=True, return_dict=True)\n",
    "    return float(out.loss.item()) if out.loss is not None and torch.isfinite(out.loss) else None\n",
    "\n",
    "# 샘플링\n",
    "DATA_PATH = \"triviaqa_hotpotqa_6000_merged2.jsonl\"\n",
    "pairs = []\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        q = rec.get(\"question\",\"\").strip()\n",
    "        d = rec.get(\"document\",\"\").strip()\n",
    "        rs = rec.get(\"responses\",[])\n",
    "        if q and d and rs:\n",
    "            pairs.append((q,d,rs[0]))\n",
    "        if len(pairs) >= 24: break\n",
    "\n",
    "# Shuffled 문서 만들기\n",
    "docs = [d for _, d, _ in pairs]\n",
    "random.shuffle(docs)\n",
    "shuffled = [(q, d_shuf, r) for (q,_,r), d_shuf in zip(pairs, docs)]\n",
    "\n",
    "def run_block(title, items, K):\n",
    "    vals = []\n",
    "    for (q,d,r) in items:\n",
    "        v = ce_for_pair(tokenizer, model, q, d, r, K)\n",
    "        if v is not None: vals.append(v)\n",
    "    print(f\"{title} | K={K} | N={len(vals)} | mean CE={mean(vals):.6f}\")\n",
    "    return vals\n",
    "\n",
    "print(\"=== CE sanity ===\")\n",
    "_ = run_block(\"LoPA\", pairs, K=4)\n",
    "_ = run_block(\"Full \", pairs, K=9999)     # effectively all layers\n",
    "_ = run_block(\"Shuf \", shuffled, K=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel  # pip install peft\n",
    "from LatentCOMP_cleaned.infer_lopa_pure import lopa_generate, ensure_mistral_special_token, _get_inner_model\n",
    "\n",
    "repo_id = \"jeongseokoh/Llama-3.1-8B-Instruct-LOPA-partial4-0specials\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if (device==\"cuda\" and torch.cuda.is_bf16_supported()) else (torch.float16 if device==\"cuda\" else torch.float32)\n",
    "\n",
    "# Tokenizer (saved at repo root)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Base (under subfolder=base)\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id, subfolder=\"base\", trust_remote_code=False, torch_dtype=dtype)\n",
    "ensure_mistral_special_token(tokenizer, model)\n",
    "\n",
    "# Merge LoRA if present (under subfolder=lora)\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, repo_id, subfolder=\"lora\").merge_and_unload()\n",
    "except Exception as e:\n",
    "    print(f\"[warn] LoRA merge failed or missing, using base only: {e}\")\n",
    "\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Force eager attention (stability)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try:\n",
    "        setattr(model.config, k, \"eager\")\n",
    "        setattr(_get_inner_model(model).config, k, \"eager\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Fill these\n",
    "system = \"You are a helpful assistant that answers questions based on the given document. \"\n",
    "document = \"Replace with your full document text here.\"\n",
    "question = \"Replace with your question here.\"\n",
    "K = 4  # same as training (partial4)\n",
    "\n",
    "# Generate with LoPA\n",
    "text = lopa_generate(\n",
    "    model, tokenizer,\n",
    "    system=system, document=document, question=question,\n",
    "    K=K, device=device,\n",
    "    max_new_tokens=256, min_length=16,\n",
    "    temperature=0.7, top_p=0.9, top_k=None,\n",
    "    do_sample=True, debug=True,\n",
    ")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edcbf962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Transformers version: 4.56.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be99633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f61e08d60147ad9d30d2f3b7ccdcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 20:39:19,291] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::substr(unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_stringbuf_init(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_dispose(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::swap(std::__detail::_List_node_base&, std::__detail::_List_node_base&)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_mutate(unsigned long, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_next_bkt(unsigned long) const@GLIBCXX_3.4.18'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::end()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::flush<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::string const&, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::runtime_error(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(std::string const&) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 20:39:20,573] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "Rewrote clean base to: LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/base\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# 1) 경로와 원본 모델 ID 지정\n",
    "best_dir = \"LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\"\n",
    "base_dir = f\"{best_dir}/base\"\n",
    "orig_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # 학습에 쓴 원본 모델 이름\n",
    "\n",
    "# 2) 원본 베이스 모델 로드(캐시 사용), 저장\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    orig_model_id,\n",
    "    trust_remote_code=True,          # Llama3 계열이면 True 괜찮습니다\n",
    "    cache_dir=\"/data2/jeongseokoh/hub/model\"  # 학습 때 쓰던 캐시 경로가 있으면 지정\n",
    ")\n",
    "# base/config.json에 remote code 흔적이 섞이지 않도록 auto_map 클리어(있다면)\n",
    "try:\n",
    "    setattr(m.config, \"auto_map\", None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "m.save_pretrained(base_dir, safe_serialization=True)\n",
    "del m\n",
    "\n",
    "print(\"Rewrote clean base to:\", base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c22a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26170ccbacaf4356b8fe1991f271731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready to help. What would you like to know? Please go ahead and ask your question.\n"
     ]
    }
   ],
   "source": [
    "# LoPA (low-only prefill) scratch pipeline without trust_remote_code\n",
    "# Torch 2.5.1, Transformers 4.56.1, GPU+SDPA\n",
    "# - Phase1: lower-K only prefill (no assistant header)\n",
    "# - Phase2: generation (first feed assistant header tokens step-by-step)\n",
    "# - Positions unchanged; no remapping\n",
    "# - LoRA: best/lora attach → merge\n",
    "# - MinLength 수동 적용(4.56.1 디바이스 불일치 회피)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "K = 4  # prefill layers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (device == \"cuda\" and torch.cuda.is_bf16_supported())\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "cache_dir_model = \"/data2/jeongseokoh/hub/model\"\n",
    "cache_dir_tok   = \"/data2/jeongseokoh/hub/tokenizer\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (best_dir 우선)\n",
    "# -----------------------------\n",
    "tok_src = str(best_dir) if (best_dir / \"tokenizer.json\").is_file() else backbone_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir_tok, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone + LoRA (merge)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir=cache_dir_model,\n",
    ")\n",
    "base_model.to(device).eval()\n",
    "\n",
    "from peft import PeftModel\n",
    "assert lora_dir.is_dir(), f\"LoRA folder not found: {lora_dir}\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "model = peft_model.merge_and_unload().to(device).eval()\n",
    "del peft_model\n",
    "\n",
    "# SDPA 고정 (필요시 eager로 비교)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try: setattr(model.config, k, \"sdpa\")\n",
    "    except: pass\n",
    "try:\n",
    "    for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "        setattr(model.model.config, k, \"sdpa\")\n",
    "except: pass\n",
    "\n",
    "# -----------------------------\n",
    "# Cache helpers\n",
    "# -----------------------------\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    try: return len(pkv)\n",
    "    except: return 0\n",
    "\n",
    "def pkv_get(pkv, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)  # layer index 유지\n",
    "    return dc\n",
    "\n",
    "def cache_slice_doc_only(k: torch.Tensor, v: torch.Tensor, doc_len: int):\n",
    "    return k[:, :, -doc_len:, :], v[:, :, -doc_len:, :]\n",
    "\n",
    "def build_mask(length: int, batch: int = 1):\n",
    "    return torch.ones(batch, length, device=device, dtype=torch.long)\n",
    "\n",
    "def build_pos_ids(start: int, length: int, batch: int = 1):\n",
    "    return torch.arange(start, start + length, device=device, dtype=torch.long).unsqueeze(0).expand(batch, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt builders (Gen3)\n",
    "# -----------------------------\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    s = apply_chat_template(messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# LoPA generate (low-only prefill; no upper continue)\n",
    "# -----------------------------\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    system: str,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    K: int = 4,\n",
    "    max_new_tokens: int = 256,\n",
    "    min_length: int = 16,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: Optional[int] = None,\n",
    "    do_sample: bool = True,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    start_with_assistant_header: bool = True, # Phase2 첫 토큰: header를 단계적으로 투입\n",
    "    force_attn_impl: Optional[str] = None,   # \"sdpa\" | \"eager\"\n",
    "):\n",
    "    if force_attn_impl:\n",
    "        for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "            try:\n",
    "                setattr(model.config, k, force_attn_impl)\n",
    "                setattr(model.model.config, k, force_attn_impl)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Phase-1 ids\n",
    "    msgs = build_messages(system, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(msgs, add_generation_prompt=False)  # [1, L_sys+doc]\n",
    "    ids_hdr    = tokens_from_messages(msgs, add_generation_prompt=True)   # [1, L_sys+doc + header]\n",
    "    sys_only   = tokens_from_messages([{\"role\":\"system\",\"content\":system}], add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Phase-1 must include document tokens.\"\n",
    "\n",
    "    # 1) System-only prefill (base model)\n",
    "    out_sys = model.model(\n",
    "        input_ids=sys_only,\n",
    "        attention_mask=build_mask(L_sys),\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    n_layers = pkv_len(pkv_sys)\n",
    "\n",
    "    # 2) Lower-K doc pass (base model) — no upper continue\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "    full_layers: nn.ModuleList = model.model.layers\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "\n",
    "    model.model.layers = lower_layers\n",
    "    dc_low_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    attn_doc_full = torch.cat([build_mask(L_sys), build_mask(L_doc)], dim=1)\n",
    "\n",
    "    out_low = model.model(\n",
    "        input_ids=ids_phase1[:, L_sys:],  # doc only\n",
    "        past_key_values=dc_low_in,\n",
    "        attention_mask=attn_doc_full,     # sys+doc 길이\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # 복원\n",
    "    model.model.layers = full_layers\n",
    "\n",
    "    # 3) Combine caches\n",
    "    # lower(0..K-1): sys + doc\n",
    "    # upper(K..):    sys only  (doc 미포함; generation 시작 시점부터 쌓임)\n",
    "    combined = DynamicCache()\n",
    "    # 헤드/차원 shape 확보\n",
    "    k0_sys, v0_sys = pkv_get(pkv_sys, 0)\n",
    "    for li in range(n_layers):\n",
    "        k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "        k_sys = k_sys[:, :, :L_sys, :]\n",
    "        v_sys = v_sys[:, :, :L_sys, :]\n",
    "        if li < K_eff:\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_doc, v_doc = cache_slice_doc_only(k_low, v_low, L_doc)\n",
    "            k_cat = torch.cat([k_sys, k_doc], dim=2).contiguous()\n",
    "            v_cat = torch.cat([v_sys, v_doc], dim=2).contiguous()\n",
    "            combined.update(k_cat, v_cat, li)\n",
    "        else:\n",
    "            # upper: sys only\n",
    "            combined.update(k_sys.contiguous(), v_sys.contiguous(), li)\n",
    "\n",
    "    # 4) Phase-2: Generation\n",
    "    # seed: assistant header를 한 토큰씩 밀어 넣어 upper past가 L_sys → L_sys+H로 자라도록 함\n",
    "    if start_with_assistant_header:\n",
    "        hdr_tail = ids_hdr[:, L_all:]  # header-only tokens (len H)\n",
    "        H = int(hdr_tail.size(1))\n",
    "        for j in range(H):\n",
    "            past_len = pkv_get(combined, 0)[0].shape[2]  # lower 기준: L_sys+L_doc+grown\n",
    "            attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "            step_tok = hdr_tail[:, j:j+1]\n",
    "            out_seed = model(\n",
    "                input_ids=step_tok,\n",
    "                past_key_values=combined,\n",
    "                attention_mask=attn_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            combined = out_seed.past_key_values\n",
    "\n",
    "    # 5) Decoding (CausalLM) with safe processors\n",
    "    from transformers.generation import LogitsProcessorList\n",
    "    from transformers.generation.logits_process import (\n",
    "        TemperatureLogitsWarper, TopPLogitsWarper, TopKLogitsWarper,\n",
    "        RepetitionPenaltyLogitsProcessor,\n",
    "    )\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    generated_ids = torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    # 첫 생성 스텝의 입력은 \"직전 토큰\" (header 마지막 또는 마지막 doc)\n",
    "    last = (ids_hdr[:, -1:] if start_with_assistant_header else ids_phase1[:, -1:])\n",
    "\n",
    "    processors = LogitsProcessorList()\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        processors.append(RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty)))\n",
    "    if temperature and temperature != 1.0:\n",
    "        processors.append(TemperatureLogitsWarper(temperature=float(temperature)))\n",
    "    if top_k is not None and top_k > 0:\n",
    "        processors.append(TopKLogitsWarper(top_k=int(top_k), filter_value=-float(\"inf\")))\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        processors.append(TopPLogitsWarper(top_p=float(top_p), min_tokens_to_keep=1))\n",
    "\n",
    "    cur = 0\n",
    "    while cur < max_new_tokens:\n",
    "        # lower past_len은 L_sys+L_doc + grown; upper past_len은 L_sys + grown\n",
    "        past_len = pkv_get(combined, 0)[0].shape[2]\n",
    "        attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=last,\n",
    "            past_key_values=combined,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        combined = out.past_key_values\n",
    "        logits = out.logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "        # 즉시 EOS 방지 (min_length 수동 적용)\n",
    "        if eos_id is not None and cur < min_length:\n",
    "            logits[:, eos_id] = -float(\"inf\")\n",
    "\n",
    "        if not torch.isfinite(logits).all():\n",
    "            logits = torch.nan_to_num(logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        inp = generated_ids if generated_ids.numel() > 0 else last.new_zeros((1, 0), dtype=torch.long, device=device)\n",
    "        inp = inp.to(logits.device)\n",
    "        logits = processors(inp, logits)\n",
    "\n",
    "        # Fallback-safe sampling\n",
    "        if not torch.isfinite(logits).any():\n",
    "            next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if do_sample:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                if (not torch.isfinite(probs).all()) or (probs.sum(dim=-1) <= 0).any():\n",
    "                    next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if eos_id is not None and int(next_tok.item()) == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_tok], dim=1)\n",
    "        last = next_tok\n",
    "        cur += 1\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "answer = lopa_generate(\n",
    "    system, document, question,\n",
    "    K=K,\n",
    "    max_new_tokens=256,\n",
    "    min_length=16,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    start_with_assistant_header=True,  # Phase2 처음에 header를 단계적으로 투입\n",
    "    # force_attn_impl=\"eager\",         # 필요시 비교\n",
    ")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ff3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcead6b3d56471c87aadf12086bbd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lens] L_sys=40, L_doc=35, L_all=75, header_len=4\n",
      "[Layers] n_layers=32, K_eff=4\n",
      "[sys] n_layers=32 | L0:K(1, 8, 40, 128) | L1:K(1, 8, 40, 128) | L31:K(1, 8, 40, 128)\n",
      "[OK]   system_prefill: all present layers seq_len == 40\n",
      "[low] h_low=(1, 35, 4096)\n",
      "[low] n_layers=4 | L0:K(1, 8, 75, 128) | L1:K(1, 8, 75, 128) | L3:K(1, 8, 75, 128)\n",
      "[up] n_layers=32 | L4:K(1, 8, 75, 128) | L5:K(1, 8, 75, 128) | L31:K(1, 8, 75, 128)\n",
      "[combined(before_hdr)] n_layers=32 | L0:K(1, 8, 75, 128) | L1:K(1, 8, 75, 128) | L31:K(1, 8, 75, 128)\n",
      "[OK]   combined_prefill: all present layers seq_len == 75\n",
      "[hdr] past_len before=75, after=79, delta=4\n",
      "[combined(after_hdr)] n_layers=32 | L0:K(1, 8, 79, 128) | L1:K(1, 8, 79, 128) | L31:K(1, 8, 79, 128)\n",
      "[decode step 0] past_len 79 -> 80 | next_id=40\n",
      "[decode step 1] past_len 80 -> 81 | next_id=2846\n",
      "[decode step 2] past_len 81 -> 82 | next_id=539\n",
      "[debug answer] I'm not able to answer your question as I don't have any information about a document. Can you please provide the document or more context about the question you are trying to ask?\n",
      "I'm not able to answer your question since you didn't provide a document. Please provide the document, and I'll be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "# LoPA (low-only prefill) scratch pipeline without trust_remote_code\n",
    "# Torch 2.5.1, Transformers 4.56.1, GPU+SDPA\n",
    "# - Phase1: lower-K only prefill (no assistant header)\n",
    "# - Phase2: generation (first feed assistant header tokens step-by-step)\n",
    "# - Positions unchanged; no remapping\n",
    "# - LoRA: best/lora attach → merge\n",
    "# - MinLength 수동 적용(4.56.1 디바이스 불일치 회피)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "K = 4  # prefill layers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (device == \"cuda\" and torch.cuda.is_bf16_supported())\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "cache_dir_model = \"/data2/jeongseokoh/hub/model\"\n",
    "cache_dir_tok   = \"/data2/jeongseokoh/hub/tokenizer\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (best_dir 우선)\n",
    "# -----------------------------\n",
    "tok_src = str(best_dir) if (best_dir / \"tokenizer.json\").is_file() else backbone_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir_tok, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone + LoRA (merge)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir=cache_dir_model,\n",
    ")\n",
    "base_model.to(device).eval()\n",
    "\n",
    "from peft import PeftModel\n",
    "assert lora_dir.is_dir(), f\"LoRA folder not found: {lora_dir}\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "model = peft_model.merge_and_unload().to(device).eval()\n",
    "del peft_model\n",
    "\n",
    "# SDPA 고정 (필요시 eager로 비교)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try: setattr(model.config, k, \"sdpa\")\n",
    "    except: pass\n",
    "try:\n",
    "    for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "        setattr(model.model.config, k, \"sdpa\")\n",
    "except: pass\n",
    "\n",
    "# -----------------------------\n",
    "# Cache helpers\n",
    "# -----------------------------\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    try: return len(pkv)\n",
    "    except: return 0\n",
    "\n",
    "def pkv_get(pkv, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)  # layer index 유지\n",
    "    return dc\n",
    "\n",
    "def cache_slice_doc_only(k: torch.Tensor, v: torch.Tensor, doc_len: int):\n",
    "    return k[:, :, -doc_len:, :], v[:, :, -doc_len:, :]\n",
    "\n",
    "def build_mask(length: int, batch: int = 1):\n",
    "    return torch.ones(batch, length, device=device, dtype=torch.long)\n",
    "\n",
    "def build_pos_ids(start: int, length: int, batch: int = 1):\n",
    "    return torch.arange(start, start + length, device=device, dtype=torch.long).unsqueeze(0).expand(batch, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt builders (Gen3)\n",
    "# -----------------------------\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    s = apply_chat_template(messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# LoPA generate (low-only prefill; no upper continue)\n",
    "# -----------------------------\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    system: str,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    K: int = 4,\n",
    "    max_new_tokens: int = 256,\n",
    "    min_length: int = 16,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: Optional[int] = None,\n",
    "    do_sample: bool = True,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    start_with_assistant_header: bool = True, # Phase2 첫 토큰: header를 단계적으로 투입\n",
    "    force_attn_impl: Optional[str] = None,   # \"sdpa\" | \"eager\"\n",
    "):\n",
    "    if force_attn_impl:\n",
    "        for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "            try:\n",
    "                setattr(model.config, k, force_attn_impl)\n",
    "                setattr(model.model.config, k, force_attn_impl)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Phase-1 ids\n",
    "    msgs = build_messages(system, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(msgs, add_generation_prompt=False)  # [1, L_sys+doc]\n",
    "    ids_hdr    = tokens_from_messages(msgs, add_generation_prompt=True)   # [1, L_sys+doc + header]\n",
    "    sys_only   = tokens_from_messages([{\"role\":\"system\",\"content\":system}], add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Phase-1 must include document tokens.\"\n",
    "\n",
    "    # 1) System-only prefill (base model)\n",
    "    out_sys = model.model(\n",
    "        input_ids=sys_only,\n",
    "        attention_mask=build_mask(L_sys),\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    n_layers = pkv_len(pkv_sys)\n",
    "\n",
    "    # 2) Lower-K doc pass (base model) — no upper continue\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "    full_layers: nn.ModuleList = model.model.layers\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "\n",
    "    model.model.layers = lower_layers\n",
    "    dc_low_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    attn_doc_full = torch.cat([build_mask(L_sys), build_mask(L_doc)], dim=1)\n",
    "\n",
    "    out_low = model.model(\n",
    "        input_ids=ids_phase1[:, L_sys:],  # doc only\n",
    "        past_key_values=dc_low_in,\n",
    "        attention_mask=attn_doc_full,     # sys+doc 길이\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # 복원\n",
    "    model.model.layers = full_layers\n",
    "\n",
    "    # 3) Combine caches\n",
    "    # lower(0..K-1): sys + doc\n",
    "    # upper(K..):    sys only  (doc 미포함; generation 시작 시점부터 쌓임)\n",
    "    combined = DynamicCache()\n",
    "    # 헤드/차원 shape 확보\n",
    "    k0_sys, v0_sys = pkv_get(pkv_sys, 0)\n",
    "    for li in range(n_layers):\n",
    "        k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "        k_sys = k_sys[:, :, :L_sys, :]\n",
    "        v_sys = v_sys[:, :, :L_sys, :]\n",
    "        if li < K_eff:\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_doc, v_doc = cache_slice_doc_only(k_low, v_low, L_doc)\n",
    "            k_cat = torch.cat([k_sys, k_doc], dim=2).contiguous()\n",
    "            v_cat = torch.cat([v_sys, v_doc], dim=2).contiguous()\n",
    "            combined.update(k_cat, v_cat, li)\n",
    "        else:\n",
    "            # upper: sys only\n",
    "            combined.update(k_sys.contiguous(), v_sys.contiguous(), li)\n",
    "\n",
    "    # 4) Phase-2: Generation\n",
    "    # seed: assistant header를 한 토큰씩 밀어 넣어 upper past가 L_sys → L_sys+H로 자라도록 함\n",
    "    if start_with_assistant_header:\n",
    "        hdr_tail = ids_hdr[:, L_all:]  # header-only tokens (len H)\n",
    "        H = int(hdr_tail.size(1))\n",
    "        for j in range(H):\n",
    "            past_len = pkv_get(combined, 0)[0].shape[2]  # lower 기준: L_sys+L_doc+grown\n",
    "            attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "            step_tok = hdr_tail[:, j:j+1]\n",
    "            out_seed = model(\n",
    "                input_ids=step_tok,\n",
    "                past_key_values=combined,\n",
    "                attention_mask=attn_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            combined = out_seed.past_key_values\n",
    "\n",
    "    # 5) Decoding (CausalLM) with safe processors\n",
    "    from transformers.generation import LogitsProcessorList\n",
    "    from transformers.generation.logits_process import (\n",
    "        TemperatureLogitsWarper, TopPLogitsWarper, TopKLogitsWarper,\n",
    "        RepetitionPenaltyLogitsProcessor,\n",
    "    )\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    generated_ids = torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    # 첫 생성 스텝의 입력은 \"직전 토큰\" (header 마지막 또는 마지막 doc)\n",
    "    last = (ids_hdr[:, -1:] if start_with_assistant_header else ids_phase1[:, -1:])\n",
    "\n",
    "    processors = LogitsProcessorList()\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        processors.append(RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty)))\n",
    "    if temperature and temperature != 1.0:\n",
    "        processors.append(TemperatureLogitsWarper(temperature=float(temperature)))\n",
    "    if top_k is not None and top_k > 0:\n",
    "        processors.append(TopKLogitsWarper(top_k=int(top_k), filter_value=-float(\"inf\")))\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        processors.append(TopPLogitsWarper(top_p=float(top_p), min_tokens_to_keep=1))\n",
    "\n",
    "    cur = 0\n",
    "    while cur < max_new_tokens:\n",
    "        # lower past_len은 L_sys+L_doc + grown; upper past_len은 L_sys + grown\n",
    "        past_len = pkv_get(combined, 0)[0].shape[2]\n",
    "        attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=last,\n",
    "            past_key_values=combined,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        combined = out.past_key_values\n",
    "        logits = out.logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "        # 즉시 EOS 방지 (min_length 수동 적용)\n",
    "        if eos_id is not None and cur < min_length:\n",
    "            logits[:, eos_id] = -float(\"inf\")\n",
    "\n",
    "        if not torch.isfinite(logits).all():\n",
    "            logits = torch.nan_to_num(logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        inp = generated_ids if generated_ids.numel() > 0 else last.new_zeros((1, 0), dtype=torch.long, device=device)\n",
    "        inp = inp.to(logits.device)\n",
    "        logits = processors(inp, logits)\n",
    "\n",
    "        # Fallback-safe sampling\n",
    "        if not torch.isfinite(logits).any():\n",
    "            next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if do_sample:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                if (not torch.isfinite(probs).all()) or (probs.sum(dim=-1) <= 0).any():\n",
    "                    next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if eos_id is not None and int(next_tok.item()) == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_tok], dim=1)\n",
    "        last = next_tok\n",
    "        cur += 1\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "answer = lopa_generate(\n",
    "    system, document, question,\n",
    "    K=K,\n",
    "    max_new_tokens=256,\n",
    "    min_length=16,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    start_with_assistant_header=True,  # Phase2 처음에 header를 단계적으로 투입\n",
    "    # force_attn_impl=\"eager\",         # 필요시 비교\n",
    ")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7acf95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1346d627656d4138b818cc49145abfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant that answers questions based on the given document.user\\n\\nDocument:\\nThe Nile is the longest river in Africa...\\n\\nQuestion: Which continent is the Nile the longest river in?assistant\\n\\nThe Nile is the longest river in Africa.']\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_dir = \"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\"\n",
    "prefill_layers = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sys.path.insert(0, model_dir)  # best/ 를 import 경로에 추가\n",
    "from modeling_partial_layer import LlamaForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir, device_map=\"cuda:0\", dtype=torch.bfloat16, trust_remote_code=True).to(device).eval()\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    setattr(model.config, k, \"sdpa\")\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "out = model.generate(\n",
    "    system=system, document=document, query=question,\n",
    "    compress=False, tokenizer=tok, prefill_layers=prefill_layers,\n",
    "    max_new_tokens=256, do_sample=False, temperature=0.7, top_p=0.9\n",
    ")\n",
    "print(tok.batch_decode(out, skip_special_tokens=True))  # 문자열 또는 문자열 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786d4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_available=True | attached=True | merged=False | is_sharded=True | source=/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best/lora | error=None | last_used_lora=True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LATENTRAG_DEBUG\"]=\"1\"\n",
    "print(model.lora_debug_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89bed7b41e444fab4a660167f1660c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da2aecd742744a68944483b961a633b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant<|end_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Africa.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 경로 설정 (혼동 방지를 위해 절대경로 권장)\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "prefill_layers = 4  # 학습 시 사용한 값과 동일하게\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# best/의 remote-code(LOPA 구현)를 import할 수 있게 경로 추가\n",
    "sys.path.insert(0, str(best_dir))\n",
    "from modeling_partial_layer import LlamaForCausalLM  # Llama 3.1 계열용\n",
    "\n",
    "# 토크나이저: chat template 포함\n",
    "tok = AutoTokenizer.from_pretrained(str(best_dir))\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# 1) 백본을 허깅페이스에서 ‘깨끗하게’ 로드 (백본=llama3.1 8B instruct)\n",
    "#    주의: trust_remote_code=False → 표준 HF 클래스 경로를 통해 가중치만 로드\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=False,  # 가중치만 가져오고 동작은 our remote-code 클래스가 담당\n",
    "    cache_dir=\"/data2/jeongseokoh/hub/model\",\n",
    ").to(device).eval()\n",
    "\n",
    "# 2) LoRA 어댑터 attach 후 merge (단일 모델로)\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "merged = peft_model.merge_and_unload().to(device).eval()\n",
    "\n",
    "# 3) LOPA 추론 (compress=True → partial prefill)\n",
    "#    이미 merge된 모델이므로 추가 attach를 막기 위해 use_lora=False로 고정\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "out = merged.generate(\n",
    "    system=system,\n",
    "    document=document,\n",
    "    query=question,\n",
    "    compress=True,                 # LOPA 경로\n",
    "    tokenizer=tok,                 # 필수\n",
    "    prefill_layers=prefill_layers, # 학습과 동일\n",
    "    use_lora=False,                # 이미 merge됐으므로 재-부착 방지\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(out)  # 문자열 또는 문자열 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913061c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
