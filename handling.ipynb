{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536499ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device=cuda, DTYPE=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Notebook 1 ‚Äî Env & Paths\n",
    "from pathlib import Path\n",
    "import os, torch\n",
    "\n",
    "# ÏïàÏ†Ñ: tokenizer Î©ÄÌã∞Ïä§Î†àÎìú Ïù¥Ïäà Î∞©ÏßÄ\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# ÏàòÏ†ïÎêú Î™®Îç∏ÎßÅ ÌååÏùº Í≤ΩÎ°ú (Ïù¥ÎØ∏ ÏïûÏÑú Ï†ÄÏû•/ÍµêÏ≤¥ÌñàÎã§Í≥† Í∞ÄÏ†ï)\n",
    "LOPA_MODELING_PATH = Path(\"lopa_llama_modeling.py\").resolve()\n",
    "assert LOPA_MODELING_PATH.exists(), f\"‚ùå modeling file not found: {LOPA_MODELING_PATH}\"\n",
    "\n",
    "# ÌïôÏäµ ÏÇ∞Ï∂úÎ¨º Í≤ΩÎ°ú (ÏòàÏãú)\n",
    "BEST_DIR  = Path(\"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\")\n",
    "BASE_DIR  = BEST_DIR / \"base\"   # base Í∞ÄÏ§ëÏπò\n",
    "LORA_DIR  = BEST_DIR / \"lora\"   # LoRA Ïñ¥ÎåëÌÑ∞(ÏûàÏúºÎ©¥ ÏÇ¨Ïö©)\n",
    "assert BASE_DIR.exists(), f\"‚ùå Base dir not found: {BASE_DIR}\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    major_cc = torch.cuda.get_device_capability(0)[0]\n",
    "    DTYPE = torch.bfloat16 if major_cc >= 8 else torch.float16\n",
    "else:\n",
    "    DTYPE = torch.float32\n",
    "\n",
    "print(f\"Device={DEVICE}, DTYPE={DTYPE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de428193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded custom LoPA modeling into transformers.models.llama.modeling_llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057ab207b1304b048c9ddd352a42434f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base weights.\n",
      "‚úÖ LoRA merged for inference\n",
      "Model dtype: torch.bfloat16 | device: cuda:0\n",
      "TOTAL_LAYERS: 32\n"
     ]
    }
   ],
   "source": [
    "# Notebook 2 ‚Äî Load custom modeling, tokenizer, model\n",
    "import importlib.util, sys, transformers, transformers.models.llama\n",
    "\n",
    "def load_custom_llama_modeling(modeling_path: Path):\n",
    "    target_name = \"transformers.models.llama.modeling_llama\"\n",
    "    if target_name in sys.modules:\n",
    "        del sys.modules[target_name]\n",
    "    spec = importlib.util.spec_from_file_location(target_name, str(modeling_path))\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise RuntimeError(f\"Failed to load spec for {modeling_path}\")\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[target_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    # Í∞ÑÎã® Í≤ÄÏ¶ù\n",
    "    for klass in (\"LlamaModel\", \"LlamaForCausalLM\"):\n",
    "        assert hasattr(module, klass), f\"{klass} missing in {modeling_path}\"\n",
    "    return module\n",
    "\n",
    "_ = load_custom_llama_modeling(LOPA_MODELING_PATH)\n",
    "print(\"‚úÖ Loaded custom LoPA modeling into transformers.models.llama.modeling_llama\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "# tokenizer: best Î£®Ìä∏(ÏóÜÏúºÎ©¥ base)\n",
    "tok_src = BEST_DIR if (BEST_DIR / \"tokenizer_config.json\").exists() else BASE_DIR\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, use_fast=True)\n",
    "if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# base Î™®Îç∏ Î°úÎìú\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_DIR,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"Loaded base weights.\")\n",
    "\n",
    "# LoRA Î≥ëÌï©(ÏûàÏúºÎ©¥)\n",
    "if LORA_DIR.exists():\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, LORA_DIR)\n",
    "        model = model.merge_and_unload()\n",
    "        print(\"‚úÖ LoRA merged for inference\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LoRA merge failed, using wrapped model: {e}\")\n",
    "\n",
    "model.eval()\n",
    "DEVICE = next(model.parameters()).device\n",
    "print(\"Model dtype:\", next(model.parameters()).dtype, \"| device:\", DEVICE)\n",
    "\n",
    "# Ï∞∏Ï°∞Ïö©: Ï†ÑÏ≤¥ Î†àÏù¥Ïñ¥ Ïàò\n",
    "TOTAL_LAYERS = model.config.num_hidden_layers\n",
    "print(\"TOTAL_LAYERS:\", TOTAL_LAYERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c124863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 3 ‚Äî Helpers\n",
    "import torch\n",
    "\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Question: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "def apply_chat_template(tokenizer, messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        tmpl = getattr(tokenizer, \"chat_template\", \"\") or \"\"\n",
    "        if add_generation_prompt and \"<|start_header_id|>\" in tmpl:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(tokenizer, messages, device, add_generation_prompt=False):\n",
    "    s = apply_chat_template(tokenizer, messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "def sample_from_logits(logits, *, do_sample=True, temperature=0.7, top_p=0.9,\n",
    "                       repetition_penalty=1.15, generated_ids=None):\n",
    "    \"\"\" nucleus + repetition penalty + temperature \"\"\"\n",
    "    if repetition_penalty != 1.0 and generated_ids is not None and generated_ids.numel() > 0:\n",
    "        uniq = torch.unique(generated_ids)\n",
    "        # (logit>0)/penalty, (logit<0)*penalty\n",
    "        gathered = logits.index_select(dim=-1, index=uniq)\n",
    "        gathered = torch.where(gathered > 0, gathered / repetition_penalty, gathered * repetition_penalty)\n",
    "        logits.scatter_(dim=-1, index=uniq.unsqueeze(0), src=gathered)\n",
    "\n",
    "    if not do_sample:\n",
    "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "    logits = logits / max(1e-6, temperature)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cumsum > top_p\n",
    "        mask[..., 1:] = mask[..., :-1].clone()\n",
    "        mask[..., 0] = False\n",
    "        sorted_probs[mask] = 0.0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "        next_local = torch.multinomial(sorted_probs, num_samples=1)\n",
    "        next_id = sorted_idx.gather(-1, next_local)\n",
    "    else:\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "    return next_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c8d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ helper: rope_modeÎ•º ÏûÑÏãúÎ°ú Î∞îÍø®Îã§Í∞Ä Î≥µÏõê\n",
    "import contextlib, torch\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _tmp_rope_mode(model, mode: str):\n",
    "    inner = getattr(model, \"model\", model)\n",
    "    prev = getattr(inner, \"lopa_rope_mode\", \"local\")\n",
    "    try:\n",
    "        inner.lopa_rope_mode = str(mode)\n",
    "        yield\n",
    "    finally:\n",
    "        inner.lopa_rope_mode = prev\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ patched lopa_generate: \"fast_global\" ÏßÄÏõê\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    system_prompt: str = \"You are a helpful assistant that answers questions based on the given document. \",\n",
    "    K: int = 8,\n",
    "    max_new_tokens: int = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = True,\n",
    "    stop_on_eos: bool = True,\n",
    "    repetition_penalty: float = 1.15,\n",
    "    seed_debug: bool = False,\n",
    "    step_debug: bool = False,\n",
    "    explicit_empty_upper: bool = False,\n",
    "    rope_mode: str = \"local\",       # ‚Üê \"local\" | \"global\" | \"fast_global\"\n",
    "    zero_pad_prefix: bool = False,  # ‚Üê globalÏóêÏÑúÎßå ÏùòÎØ∏ ÏûàÏùå; fast_globalÏóêÏÑúÎäî Î¨¥Ïãú\n",
    "):\n",
    "    if rope_mode not in (\"local\", \"global\", \"fast_global\"):\n",
    "        raise ValueError('rope_mode must be \"local\", \"global\", or \"fast_global\"')\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    inner = getattr(model, \"model\", model)\n",
    "    cfg = getattr(model, \"config\", None)\n",
    "\n",
    "    # 1) Î©îÏãúÏßÄ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï¶à\n",
    "    msgs = build_messages(system_prompt, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(tokenizer, msgs, device, add_generation_prompt=False)\n",
    "    ids_hdr    = tokens_from_messages(tokenizer, msgs, device, add_generation_prompt=True)\n",
    "\n",
    "    # 2) ÌïòÏ∏µ K Î†àÏù¥Ïñ¥Îßå prefill\n",
    "    pref = inner.lopa_prefill_lower_k(input_ids=ids_phase1, lower_k=K, use_cache=True)\n",
    "    lower_cache = pref.past_key_values\n",
    "    L_all = lower_cache.get_seq_length()\n",
    "\n",
    "    # 3) upper Ï∫êÏãú Ìï©ÏÑ±\n",
    "    if rope_mode == \"global\":\n",
    "        if zero_pad_prefix:\n",
    "            # Ï†ïÌôïÌïú zero-pad Îì±Í∞Ä: upperÏóê L_all Í∏∏Ïù¥ 0-KV\n",
    "            combined = inner.lopa_build_zero_padded_cache(\n",
    "                lower_cache, lower_k=K, batch_size=ids_phase1.size(0),\n",
    "                device=ids_phase1.device, zero_len=L_all\n",
    "            )\n",
    "        elif explicit_empty_upper:\n",
    "            combined = inner.lopa_build_combined_cache(\n",
    "                lower_cache, lower_k=K, batch_size=ids_phase1.size(0), device=ids_phase1.device\n",
    "            )\n",
    "        else:\n",
    "            combined = lower_cache\n",
    "    elif rope_mode == \"fast_global\":\n",
    "        if zero_pad_prefix:\n",
    "            print(\"[warn] zero_pad_prefix=True Îäî fast_globalÏóêÏÑú Î¨¥ÏãúÎê©ÎãàÎã§ (virtual zeroÎ°ú ÎèôÏπò Î≥¥Ï†ï).\")\n",
    "        # fast_globalÏùÄ zero-padÎ•º ÎßåÎì§ ÌïÑÏöî ÏóÜÏùå\n",
    "        if explicit_empty_upper:\n",
    "            # ÎîîÎ≤ÑÍπÖ Í∞ÄÎèÖÏÑ±Ïö©ÏúºÎ°ú ÏõêÌïòÎ©¥ Îπà KVÎ•º Î™ÖÏãúÌï† Ïàò ÏûàÏùå(ÌïÑÏàò ÏïÑÎãò)\n",
    "            combined = inner.lopa_build_combined_cache(\n",
    "                lower_cache, lower_k=K, batch_size=ids_phase1.size(0), device=ids_phase1.device\n",
    "            )\n",
    "        else:\n",
    "            combined = lower_cache\n",
    "    else:  # local\n",
    "        if explicit_empty_upper:\n",
    "            combined = inner.lopa_build_combined_cache(\n",
    "                lower_cache, lower_k=K, batch_size=ids_phase1.size(0), device=ids_phase1.device\n",
    "            )\n",
    "        else:\n",
    "            combined = lower_cache\n",
    "\n",
    "    # 4) seed ‚Üí Ï≤´ Î°úÏßì\n",
    "    seed_ids = ids_hdr[:, L_all:] if ids_hdr.size(1) > L_all else ids_phase1[:, -1:]\n",
    "\n",
    "    if seed_debug:\n",
    "        print(f\"[seed] L_all={L_all}, seed_len={seed_ids.size(1)}\")\n",
    "\n",
    "    with _tmp_rope_mode(model, rope_mode):\n",
    "        seed_out = model.lopa_step_logits(\n",
    "            input_ids=seed_ids,\n",
    "            prefix_len=L_all,\n",
    "            past_key_values=combined,\n",
    "            attention_mask_total_len=L_all + seed_ids.size(1),\n",
    "            logits_to_keep=1,\n",
    "            labels=None,\n",
    "        )\n",
    "    logits = seed_out.logits[:, -1, :]\n",
    "    pkv = seed_out.past_key_values\n",
    "    total_len = L_all + seed_ids.size(1)\n",
    "\n",
    "    # 5) Ï≤´ ÌÜ†ÌÅ∞\n",
    "    generated = []\n",
    "    next_id = sample_from_logits(\n",
    "        logits, do_sample=do_sample, temperature=temperature, top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty, generated_ids=None\n",
    "    )\n",
    "    generated.append(next_id)\n",
    "\n",
    "    eos_ids = set(t for t in [tokenizer.eos_token_id, getattr(tokenizer, \"eot_token_id\", None)] if t is not None)\n",
    "    if stop_on_eos and int(next_id[0, 0]) in eos_ids:\n",
    "        gen_ids = torch.cat(generated, dim=1)\n",
    "        return tokenizer.decode(gen_ids[0], skip_special_tokens=True), gen_ids\n",
    "\n",
    "    last = next_id\n",
    "\n",
    "    # 6) ÎîîÏΩîÎî© Î£®ÌîÑ\n",
    "    for _ in range(max_new_tokens - 1):\n",
    "        if step_debug:\n",
    "            print(\"total_len(before):\", total_len)\n",
    "\n",
    "        with _tmp_rope_mode(model, rope_mode):\n",
    "            step_out = model.lopa_step_logits(\n",
    "                input_ids=last,\n",
    "                prefix_len=total_len,                      # Ïù¥Î≤à ÌÜ†ÌÅ∞Ïùò Ï†àÎåÄ ÏãúÏûë ÏúÑÏπò\n",
    "                past_key_values=pkv,\n",
    "                attention_mask_total_len=total_len + 1,    # Ìïú ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä ÌõÑ Ï¥ù Í∏∏Ïù¥\n",
    "                logits_to_keep=1,\n",
    "                labels=None,\n",
    "            )\n",
    "        logits = step_out.logits[:, -1, :]\n",
    "        pkv = step_out.past_key_values\n",
    "        total_len += 1\n",
    "\n",
    "        next_id = sample_from_logits(\n",
    "            logits, do_sample=do_sample, temperature=temperature, top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            generated_ids=torch.cat(generated, dim=1)\n",
    "        )\n",
    "        generated.append(next_id)\n",
    "        last = next_id\n",
    "        if stop_on_eos and int(next_id[0, 0]) in eos_ids:\n",
    "            break\n",
    "\n",
    "    gen_ids = torch.cat(generated, dim=1) if generated else torch.zeros((1, 0), dtype=torch.long, device=device)\n",
    "    text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    return text, gen_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7bd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] zero_pad_prefix=True Îäî fast_globalÏóêÏÑú Î¨¥ÏãúÎê©ÎãàÎã§ (virtual zeroÎ°ú ÎèôÏπò Î≥¥Ï†ï).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how much she earned, we need to look at the information provided in the document. However, since you didn't provide any specific details about her earnings, I'll have to make an assumption.\n",
      "\n",
      "Let's assume that the question is asking for a general idea of what she might earn based on the context given. In this case, the document doesn't mention anything about her salary or income directly. \n",
      "\n",
      "However, if we were to consider a more hypothetical scenario where she was working as a writer or editor (given the context), here are some rough estimates:\n",
      "\n",
      "- According to the Bureau of Labor Statistics, the median annual wage for writers and authors in 2020 was around $67,120.\n",
      "- For editors, it was around $61,370 per year.\n",
      "\n",
      "Please note these figures are just examples and may not be relevant to your specific situation. If you could provide more context or clarify which \"she\" refers to, I'd be happy to try and help further!\n"
     ]
    }
   ],
   "source": [
    "# Notebook 5 ‚Äî Run simple example\n",
    "doc = \"\"\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting.\"\"\"\n",
    "q   = \"How much did she earn? Let's think step by step. \"\n",
    "txt, _ = lopa_generate(model, tokenizer, document=doc, question=q,\n",
    "                       K=8, do_sample=False, rope_mode=\"fast_global\", zero_pad_prefix=True)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd317d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[seed] L_all=88, seed_len=4\n",
      "li | past | global_start | off | local_start\n",
      "[00]   88 |   92 |   +4 |   88\n",
      "[01]   88 |   92 |   +4 |   88\n",
      "[02]   88 |   92 |   +4 |   88\n",
      "[03]   88 |   92 |   +4 |   88\n",
      "[04]   88 |   92 |   +4 |   88\n",
      "[05]   88 |   92 |   +4 |   88\n",
      "[06]   88 |   92 |   +4 |   88\n",
      "[07]   88 |   92 |   +4 |   88\n",
      "[28]    0 |   92 |  +92 |    0\n",
      "[29]    0 |   92 |  +92 |    0\n",
      "[30]    0 |   92 |  +92 |    0\n",
      "[31]    0 |   92 |  +92 |    0\n",
      "Greedy Answer:\n",
      " The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The\n",
      "\n",
      "Sampled Answer:\n",
      " To answer this question, To find the answer to the given information provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided provided\n"
     ]
    }
   ],
   "source": [
    "# Notebook 5 ‚Äî Run simple example\n",
    "doc = \"\"\"The Nile is a major north-flowing river in northeastern Africa, widely regarded as the longest river in the world...\"\"\"\n",
    "q   = \"Which continent is the Nile river located in? Let's think step by step. \"\n",
    "\n",
    "# Ïû¨ÌòÑÏÑ±(ÌÉêÏöïÏùÄ ÏùòÎØ∏ ÏóÜÏùå, ÏÉòÌîåÎßÅÏãúÎßå ÏòÅÌñ•)\n",
    "torch.manual_seed(0)\n",
    "model.model.lopa_rope_mode = \"global\"\n",
    "# Greedy (Î∞òÎ≥µ Î∂ïÍ¥¥ ÏßÑÎã®Ïóê Ï¢ãÏùå)\n",
    "answer_greedy, _ = lopa_generate(\n",
    "    model, tokenizer, document=doc, question=q,\n",
    "    K=8, max_new_tokens=64,\n",
    "    do_sample=False, stop_on_eos=True,\n",
    "    seed_debug=True, step_debug=False,   # ÌïÑÏöîÏãú TrueÎ°ú\n",
    "    explicit_empty_upper=False\n",
    ")\n",
    "print(\"Greedy Answer:\\n\", answer_greedy)\n",
    "\n",
    "# Sampling\n",
    "answer_sample, _ = lopa_generate(\n",
    "    model, tokenizer, document=doc, question=q,\n",
    "    K=8, max_new_tokens=64,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    do_sample=True, stop_on_eos=True,\n",
    "    repetition_penalty=1.15,\n",
    "    seed_debug=False, step_debug=False\n",
    ")\n",
    "print(\"\\nSampled Answer:\\n\", answer_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a48de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== K=8 ===\n",
      "The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The\n",
      "\n",
      "=== K=32 ===\n",
      "To answer the question, we need to look at the given information. \n",
      "\n",
      "Step 1: The Nile river is described as being in northeastern Africa.\n",
      "\n",
      "Step 2: The question asks for the continent where the Nile river is located.\n",
      "\n",
      "Step 3: Since Africa is a continent and the Nile river is in northeastern Africa\n"
     ]
    }
   ],
   "source": [
    "# Notebook 6 ‚Äî Compare different K\n",
    "def run_compare_K(doc, q, Ks=(8, None)):\n",
    "    outs = {}\n",
    "    for K in Ks:\n",
    "        k_val = TOTAL_LAYERS if (K is None or K == \"full\") else int(K)\n",
    "        txt, _ = lopa_generate(\n",
    "            model, tokenizer, document=doc, question=q,\n",
    "            K=k_val, max_new_tokens=64,\n",
    "            do_sample=False, stop_on_eos=True,\n",
    "            repetition_penalty=1.0,\n",
    "            seed_debug=False, step_debug=False\n",
    "        )\n",
    "        outs[k_val] = txt\n",
    "    return outs\n",
    "\n",
    "outs = run_compare_K(doc, q, Ks=(8, \"full\"))\n",
    "for k, txt in outs.items():\n",
    "    print(f\"\\n=== K={k} ===\\n{txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 ‚Äî Í∏∞Î≥∏ ÏÑ§Ï†ï\n",
    "import os, sys, math, contextlib\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "MISTRAL_ASSIST_START = \"<Mistral_start>\"\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# üîß Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î£®Ìä∏ (trainerÍ∞Ä Ï†ÄÏû•Ìïú _best_ckpt)\n",
    "CKPT_ROOT = \"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\"   # <-- ÌïÑÏöîÏãú Ï†àÎåÄÍ≤ΩÎ°úÎ°ú Î∞îÍæ∏ÏÑ∏Ïöî. Íµ¨Ï°∞: base/, (ÏòµÏÖò) lora/, tokenizer ÌååÏùº\n",
    "PREFILL_LAYERS = 8                         # ÌïòÏúÑ K Î†àÏù¥Ïñ¥Îßå prefill\n",
    "DTYPE = \"auto\"               # \"auto\" | \"bf16\" | \"fp16\" | \"fp32\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228dc53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48255f449f44723a31bf1ff3c208c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] LoRA adapters loaded.\n",
      "Loaded model dtype: torch.bfloat16 | device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 ‚Äî Î°úÎìú Ïú†Ìã∏\n",
    "\n",
    "def _get_inner_model(m):\n",
    "    if hasattr(m, \"module\"): m = m.module\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        if isinstance(m, PeftModel):\n",
    "            try: m = m.get_base_model()\n",
    "            except Exception: m = getattr(m, \"base_model\", m)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for attr in (\"model\",\"transformer\",\"backbone\",\"base_model\",\"language_model\"):\n",
    "        if hasattr(m, attr):\n",
    "            cand = getattr(m, attr)\n",
    "            if hasattr(cand, \"layers\") and isinstance(getattr(cand, \"layers\", None), nn.ModuleList):\n",
    "                return cand\n",
    "            if hasattr(cand, \"decoder\") and hasattr(cand.decoder, \"layers\") and isinstance(cand.decoder.layers, nn.ModuleList):\n",
    "                return cand.decoder\n",
    "    if hasattr(m, \"layers\") and isinstance(getattr(m, \"layers\", None), nn.ModuleList):\n",
    "        return m\n",
    "    for child in m.modules():\n",
    "        if child is m: continue\n",
    "        if hasattr(child, \"layers\") and isinstance(getattr(child, \"layers\", None), nn.ModuleList):\n",
    "            return child\n",
    "    raise AttributeError(\"Could not locate inner base model with a .layers attribute\")\n",
    "\n",
    "def _is_mistral_template(tokenizer) -> bool:\n",
    "    tmpl = getattr(tokenizer, \"chat_template\", \"\") or \"\"\n",
    "    name = getattr(getattr(tokenizer, \"init_kwargs\", {}), \"get\", lambda k, d=None: d)(\"name_or_path\", \"\")\n",
    "    return (\"[INST]\" in tmpl) or (\"mistral\" in str(name).lower()) or (\"mistral\" in tmpl.lower())\n",
    "\n",
    "def ensure_mistral_special_token(tokenizer, model=None):\n",
    "    if not _is_mistral_template(tokenizer):\n",
    "        return False\n",
    "    add_tok = []\n",
    "    cur = set(tokenizer.get_vocab().keys())\n",
    "    if MISTRAL_ASSIST_START not in cur:\n",
    "        add_tok.append(MISTRAL_ASSIST_START)\n",
    "    if add_tok:\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": tokenizer.special_tokens_map_extended.get(\"additional_special_tokens\", []) + add_tok})\n",
    "        if model is not None:\n",
    "            try: model.resize_token_embeddings(len(tokenizer))\n",
    "            except Exception: pass\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "def apply_chat_template(tokenizer, messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        tmpl = getattr(tokenizer, \"chat_template\", \"\") or \"\"\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt and \"<|start_header_id|>\" in tmpl:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(tokenizer, messages, device, add_generation_prompt=False):\n",
    "    s = apply_chat_template(tokenizer, messages, add_generation_prompt)\n",
    "    print(s)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    return len(pkv)\n",
    "\n",
    "def pkv_get(pkv, idx: int):\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)\n",
    "    return dc\n",
    "\n",
    "def _kv_meta_from_model(model_like):\n",
    "    try: cfg = getattr(model_like, \"config\", None) or getattr(_get_inner_model(model_like), \"config\", None)\n",
    "    except Exception: cfg = getattr(_get_inner_model(model_like), \"config\", None)\n",
    "    num_heads = getattr(cfg, \"num_attention_heads\", None)\n",
    "    num_kv    = getattr(cfg, \"num_key_value_heads\", None) or num_heads\n",
    "    hidden    = getattr(cfg, \"hidden_size\", None)\n",
    "    head_dim  = (hidden // num_heads) if (hidden and num_heads) else None\n",
    "    try: dtype = next(_get_inner_model(model_like).parameters()).dtype\n",
    "    except Exception: dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    return int(num_kv), int(head_dim), dtype\n",
    "\n",
    "def _make_empty_kv(batch: int, num_kv: int, head_dim: int, device, dtype):\n",
    "    shape = (batch, num_kv, 0, head_dim)\n",
    "    k = torch.empty(shape, device=device, dtype=dtype)\n",
    "    v = torch.empty(shape, device=device, dtype=dtype)\n",
    "    return k.contiguous(), v.contiguous()\n",
    "\n",
    "# dtype ÏÑ†ÌÉù\n",
    "if DTYPE == \"fp32\": TORCH_DTYPE = torch.float32\n",
    "elif DTYPE == \"bf16\": TORCH_DTYPE = torch.bfloat16\n",
    "elif DTYPE == \"fp16\": TORCH_DTYPE = torch.float16\n",
    "else:\n",
    "    TORCH_DTYPE = (torch.bfloat16 if (DEVICE==\"cuda\" and torch.cuda.is_bf16_supported())\n",
    "                   else (torch.float16 if DEVICE==\"cuda\" else torch.float32))\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä & Î™®Îç∏ Î°úÎìú\n",
    "tok = AutoTokenizer.from_pretrained(CKPT_ROOT, use_fast=True)\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "base_dir = Path(CKPT_ROOT) / \"base\"\n",
    "assert base_dir.is_dir(), f\"Base backbone not found at {base_dir}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(str(base_dir), trust_remote_code=False, torch_dtype=TORCH_DTYPE).to(DEVICE)\n",
    "\n",
    "lora_dir = Path(CKPT_ROOT) / \"lora\"\n",
    "if lora_dir.is_dir():\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, str(lora_dir))\n",
    "        print(\"[Info] LoRA adapters loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Warn] LoRA load failed: {e}\")\n",
    "\n",
    "# Mistral ÌÖúÌîåÎ¶øÏù¥Î©¥ start special Î≥¥Ïû•\n",
    "_ = ensure_mistral_special_token(tok, model)\n",
    "\n",
    "# ÏïàÏ†ïÏÑ±: eager Í∞ïÏ†ú (ÌõàÎ†®Í≥º ÎèôÏùº)\n",
    "for k in (\"attn_implementation\",\"_attn_implementation\"):\n",
    "    try:\n",
    "        setattr(model.config, k, \"eager\")\n",
    "        setattr(_get_inner_model(model).config, k, \"eager\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded model dtype:\", next(model.parameters()).dtype, \"| device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9dda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def lopa_cache_position_patch_global(model, past_key_values, doc_len: int):\n",
    "    inner = _get_inner_model(model)\n",
    "\n",
    "    def _pkv_past_len(li: int) -> int:\n",
    "        if hasattr(past_key_values, \"key_cache\"):\n",
    "            return int(past_key_values.key_cache[li].shape[2])\n",
    "        if hasattr(past_key_values, \"layers\"):\n",
    "            return int(past_key_values.layers[li].keys.shape[2])\n",
    "        return int(past_key_values[li][0].shape[2])\n",
    "\n",
    "    nL = len(inner.layers)\n",
    "    past_lens = [ _pkv_past_len(li) for li in range(nL) ]\n",
    "\n",
    "    handles = []\n",
    "    for li, layer in enumerate(inner.layers):\n",
    "        layer._lopa_past = past_lens[li]\n",
    "        layer._lopa_li = li\n",
    "\n",
    "        def _pre_hook(module, args, kwargs):\n",
    "            li_local = getattr(module, \"_lopa_li\", 0)\n",
    "            past_len = getattr(module, \"_lopa_past\", 0)\n",
    "\n",
    "            cp = kwargs.get(\"cache_position\", None)\n",
    "            pi = kwargs.get(\"position_ids\", None)\n",
    "            start_val = None\n",
    "            if isinstance(cp, torch.Tensor) and cp.numel() > 0:\n",
    "                start_val = int(cp.view(-1)[0].item())\n",
    "            elif isinstance(pi, torch.Tensor) and pi.numel() > 0:\n",
    "                start_val = int(pi.view(-1)[0].item())\n",
    "            if start_val is None:\n",
    "                return args, kwargs\n",
    "\n",
    "            # Ï†Ñ Î†àÏù¥Ïñ¥Í∞Ä 'Ï†ÑÏó≠' Ï†àÎåÄ ÏúÑÏπò(=past_len + doc_len Í∏∞Ï§Ä)Î°ú Î≥¥Ïù¥ÎèÑÎ°ù Î≥¥Ï†ï\n",
    "            # ÌïòÏ∏µ(K): past_len ÏïΩ L_sys+L_doc ‚Üí doc_len ÎçîÌï¥ÎèÑ ÌÅ∞ Î≥ÄÌôî ÏóÜÏùå\n",
    "            # ÏÉÅÏ∏µ  : past_len=seed_len ‚Üí doc_lenÏùÑ ÎçîÌï¥ Ï†àÎåÄ ÌîÑÎ†àÏûÑÏùÑ ÌïòÏ∏µÍ≥º ÎßûÏ∂§\n",
    "            desired_start = past_len + doc_len\n",
    "            off = start_val - desired_start\n",
    "            if off != 0:\n",
    "                if isinstance(cp, torch.Tensor): kwargs[\"cache_position\"] = cp - off\n",
    "                if isinstance(pi, torch.Tensor): kwargs[\"position_ids\"] = pi - off\n",
    "            return args, kwargs\n",
    "\n",
    "        handles.append(layer.register_forward_pre_hook(_pre_hook, with_kwargs=True))\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles: h.remove()\n",
    "        for layer in inner.layers:\n",
    "            for a in (\"_lopa_past\",\"_lopa_li\"):\n",
    "                if hasattr(layer,a): delattr(layer,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4e9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 ‚Äî prefill Ï∫êÏãú Íµ¨ÏÑ± (ÌïòÏúÑ KÎßå sys+doc, ÏÉÅÏúÑÎäî Îπà KV)\n",
    "\n",
    "def build_combined_prefill_cache(model, tokenizer, system_prompt: str, document: str, question: str, K: int, device):\n",
    "    msgs = build_messages(system_prompt, document, question, include_query=True)\n",
    "    print(\"=== Phase1 ===\")\n",
    "    ids_phase1 = tokens_from_messages(tokenizer, msgs, device, add_generation_prompt=False)\n",
    "    print(\"=== Header ===\\n\")\n",
    "    ids_hdr    = tokens_from_messages(tokenizer, msgs, device, add_generation_prompt=True)\n",
    "    sys_only   = tokens_from_messages(tokenizer, [{\"role\":\"system\",\"content\":system_prompt}], device, add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Document tokens must be > 0\"\n",
    "\n",
    "    inner = _get_inner_model(model)\n",
    "    full_layers: nn.ModuleList = inner.layers\n",
    "    n_layers = len(full_layers)\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "\n",
    "    # lower K: [system] prefill\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "    inner.layers = lower_layers\n",
    "    out_sys_low = inner(input_ids=sys_only, attention_mask=torch.ones_like(sys_only), use_cache=True, return_dict=True)\n",
    "    pkv_sys_low = out_sys_low.past_key_values\n",
    "\n",
    "    # then [document] prefill (still lower K)\n",
    "    dc_low_in = dc_from_subset(pkv_sys_low, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    out_low = inner(input_ids=ids_phase1[:, L_sys:], past_key_values=dc_low_in,\n",
    "                    attention_mask=None, use_cache=True, return_dict=True)\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # restore all layers\n",
    "    inner.layers = full_layers\n",
    "\n",
    "    # combined cache: lower K = sys+doc, upper = empty\n",
    "    combined = DynamicCache()\n",
    "    num_kv, head_dim, kv_dtype = _kv_meta_from_model(model)\n",
    "    for li in range(n_layers):\n",
    "        if li < K_eff:\n",
    "            k_sys, v_sys = pkv_get(pkv_sys_low, li)\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_sys_slice = k_sys[:, :, :L_sys, :]\n",
    "            v_sys_slice = v_sys[:, :, :L_sys, :]\n",
    "            k_doc = k_low[:, :, -L_doc:, :]\n",
    "            v_doc = v_low[:, :, -L_doc:, :]\n",
    "            combined.update(torch.cat([k_sys_slice, k_doc], dim=2).contiguous(),\n",
    "                            torch.cat([v_sys_slice, v_doc], dim=2).contiguous(), li)\n",
    "        else:\n",
    "            k_empty, v_empty = _make_empty_kv(1, num_kv, head_dim, DEVICE, kv_dtype)\n",
    "            combined.update(k_empty, v_empty, li)\n",
    "\n",
    "    # header tail as seed\n",
    "    hdr_tail = ids_hdr[:, L_all:]\n",
    "    seed_default = hdr_tail if hdr_tail.numel() > 0 else ids_phase1[:, -1:]\n",
    "\n",
    "    meta = dict(L_sys=int(L_sys), L_doc=int(L_doc), L_all=int(L_all),\n",
    "                n_layers=int(n_layers), K_eff=int(K_eff))\n",
    "    return combined, seed_default, ids_phase1, ids_hdr, meta\n",
    "\n",
    "def print_cache_lengths(cache, tag: str):\n",
    "    print(f\"\\n[{tag}] per-layer past lengths\")\n",
    "    n = pkv_len(cache)\n",
    "    for li in range(n):\n",
    "        k, _ = pkv_get(cache, li)\n",
    "        print(f\"  layer {li:02d}: past_seq = {int(k.shape[2])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407dede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase1 ===\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant that answers questions based on the given document.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Document:\n",
      "France is a country in Western Europe. Its capital city is Paris. It is known for art, fashion, and cuisine.\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Finally, provide your answer in '\\boxed{answer}' at the end of your explanation.<|eot_id|>\n",
      "=== Header ===\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant that answers questions based on the given document.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Document:\n",
      "France is a country in Western Europe. Its capital city is Paris. It is known for art, fashion, and cuisine.\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Finally, provide your answer in '\\boxed{answer}' at the end of your explanation.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant that answers questions based on the given document.<|eot_id|>\n",
      "Layers total= 32 | K_eff= 8 | L_sys= 40 | L_doc= 59 | L_all= 99\n",
      "\n",
      "[prefill] per-layer past lengths\n",
      "  layer 00: past_seq = 99\n",
      "  layer 01: past_seq = 99\n",
      "  layer 02: past_seq = 99\n",
      "  layer 03: past_seq = 99\n",
      "  layer 04: past_seq = 99\n",
      "  layer 05: past_seq = 99\n",
      "  layer 06: past_seq = 99\n",
      "  layer 07: past_seq = 99\n",
      "  layer 08: past_seq = 0\n",
      "  layer 09: past_seq = 0\n",
      "  layer 10: past_seq = 0\n",
      "  layer 11: past_seq = 0\n",
      "  layer 12: past_seq = 0\n",
      "  layer 13: past_seq = 0\n",
      "  layer 14: past_seq = 0\n",
      "  layer 15: past_seq = 0\n",
      "  layer 16: past_seq = 0\n",
      "  layer 17: past_seq = 0\n",
      "  layer 18: past_seq = 0\n",
      "  layer 19: past_seq = 0\n",
      "  layer 20: past_seq = 0\n",
      "  layer 21: past_seq = 0\n",
      "  layer 22: past_seq = 0\n",
      "  layer 23: past_seq = 0\n",
      "  layer 24: past_seq = 0\n",
      "  layer 25: past_seq = 0\n",
      "  layer 26: past_seq = 0\n",
      "  layer 27: past_seq = 0\n",
      "  layer 28: past_seq = 0\n",
      "  layer 29: past_seq = 0\n",
      "  layer 30: past_seq = 0\n",
      "  layer 31: past_seq = 0\n",
      "Prefill KV check: OK\n",
      "\n",
      "[Seed]\n",
      "  seed length = 11 | preview: <|start_header_id|>assistant<|end_header_id|>‚èé‚èéTo find the capital of France,\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ‚Äî ÏûÖÎ†•Í≥º prefill Ïã§Ìñâ/Í≤ÄÏÇ¨\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant that answers questions based on the given document. \"\n",
    "QUESTION = \"What is the capital of France?\\nFinally, provide your answer in '\\\\boxed{answer}' at the end of your explanation.\"\n",
    "DOCUMENT = \"France is a country in Western Europe. Its capital city is Paris. It is known for art, fashion, and cuisine.\"\n",
    "\n",
    "combined, seed_default, ids_phase1, ids_hdr, meta = build_combined_prefill_cache(\n",
    "    model, tok, SYSTEM_PROMPT, DOCUMENT, QUESTION, PREFILL_LAYERS, DEVICE\n",
    ")\n",
    "\n",
    "L_sys, L_doc, L_all, n_layers, K_eff = meta[\"L_sys\"], meta[\"L_doc\"], meta[\"L_all\"], meta[\"n_layers\"], meta[\"K_eff\"]\n",
    "\n",
    "print(\"Layers total=\", n_layers, \"| K_eff=\", K_eff, \"| L_sys=\", L_sys, \"| L_doc=\", L_doc, \"| L_all=\", L_all)\n",
    "print_cache_lengths(combined, tag=\"prefill\")\n",
    "\n",
    "# Í≤ÄÏ¶ù: ÌïòÏúÑ KÎäî L_sys+L_doc, ÏÉÅÏúÑÎäî 0\n",
    "ok = True\n",
    "for li in range(n_layers):\n",
    "    k, _ = pkv_get(combined, li)\n",
    "    expect = (L_sys+L_doc) if li < K_eff else 0\n",
    "    if int(k.shape[2]) != expect:\n",
    "        ok = False\n",
    "print(\"Prefill KV check:\", \"OK\" if ok else \"MISMATCH\")\n",
    "\n",
    "# Seed ÌôïÏ†ï (header tail Ïö∞ÏÑ†, Í∑∏ Îã§Ïùå Mistral start, Í∑∏ Îã§Ïùå fallback)\n",
    "seed = seed_default\n",
    "if seed.numel() == 0:\n",
    "    if _is_mistral_template(tok):\n",
    "        tid = tok.convert_tokens_to_ids(MISTRAL_ASSIST_START)\n",
    "        if tid is not None and tid >= 0:\n",
    "            seed = torch.tensor([[int(tid)]], device=DEVICE, dtype=ids_hdr.dtype)\n",
    "    if seed.numel() == 0:\n",
    "        seed = ids_phase1[:, -1:]\n",
    "# ÏãúÎìú Îí§Ïóê 'Answer: ' Í∞ôÏùÄ Ïò§ÌîÑÎÑàÎ•º 1ÌöåÎßå ÎçßÎ∂ôÏù∏Îã§.\n",
    "SEED_PREFIX = \"To find the capital of France,\"  # ÎòêÎäî \"ÎãµÎ≥Ä: \", \"Final answer: \" Îì± ÌÉúÏä§ÌÅ¨Ïóê ÎßûÍ≤å\n",
    "prefix_ids = tok(SEED_PREFIX, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "seed = torch.cat([seed, prefix_ids], dim=1)\n",
    "\n",
    "# Í∏∏Ïù¥ Í∏∞ÎåÄÏπò Í≤ÄÏ¶ù ÏúÑÌï¥ metaÏóê Î∞òÏòÅ(ÏÑ†ÌÉù)\n",
    "meta[\"seed_len\"] = int(seed.size(1))\n",
    "def decode_piece(ids: torch.Tensor, limit=120):\n",
    "    return tok.decode(ids[0].tolist(), skip_special_tokens=False)[:limit].replace(\"\\n\",\"‚èé\")\n",
    "\n",
    "print(\"\\n[Seed]\")\n",
    "print(\"  seed length =\", int(seed.size(1)), \"| preview:\", decode_piece(seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9af6580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Next-token distribution | after SEED]\n",
      "   1. id=  2057  p= 0.98828  tok=' To'\n",
      "   2. id=   311  p= 0.01099  tok=' to'\n",
      "   3. id=    11  p= 0.00004  tok=','\n",
      "   4. id=  1271  p= 0.00000  tok='To'\n",
      "   5. id=  5257  p= 0.00000  tok=' TO'\n",
      "   6. id=   315  p= 0.00000  tok=' of'\n",
      "   7. id=   320  p= 0.00000  tok=' ('\n",
      "   8. id=  4194  p= 0.00000  tok='\\xa0'\n",
      "   9. id=  9822  p= 0.00000  tok=' France'\n",
      "  10. id=   264  p= 0.00000  tok=' a'\n",
      "\n",
      "[after-seed] per-layer past lengths\n",
      "  layer 00: past_seq = 110\n",
      "  layer 01: past_seq = 110\n",
      "  layer 02: past_seq = 110\n",
      "  layer 03: past_seq = 110\n",
      "  layer 04: past_seq = 110\n",
      "  layer 05: past_seq = 110\n",
      "  layer 06: past_seq = 110\n",
      "  layer 07: past_seq = 110\n",
      "  layer 08: past_seq = 11\n",
      "  layer 09: past_seq = 11\n",
      "  layer 10: past_seq = 11\n",
      "  layer 11: past_seq = 11\n",
      "  layer 12: past_seq = 11\n",
      "  layer 13: past_seq = 11\n",
      "  layer 14: past_seq = 11\n",
      "  layer 15: past_seq = 11\n",
      "  layer 16: past_seq = 11\n",
      "  layer 17: past_seq = 11\n",
      "  layer 18: past_seq = 11\n",
      "  layer 19: past_seq = 11\n",
      "  layer 20: past_seq = 11\n",
      "  layer 21: past_seq = 11\n",
      "  layer 22: past_seq = 11\n",
      "  layer 23: past_seq = 11\n",
      "  layer 24: past_seq = 11\n",
      "  layer 25: past_seq = 11\n",
      "  layer 26: past_seq = 11\n",
      "  layer 27: past_seq = 11\n",
      "  layer 28: past_seq = 11\n",
      "  layer 29: past_seq = 11\n",
      "  layer 30: past_seq = 11\n",
      "  layer 31: past_seq = 11\n",
      "After-seed KV check: OK\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 ‚Äî seedÎßå ÎÑ£Ïñ¥ logits/Top-K Î≥¥Í∏∞\n",
    "\n",
    "@torch.no_grad()\n",
    "def step_once(model, cache, input_ids, patch=True):\n",
    "    if patch:\n",
    "        doc_len = L_sys + L_doc\n",
    "        with lopa_cache_position_patch_global(model, cache, doc_len):\n",
    "            out = model(input_ids=input_ids, past_key_values=cache, use_cache=True, return_dict=True)\n",
    "    else:\n",
    "        out = model(input_ids=input_ids, past_key_values=cache, use_cache=True, return_dict=True)\n",
    "    return out.logits, out.past_key_values\n",
    "\n",
    "def topk_from_logits(logits, tokenizer, k=10, temperature=0.0):\n",
    "    last = logits[:, -1, :]\n",
    "    if temperature and temperature > 0.0:\n",
    "        last = last / float(temperature)\n",
    "    probs = last.softmax(dim=-1)\n",
    "    top_p, top_i = torch.topk(probs, k, dim=-1)\n",
    "    toks = [tokenizer.decode([int(t.item())], skip_special_tokens=False) for t in top_i[0]]\n",
    "    return [(int(i.item()), float(p.item()), t) for i, p, t in zip(top_i[0], top_p[0], toks)]\n",
    "\n",
    "# seed ‚Üí Ìïú Î≤à forward\n",
    "logits, combined = step_once(model, combined, input_ids=seed, patch=True)\n",
    "topk = topk_from_logits(logits, tok, k=10, temperature=0.0)\n",
    "\n",
    "print(\"[Next-token distribution | after SEED]\")\n",
    "for r,(tid,prob,txt) in enumerate(topk, 1):\n",
    "    print(f\"  {r:2d}. id={tid:6d}  p={prob:8.5f}  tok={repr(txt)}\")\n",
    "\n",
    "print_cache_lengths(combined, tag=\"after-seed\")\n",
    "\n",
    "# Í∏∞ÎåÄ Í∏∏Ïù¥: ÌïòÏúÑK = L_sys+L_doc+len(seed), ÏÉÅÏúÑ = len(seed)\n",
    "seed_len = int(seed.size(1))\n",
    "ok2 = True\n",
    "for li in range(n_layers):\n",
    "    k, _ = pkv_get(combined, li)\n",
    "    expect = (L_sys+L_doc+seed_len) if li < K_eff else seed_len\n",
    "    if int(k.shape[2]) != expect:\n",
    "        ok2 = False\n",
    "print(\"After-seed KV check:\", \"OK\" if ok2 else \"MISMATCH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4169608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7b ‚Äî ÏãúÎìúÎ•º Ïù¥ÎØ∏ Ìïú Î≤à Ï†ÅÏö©Ìïú ÏÉÅÌÉú(after-seed)ÏóêÏÑú Î∞îÎ°ú Ïä§ÌÖù ÏãúÏûë\n",
    "@torch.no_grad()\n",
    "def generate_stepwise_from_after_seed(model, tokenizer, cache, first_logits, meta, max_new_tokens=20, temperature=0.0, topk=10):\n",
    "    logits = first_logits  # Cell 6ÏóêÏÑú ÏñªÏùÄ logitsÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö© (seedÎ•º Îã§Ïãú ÎÑ£ÏßÄ ÏïäÏùå)\n",
    "    gen = []\n",
    "    for step in range(max_new_tokens):\n",
    "        # 1) ÏßÅÏ†Ñ logitsÏóêÏÑú Îã§Ïùå ÌÜ†ÌÅ∞ Í≤∞Ï†ï\n",
    "        if temperature and temperature > 0.0:\n",
    "            dist = torch.distributions.Categorical(logits=logits[:, -1, :]/float(temperature))\n",
    "            next_id = dist.sample().unsqueeze(0)\n",
    "        else:\n",
    "            next_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        # if step == 0:\n",
    "        #     next_id = [[11439]]\n",
    "        print(next_id)\n",
    "        # 2) Í∑∏ ÌÜ†ÌÅ∞ÏùÑ ÏûÖÎ†•ÏúºÎ°ú Ìïú Ïä§ÌÖù Ï†ÑÏßÑ\n",
    "        logits, cache = step_once(model, cache, input_ids=next_id, patch=True)\n",
    "\n",
    "        # 3) Î¶¨Ìè¨Ìä∏\n",
    "        report = topk_from_logits(logits, tokenizer, k=topk, temperature=temperature)\n",
    "        gen.append(int(next_id[0,0].item()))\n",
    "        print(f\"\\n[Step {step:02d}] input id={int(next_id[0,0])} '{tokenizer.decode([int(next_id[0,0])], skip_special_tokens=False)}'\")\n",
    "        for r,(tid,prob,txt) in enumerate(report, 1):\n",
    "            print(f\"  {r:2d}. id={tid:6d}  p={prob:8.5f}  tok={repr(txt)}'\")\n",
    "\n",
    "        # 4) Í∏∏Ïù¥ Í≤ÄÏ¶ù (expect: ÌïòÏúÑK = L_sys+L_doc+seed_len+(step+1), ÏÉÅÏúÑ = seed_len+(step+1))\n",
    "        seed_len =  meta[\"L_all\"] - meta[\"L_sys\"]  # = L_doc?  ‚Üê Ïù¥ÎØ∏ after-seedÎùºÎ©¥ seed_lenÏùÄ Cell 5ÏóêÏÑú Î≥ÑÎèÑÎ°ú Î≥¥Í¥ÄÌï¥Îëî Í∞íÏùÑ Ïì∞Îäî Í≤å ÍπîÎÅî\n",
    "        # Í∂åÏû•: Cell 5ÏóêÏÑú seed_lenÏùÑ metaÏóê ÎÑ£Ïñ¥ÎëêÏÑ∏Ïöî. ÏïÑÎûòÎäî ÏòàÏãúÎ°ú seed_lenÏùÑ Î≥ÑÎèÑÎ°ú ÎÑòÍ≤® Î∞õÎäî Ï™ΩÏù¥ ÏïàÏ†Ñ.\n",
    "        # Ïó¨Í∏∞ÏÑúÎäî ÏÑ§Î™Ö Îã®ÏàúÌôîÎ•º ÏúÑÌï¥ metaÏóê seed_lenÏù¥ Ï†ÄÏû•Îèº ÏûàÎã§Í≥† Í∞ÄÏ†ï:\n",
    "        seed_len = meta.get(\"seed_len\", 4)\n",
    "        ok_step = True\n",
    "        for li in range(meta[\"n_layers\"]):\n",
    "            k, _ = pkv_get(cache, li)\n",
    "            expect = (meta[\"L_sys\"]+meta[\"L_doc\"]+seed_len+(step+1)) if li < meta[\"K_eff\"] else (seed_len+(step+1))\n",
    "            if int(k.shape[2]) != expect:\n",
    "                ok_step = False\n",
    "        print(\"  KV check:\", \"OK\" if ok_step else \"MISMATCH\")\n",
    "        print(\"  partial:\", tokenizer.decode(gen, skip_special_tokens=False).replace(\"\\n\",\"‚èé\")[:200])\n",
    "\n",
    "    return tokenizer.decode(gen, skip_special_tokens=False), cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da085920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2057]], device='cuda:0')\n",
      "\n",
      "[Step 00] input id=2057 ' To'\n",
      "   1. id=  1505  p= 1.00000  tok=' find''\n",
      "   2. id=  1766  p= 0.00000  tok=' found''\n",
      "   3. id=  2733  p= 0.00000  tok=' feel''\n",
      "   4. id=  2725  p= 0.00000  tok='.find''\n",
      "   5. id= 14035  p= 0.00000  tok=' finds''\n",
      "   6. id=  3990  p= 0.00000  tok='find''\n",
      "   7. id=  1833  p= 0.00000  tok=' follow''\n",
      "   8. id=  9455  p= 0.00000  tok=' finding''\n",
      "   9. id=  7531  p= 0.00000  tok=' Find''\n",
      "  10. id=  5266  p= 0.00000  tok=' fill''\n",
      "  KV check: OK\n",
      "  partial:  To\n",
      "tensor([[1505]], device='cuda:0')\n",
      "\n",
      "[Step 01] input id=1505 ' find'\n",
      "   1. id=   279  p= 1.00000  tok=' the''\n",
      "   2. id=  1820  p= 0.00000  tok='the''\n",
      "   3. id=   264  p= 0.00000  tok=' a''\n",
      "   4. id=   578  p= 0.00000  tok=' The''\n",
      "   5. id=    11  p= 0.00000  tok=',''\n",
      "   6. id=   315  p= 0.00000  tok=' of''\n",
      "   7. id=  1505  p= 0.00000  tok=' find''\n",
      "   8. id=  1766  p= 0.00000  tok=' found''\n",
      "   9. id=  2057  p= 0.00000  tok=' To''\n",
      "  10. id=  3247  p= 0.00000  tok=' THE''\n",
      "  KV check: OK\n",
      "  partial:  To find\n",
      "tensor([[279]], device='cuda:0')\n",
      "\n",
      "[Step 02] input id=279 ' the'\n",
      "   1. id=  6864  p= 1.00000  tok=' capital''\n",
      "   2. id= 66163  p= 0.00000  tok='capital''\n",
      "   3. id= 24862  p= 0.00000  tok=' captain''\n",
      "   4. id= 18880  p= 0.00000  tok=' Capital''\n",
      "   5. id= 17703  p= 0.00000  tok=' caption''\n",
      "   6. id= 53155  p= 0.00000  tok=' capita''\n",
      "   7. id= 41067  p= 0.00000  tok=' capitalist''\n",
      "   8. id= 98231  p= 0.00000  tok='-capital''\n",
      "   9. id= 53825  p= 0.00000  tok=' capitalize''\n",
      "  10. id=  2107  p= 0.00000  tok=' cap''\n",
      "  KV check: OK\n",
      "  partial:  To find the\n",
      "tensor([[6864]], device='cuda:0')\n",
      "\n",
      "[Step 03] input id=6864 ' capital'\n",
      "   1. id=   315  p= 1.00000  tok=' of''\n",
      "   2. id= 60835  p= 0.00000  tok=' c·ªßa''\n",
      "   3. id=    11  p= 0.00000  tok=',''\n",
      "   4. id=   279  p= 0.00000  tok=' the''\n",
      "   5. id=   297  p= 0.00000  tok=' o''\n",
      "   6. id=   304  p= 0.00000  tok=' in''\n",
      "   7. id=   320  p= 0.00000  tok=' (''\n",
      "   8. id=   264  p= 0.00000  tok=' a''\n",
      "   9. id=   369  p= 0.00000  tok=' for''\n",
      "  10. id=   323  p= 0.00000  tok=' and''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital\n",
      "tensor([[315]], device='cuda:0')\n",
      "\n",
      "[Step 04] input id=315 ' of'\n",
      "   1. id=  9822  p= 1.00000  tok=' France''\n",
      "   2. id= 50100  p= 0.00000  tok='France''\n",
      "   3. id=  9635  p= 0.00000  tok=' England''\n",
      "   4. id= 48687  p= 0.00000  tok=' france''\n",
      "   5. id=  8753  p= 0.00000  tok=' French''\n",
      "   6. id= 10057  p= 0.00000  tok=' Germany''\n",
      "   7. id=  9893  p= 0.00000  tok=' Franc''\n",
      "   8. id=  4606  p= 0.00000  tok=' Europe''\n",
      "   9. id= 26184  p= 0.00000  tok=' Francis''\n",
      "  10. id= 18157  p= 0.00000  tok=' Spain''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of\n",
      "tensor([[9822]], device='cuda:0')\n",
      "\n",
      "[Step 05] input id=9822 ' France'\n",
      "   1. id=    11  p= 1.00000  tok=',''\n",
      "   2. id=  3638  p= 0.00000  tok=',\\n\\n''\n",
      "   3. id=   320  p= 0.00000  tok=' (''\n",
      "   4. id= 10856  p= 0.00000  tok=',,''\n",
      "   5. id=   345  p= 0.00000  tok=',\\n''\n",
      "   6. id=  1174  p= 0.00000  tok=',''\n",
      "   7. id=    25  p= 0.00000  tok=':''\n",
      "   8. id=    26  p= 0.00000  tok=';''\n",
      "   9. id=   705  p= 0.00000  tok='),''\n",
      "  10. id=  9822  p= 0.00000  tok=' France''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France\n",
      "tensor([[11]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 06] input id=11 ','\n",
      "   1. id=  2057  p= 1.00000  tok=' To''\n",
      "   2. id=  5257  p= 0.00000  tok=' TO''\n",
      "   3. id=   350  p= 0.00000  tok=' T''\n",
      "   4. id=  1271  p= 0.00000  tok='To''\n",
      "   5. id=   311  p= 0.00000  tok=' to''\n",
      "   6. id=  3354  p= 0.00000  tok='.To''\n",
      "   7. id=   578  p= 0.00000  tok=' The''\n",
      "   8. id=  1183  p= 0.00000  tok=' Tr''\n",
      "   9. id=  7054  p= 0.00000  tok=' Top''\n",
      "  10. id=   666  p= 0.00000  tok=' Th''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France,\n",
      "tensor([[2057]], device='cuda:0')\n",
      "\n",
      "[Step 07] input id=2057 ' To'\n",
      "   1. id=  1505  p= 1.00000  tok=' find''\n",
      "   2. id=  1766  p= 0.00000  tok=' found''\n",
      "   3. id=  2733  p= 0.00000  tok=' feel''\n",
      "   4. id=  2725  p= 0.00000  tok='.find''\n",
      "   5. id= 14035  p= 0.00000  tok=' finds''\n",
      "   6. id=  5266  p= 0.00000  tok=' fill''\n",
      "   7. id=  9455  p= 0.00000  tok=' finding''\n",
      "   8. id=  1833  p= 0.00000  tok=' follow''\n",
      "   9. id=  3887  p= 0.00000  tok=' fund''\n",
      "  10. id=  3990  p= 0.00000  tok='find''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To\n",
      "tensor([[1505]], device='cuda:0')\n",
      "\n",
      "[Step 08] input id=1505 ' find'\n",
      "   1. id=   279  p= 1.00000  tok=' the''\n",
      "   2. id=  1820  p= 0.00000  tok='the''\n",
      "   3. id=    11  p= 0.00000  tok=',''\n",
      "   4. id=   264  p= 0.00000  tok=' a''\n",
      "   5. id=   315  p= 0.00000  tok=' of''\n",
      "   6. id=   323  p= 0.00000  tok=' and''\n",
      "   7. id=   420  p= 0.00000  tok=' this''\n",
      "   8. id=  1505  p= 0.00000  tok=' find''\n",
      "   9. id=   578  p= 0.00000  tok=' The''\n",
      "  10. id=   430  p= 0.00000  tok=' that''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find\n",
      "tensor([[279]], device='cuda:0')\n",
      "\n",
      "[Step 09] input id=279 ' the'\n",
      "   1. id=  6864  p= 1.00000  tok=' capital''\n",
      "   2. id= 66163  p= 0.00000  tok='capital''\n",
      "   3. id= 24862  p= 0.00000  tok=' captain''\n",
      "   4. id= 18880  p= 0.00000  tok=' Capital''\n",
      "   5. id= 53155  p= 0.00000  tok=' capita''\n",
      "   6. id= 17703  p= 0.00000  tok=' caption''\n",
      "   7. id= 53825  p= 0.00000  tok=' capitalize''\n",
      "   8. id= 41067  p= 0.00000  tok=' capitalist''\n",
      "   9. id= 98231  p= 0.00000  tok='-capital''\n",
      "  10. id= 32682  p= 0.00000  tok=' capitalism''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the\n",
      "tensor([[6864]], device='cuda:0')\n",
      "\n",
      "[Step 10] input id=6864 ' capital'\n",
      "   1. id=   315  p= 1.00000  tok=' of''\n",
      "   2. id=    11  p= 0.00000  tok=',''\n",
      "   3. id= 60835  p= 0.00000  tok=' c·ªßa''\n",
      "   4. id=   279  p= 0.00000  tok=' the''\n",
      "   5. id=   297  p= 0.00000  tok=' o''\n",
      "   6. id=   304  p= 0.00000  tok=' in''\n",
      "   7. id=   389  p= 0.00000  tok=' on''\n",
      "   8. id=   369  p= 0.00000  tok=' for''\n",
      "   9. id=   323  p= 0.00000  tok=' and''\n",
      "  10. id=   311  p= 0.00000  tok=' to''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital\n",
      "tensor([[315]], device='cuda:0')\n",
      "\n",
      "[Step 11] input id=315 ' of'\n",
      "   1. id=  9822  p= 1.00000  tok=' France''\n",
      "   2. id= 50100  p= 0.00000  tok='France''\n",
      "   3. id= 26184  p= 0.00000  tok=' Francis''\n",
      "   4. id=  9893  p= 0.00000  tok=' Franc''\n",
      "   5. id=  8753  p= 0.00000  tok=' French''\n",
      "   6. id= 48687  p= 0.00000  tok=' france''\n",
      "   7. id=  9635  p= 0.00000  tok=' England''\n",
      "   8. id= 43833  p= 0.00000  tok=' Frances''\n",
      "   9. id= 10057  p= 0.00000  tok=' Germany''\n",
      "  10. id= 44943  p= 0.00000  tok=' Franco''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of\n",
      "tensor([[9822]], device='cuda:0')\n",
      "\n",
      "[Step 12] input id=9822 ' France'\n",
      "   1. id=    11  p= 1.00000  tok=',''\n",
      "   2. id=  3638  p= 0.00000  tok=',\\n\\n''\n",
      "   3. id=   320  p= 0.00000  tok=' (''\n",
      "   4. id= 10856  p= 0.00000  tok=',,''\n",
      "   5. id=    25  p= 0.00000  tok=':''\n",
      "   6. id=   345  p= 0.00000  tok=',\\n''\n",
      "   7. id=  1174  p= 0.00000  tok=',''\n",
      "   8. id=    26  p= 0.00000  tok=';''\n",
      "   9. id=   705  p= 0.00000  tok='),''\n",
      "  10. id=   315  p= 0.00000  tok=' of''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France\n",
      "tensor([[11]], device='cuda:0')\n",
      "\n",
      "[Step 13] input id=11 ','\n",
      "   1. id=  2057  p= 1.00000  tok=' To''\n",
      "   2. id=   350  p= 0.00000  tok=' T''\n",
      "   3. id=  1271  p= 0.00000  tok='To''\n",
      "   4. id=  5257  p= 0.00000  tok=' TO''\n",
      "   5. id=  3354  p= 0.00000  tok='.To''\n",
      "   6. id=   311  p= 0.00000  tok=' to''\n",
      "   7. id=   578  p= 0.00000  tok=' The''\n",
      "   8. id=   666  p= 0.00000  tok=' Th''\n",
      "   9. id=  1183  p= 0.00000  tok=' Tr''\n",
      "  10. id=  2722  p= 0.00000  tok=' Te''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France,\n",
      "tensor([[2057]], device='cuda:0')\n",
      "\n",
      "[Step 14] input id=2057 ' To'\n",
      "   1. id=  1505  p= 1.00000  tok=' find''\n",
      "   2. id=  1766  p= 0.00000  tok=' found''\n",
      "   3. id=  2733  p= 0.00000  tok=' feel''\n",
      "   4. id=  2725  p= 0.00000  tok='.find''\n",
      "   5. id= 14035  p= 0.00000  tok=' finds''\n",
      "   6. id=  9455  p= 0.00000  tok=' finding''\n",
      "   7. id=  5266  p= 0.00000  tok=' fill''\n",
      "   8. id=  3990  p= 0.00000  tok='find''\n",
      "   9. id=  1833  p= 0.00000  tok=' follow''\n",
      "  10. id=  3887  p= 0.00000  tok=' fund''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To\n",
      "tensor([[1505]], device='cuda:0')\n",
      "\n",
      "[Step 15] input id=1505 ' find'\n",
      "   1. id=   279  p= 1.00000  tok=' the''\n",
      "   2. id=  1820  p= 0.00000  tok='the''\n",
      "   3. id=   315  p= 0.00000  tok=' of''\n",
      "   4. id=    11  p= 0.00000  tok=',''\n",
      "   5. id=   264  p= 0.00000  tok=' a''\n",
      "   6. id=   578  p= 0.00000  tok=' The''\n",
      "   7. id=   430  p= 0.00000  tok=' that''\n",
      "   8. id=   323  p= 0.00000  tok=' and''\n",
      "   9. id=   420  p= 0.00000  tok=' this''\n",
      "  10. id=  1505  p= 0.00000  tok=' find''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find\n",
      "tensor([[279]], device='cuda:0')\n",
      "\n",
      "[Step 16] input id=279 ' the'\n",
      "   1. id=  6864  p= 1.00000  tok=' capital''\n",
      "   2. id= 66163  p= 0.00000  tok='capital''\n",
      "   3. id= 18880  p= 0.00000  tok=' Capital''\n",
      "   4. id= 24862  p= 0.00000  tok=' captain''\n",
      "   5. id= 53155  p= 0.00000  tok=' capita''\n",
      "   6. id= 53825  p= 0.00000  tok=' capitalize''\n",
      "   7. id= 41067  p= 0.00000  tok=' capitalist''\n",
      "   8. id= 98231  p= 0.00000  tok='-capital''\n",
      "   9. id= 17703  p= 0.00000  tok=' caption''\n",
      "  10. id= 32682  p= 0.00000  tok=' capitalism''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find the\n",
      "tensor([[6864]], device='cuda:0')\n",
      "\n",
      "[Step 17] input id=6864 ' capital'\n",
      "   1. id=   315  p= 1.00000  tok=' of''\n",
      "   2. id= 60835  p= 0.00000  tok=' c·ªßa''\n",
      "   3. id=    11  p= 0.00000  tok=',''\n",
      "   4. id=   279  p= 0.00000  tok=' the''\n",
      "   5. id=   297  p= 0.00000  tok=' o''\n",
      "   6. id=   304  p= 0.00000  tok=' in''\n",
      "   7. id=   323  p= 0.00000  tok=' and''\n",
      "   8. id=   389  p= 0.00000  tok=' on''\n",
      "   9. id=   369  p= 0.00000  tok=' for''\n",
      "  10. id=   311  p= 0.00000  tok=' to''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find the capital\n",
      "tensor([[315]], device='cuda:0')\n",
      "\n",
      "[Step 18] input id=315 ' of'\n",
      "   1. id=  9822  p= 1.00000  tok=' France''\n",
      "   2. id= 50100  p= 0.00000  tok='France''\n",
      "   3. id=  9893  p= 0.00000  tok=' Franc''\n",
      "   4. id= 26184  p= 0.00000  tok=' Francis''\n",
      "   5. id= 48687  p= 0.00000  tok=' france''\n",
      "   6. id=  8753  p= 0.00000  tok=' French''\n",
      "   7. id=  9635  p= 0.00000  tok=' England''\n",
      "   8. id= 10057  p= 0.00000  tok=' Germany''\n",
      "   9. id= 43833  p= 0.00000  tok=' Frances''\n",
      "  10. id=  4606  p= 0.00000  tok=' Europe''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find the capital of\n",
      "tensor([[9822]], device='cuda:0')\n",
      "\n",
      "[Step 19] input id=9822 ' France'\n",
      "   1. id=    11  p= 1.00000  tok=',''\n",
      "   2. id=  3638  p= 0.00000  tok=',\\n\\n''\n",
      "   3. id=   320  p= 0.00000  tok=' (''\n",
      "   4. id=    25  p= 0.00000  tok=':''\n",
      "   5. id=    26  p= 0.00000  tok=';''\n",
      "   6. id= 10856  p= 0.00000  tok=',,''\n",
      "   7. id=   705  p= 0.00000  tok='),''\n",
      "   8. id=  1174  p= 0.00000  tok=',''\n",
      "   9. id=   345  p= 0.00000  tok=',\\n''\n",
      "  10. id=    13  p= 0.00000  tok='.''\n",
      "  KV check: OK\n",
      "  partial:  To find the capital of France, To find the capital of France, To find the capital of France\n",
      "\n",
      "=== [Final output (raw)] ===\n",
      " To find the capital of France, To find the capital of France, To find the capital of France\n"
     ]
    }
   ],
   "source": [
    "# Cell 6ÏùÑ Ïã§ÌñâÌïú ÏßÅÌõÑÏóê:\n",
    "first_logits = logits  # Cell 6ÏóêÏÑú ÎÇòÏò® logits\n",
    "# seed_lenÏùÑ metaÏóê Í∏∞Î°ùÌï¥ÎëêÎ©¥ ÏúÑ Ìï®ÏàòÏùò Í∏∞ÎåÄ Í∏∏Ïù¥ Í≥ÑÏÇ∞Ïù¥ Ï†ïÌôïÌï¥ÏßëÎãàÎã§.\n",
    "meta[\"seed_len\"] = int(seed.size(1))  # Ïó¨Í∏∞ÏÑúÎäî 4\n",
    "\n",
    "FINAL, combined = generate_stepwise_from_after_seed(\n",
    "    model, tok, combined, first_logits, meta,\n",
    "    max_new_tokens=20, temperature=0.0, topk=10\n",
    ")\n",
    "print(\"\\n=== [Final output (raw)] ===\")\n",
    "print(FINAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bcb0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 11439]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tok.encode(\"According\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964dceee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatistics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ÌïôÏäµ Ïä§ÌÅ¨Î¶ΩÌä∏ÏóêÏÑú Ïì∞Îçò Ïú†Ìã∏ alias Í∞ÄÏ†∏Ïò§Í∏∞ (Ïù¥ÎØ∏ Ï†Ñ ÏÖÄÏóêÏÑú wired ÌñàÎã§Î©¥ ÏÉùÎûµ)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# from train_lopa_pure import build_messages, tokens_from_messages, _get_inner_model, pkv_get, _kv_meta_from_model, _make_empty_kv, dc_from_subset, lopa_cache_position_patch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m DEVICE = \u001b[38;5;28mnext\u001b[39m(\u001b[43mmodel\u001b[49m.parameters()).device\n\u001b[32m      8\u001b[39m SYSTEM = \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant that answers questions based on the given document. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_combined\u001b[39m(tokenizer, model, system_prompt, d, q, K):\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import json, random, torch\n",
    "from statistics import mean\n",
    "\n",
    "# ÌïôÏäµ Ïä§ÌÅ¨Î¶ΩÌä∏ÏóêÏÑú Ïì∞Îçò Ïú†Ìã∏ alias Í∞ÄÏ†∏Ïò§Í∏∞ (Ïù¥ÎØ∏ Ï†Ñ ÏÖÄÏóêÏÑú wired ÌñàÎã§Î©¥ ÏÉùÎûµ)\n",
    "# from train_lopa_pure import build_messages, tokens_from_messages, _get_inner_model, pkv_get, _kv_meta_from_model, _make_empty_kv, dc_from_subset, lopa_cache_position_patch\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "SYSTEM = \"You are a helpful assistant that answers questions based on the given document. \"\n",
    "\n",
    "def make_combined(tokenizer, model, system_prompt, d, q, K):\n",
    "    msgs = build_messages(system_prompt, d, q, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(tokenizer, msgs, DEVICE, add_generation_prompt=False)\n",
    "    ids_hdr    = tokens_from_messages(tokenizer, msgs, DEVICE, add_generation_prompt=True)\n",
    "    sys_only   = tokens_from_messages(tokenizer, [{\"role\":\"system\",\"content\":system_prompt}], DEVICE, add_generation_prompt=False)\n",
    "\n",
    "    L_sys, L_all = sys_only.size(1), ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0\n",
    "    inner = _get_inner_model(model)\n",
    "    full_layers = inner.layers\n",
    "    n_layers = len(full_layers)\n",
    "    K_eff = max(0, min(int(K), n_layers))\n",
    "\n",
    "    # lower-K prefill (model(...) Í≤ΩÏú†Î°úÎèÑ Í∞ÄÎä•ÌïòÏßÄÎßå Ïó¨Í∏∞ÏÑ† Îπ†Î•∏ ÏßÑÎã® Î™©Ï†Å: inner ÏÇ¨Ïö©, no_grad)\n",
    "    lower = torch.nn.ModuleList([full_layers[i] for i in range(K_eff)])\n",
    "    inner.layers = lower\n",
    "    with torch.no_grad():\n",
    "        out_sys = inner(input_ids=sys_only, attention_mask=torch.ones_like(sys_only),\n",
    "                        use_cache=True, return_dict=True)\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    dc_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff>0 else DynamicCache()\n",
    "    with torch.no_grad():\n",
    "        out_low = inner(input_ids=ids_phase1[:, L_sys:], past_key_values=dc_in,\n",
    "                        attention_mask=None, use_cache=True, return_dict=True)\n",
    "    pkv_low = out_low.past_key_values\n",
    "    inner.layers = full_layers\n",
    "\n",
    "    combined = DynamicCache()\n",
    "    num_kv, head_dim, kv_dtype = _kv_meta_from_model(model)\n",
    "    for li in range(n_layers):\n",
    "        if li < K_eff:\n",
    "            k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_cat = torch.cat([k_sys[:, :, :L_sys, :], k_low[:, :, -L_doc:, :]], dim=2)\n",
    "            v_cat = torch.cat([v_sys[:, :, :L_sys, :], v_low[:, :, -L_doc:, :]], dim=2)\n",
    "        else:\n",
    "            k_cat, v_cat = _make_empty_kv(1, num_kv, head_dim, DEVICE, kv_dtype)\n",
    "        combined.update(k_cat.contiguous(), v_cat.contiguous(), li)\n",
    "\n",
    "    hdr_tail = tokens_from_messages(tokenizer, msgs, DEVICE, add_generation_prompt=True)[:, L_all:]\n",
    "    seed = hdr_tail if hdr_tail.numel() > 0 else ids_phase1[:, -1:]\n",
    "    return combined, seed, ids_hdr\n",
    "\n",
    "@torch.inference_mode()\n",
    "def ce_for_pair(tokenizer, model, q, d, resp, K):\n",
    "    combined, seed, ids_hdr = make_combined(tokenizer, model, SYSTEM, d, q, K)\n",
    "    msgs = build_messages(SYSTEM, d, q, include_query=True)\n",
    "    msgs_ass = msgs + [{\"role\":\"assistant\",\"content\":resp}]\n",
    "    full_ids = tokens_from_messages(tokenizer, msgs_ass, DEVICE, add_generation_prompt=False)\n",
    "    a = full_ids[:, ids_hdr.size(1):]\n",
    "    if a.numel() == 0:\n",
    "        return None\n",
    "    inp = torch.cat([seed, a], dim=1)\n",
    "    lab = inp.clone(); lab[:, :seed.size(1)] = -100\n",
    "    doc_len = L_sys + L_doc\n",
    "    with lopa_cache_position_patch_global(model, combined, doc_len):\n",
    "        out = model(input_ids=inp, past_key_values=combined, labels=lab, use_cache=True, return_dict=True)\n",
    "    return float(out.loss.item()) if out.loss is not None and torch.isfinite(out.loss) else None\n",
    "\n",
    "# ÏÉòÌîåÎßÅ\n",
    "DATA_PATH = \"triviaqa_hotpotqa_6000_merged2.jsonl\"\n",
    "pairs = []\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        q = rec.get(\"question\",\"\").strip()\n",
    "        d = rec.get(\"document\",\"\").strip()\n",
    "        rs = rec.get(\"responses\",[])\n",
    "        if q and d and rs:\n",
    "            pairs.append((q,d,rs[0]))\n",
    "        if len(pairs) >= 24: break\n",
    "\n",
    "# Shuffled Î¨∏ÏÑú ÎßåÎì§Í∏∞\n",
    "docs = [d for _, d, _ in pairs]\n",
    "random.shuffle(docs)\n",
    "shuffled = [(q, d_shuf, r) for (q,_,r), d_shuf in zip(pairs, docs)]\n",
    "\n",
    "def run_block(title, items, K):\n",
    "    vals = []\n",
    "    for (q,d,r) in items:\n",
    "        v = ce_for_pair(tokenizer, model, q, d, r, K)\n",
    "        if v is not None: vals.append(v)\n",
    "    print(f\"{title} | K={K} | N={len(vals)} | mean CE={mean(vals):.6f}\")\n",
    "    return vals\n",
    "\n",
    "print(\"=== CE sanity ===\")\n",
    "_ = run_block(\"LoPA\", pairs, K=4)\n",
    "_ = run_block(\"Full \", pairs, K=9999)     # effectively all layers\n",
    "_ = run_block(\"Shuf \", shuffled, K=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel  # pip install peft\n",
    "from LatentCOMP_cleaned.infer_lopa_pure import lopa_generate, ensure_mistral_special_token, _get_inner_model\n",
    "\n",
    "repo_id = \"jeongseokoh/Llama-3.1-8B-Instruct-LOPA-partial4-0specials\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if (device==\"cuda\" and torch.cuda.is_bf16_supported()) else (torch.float16 if device==\"cuda\" else torch.float32)\n",
    "\n",
    "# Tokenizer (saved at repo root)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Base (under subfolder=base)\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id, subfolder=\"base\", trust_remote_code=False, torch_dtype=dtype)\n",
    "ensure_mistral_special_token(tokenizer, model)\n",
    "\n",
    "# Merge LoRA if present (under subfolder=lora)\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, repo_id, subfolder=\"lora\").merge_and_unload()\n",
    "except Exception as e:\n",
    "    print(f\"[warn] LoRA merge failed or missing, using base only: {e}\")\n",
    "\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Force eager attention (stability)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try:\n",
    "        setattr(model.config, k, \"eager\")\n",
    "        setattr(_get_inner_model(model).config, k, \"eager\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Fill these\n",
    "system = \"You are a helpful assistant that answers questions based on the given document. \"\n",
    "document = \"Replace with your full document text here.\"\n",
    "question = \"Replace with your question here.\"\n",
    "K = 4  # same as training (partial4)\n",
    "\n",
    "# Generate with LoPA\n",
    "text = lopa_generate(\n",
    "    model, tokenizer,\n",
    "    system=system, document=document, question=question,\n",
    "    K=K, device=device,\n",
    "    max_new_tokens=256, min_length=16,\n",
    "    temperature=0.7, top_p=0.9, top_k=None,\n",
    "    do_sample=True, debug=True,\n",
    ")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edcbf962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Transformers version: 4.56.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be99633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f61e08d60147ad9d30d2f3b7ccdcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 20:39:19,291] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::substr(unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_stringbuf_init(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_dispose(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::swap(std::__detail::_List_node_base&, std::__detail::_List_node_base&)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_mutate(unsigned long, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_next_bkt(unsigned long) const@GLIBCXX_3.4.18'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::end()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::flush<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::string const&, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::runtime_error(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(std::string const&) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 20:39:20,573] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "Rewrote clean base to: LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/base\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# 1) Í≤ΩÎ°úÏôÄ ÏõêÎ≥∏ Î™®Îç∏ ID ÏßÄÏ†ï\n",
    "best_dir = \"LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\"\n",
    "base_dir = f\"{best_dir}/base\"\n",
    "orig_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # ÌïôÏäµÏóê Ïì¥ ÏõêÎ≥∏ Î™®Îç∏ Ïù¥Î¶Ñ\n",
    "\n",
    "# 2) ÏõêÎ≥∏ Î≤†Ïù¥Ïä§ Î™®Îç∏ Î°úÎìú(Ï∫êÏãú ÏÇ¨Ïö©), Ï†ÄÏû•\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    orig_model_id,\n",
    "    trust_remote_code=True,          # Llama3 Í≥ÑÏó¥Ïù¥Î©¥ True Í¥úÏ∞ÆÏäµÎãàÎã§\n",
    "    cache_dir=\"/data2/jeongseokoh/hub/model\"  # ÌïôÏäµ Îïå Ïì∞Îçò Ï∫êÏãú Í≤ΩÎ°úÍ∞Ä ÏûàÏúºÎ©¥ ÏßÄÏ†ï\n",
    ")\n",
    "# base/config.jsonÏóê remote code ÌùîÏ†ÅÏù¥ ÏÑûÏù¥ÏßÄ ÏïäÎèÑÎ°ù auto_map ÌÅ¥Î¶¨Ïñ¥(ÏûàÎã§Î©¥)\n",
    "try:\n",
    "    setattr(m.config, \"auto_map\", None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "m.save_pretrained(base_dir, safe_serialization=True)\n",
    "del m\n",
    "\n",
    "print(\"Rewrote clean base to:\", base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c22a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26170ccbacaf4356b8fe1991f271731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready to help. What would you like to know? Please go ahead and ask your question.\n"
     ]
    }
   ],
   "source": [
    "# LoPA (low-only prefill) scratch pipeline without trust_remote_code\n",
    "# Torch 2.5.1, Transformers 4.56.1, GPU+SDPA\n",
    "# - Phase1: lower-K only prefill (no assistant header)\n",
    "# - Phase2: generation (first feed assistant header tokens step-by-step)\n",
    "# - Positions unchanged; no remapping\n",
    "# - LoRA: best/lora attach ‚Üí merge\n",
    "# - MinLength ÏàòÎèô Ï†ÅÏö©(4.56.1 ÎîîÎ∞îÏù¥Ïä§ Î∂àÏùºÏπò ÌöåÌîº)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "K = 4  # prefill layers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (device == \"cuda\" and torch.cuda.is_bf16_supported())\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "cache_dir_model = \"/data2/jeongseokoh/hub/model\"\n",
    "cache_dir_tok   = \"/data2/jeongseokoh/hub/tokenizer\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (best_dir Ïö∞ÏÑ†)\n",
    "# -----------------------------\n",
    "tok_src = str(best_dir) if (best_dir / \"tokenizer.json\").is_file() else backbone_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir_tok, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone + LoRA (merge)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir=cache_dir_model,\n",
    ")\n",
    "base_model.to(device).eval()\n",
    "\n",
    "from peft import PeftModel\n",
    "assert lora_dir.is_dir(), f\"LoRA folder not found: {lora_dir}\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "model = peft_model.merge_and_unload().to(device).eval()\n",
    "del peft_model\n",
    "\n",
    "# SDPA Í≥†Ï†ï (ÌïÑÏöîÏãú eagerÎ°ú ÎπÑÍµê)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try: setattr(model.config, k, \"sdpa\")\n",
    "    except: pass\n",
    "try:\n",
    "    for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "        setattr(model.model.config, k, \"sdpa\")\n",
    "except: pass\n",
    "\n",
    "# -----------------------------\n",
    "# Cache helpers\n",
    "# -----------------------------\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    try: return len(pkv)\n",
    "    except: return 0\n",
    "\n",
    "def pkv_get(pkv, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)  # layer index Ïú†ÏßÄ\n",
    "    return dc\n",
    "\n",
    "def cache_slice_doc_only(k: torch.Tensor, v: torch.Tensor, doc_len: int):\n",
    "    return k[:, :, -doc_len:, :], v[:, :, -doc_len:, :]\n",
    "\n",
    "def build_mask(length: int, batch: int = 1):\n",
    "    return torch.ones(batch, length, device=device, dtype=torch.long)\n",
    "\n",
    "def build_pos_ids(start: int, length: int, batch: int = 1):\n",
    "    return torch.arange(start, start + length, device=device, dtype=torch.long).unsqueeze(0).expand(batch, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt builders (Gen3)\n",
    "# -----------------------------\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    s = apply_chat_template(messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# LoPA generate (low-only prefill; no upper continue)\n",
    "# -----------------------------\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    system: str,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    K: int = 4,\n",
    "    max_new_tokens: int = 256,\n",
    "    min_length: int = 16,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: Optional[int] = None,\n",
    "    do_sample: bool = True,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    start_with_assistant_header: bool = True, # Phase2 Ï≤´ ÌÜ†ÌÅ∞: headerÎ•º Îã®Í≥ÑÏ†ÅÏúºÎ°ú Ìà¨ÏûÖ\n",
    "    force_attn_impl: Optional[str] = None,   # \"sdpa\" | \"eager\"\n",
    "):\n",
    "    if force_attn_impl:\n",
    "        for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "            try:\n",
    "                setattr(model.config, k, force_attn_impl)\n",
    "                setattr(model.model.config, k, force_attn_impl)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Phase-1 ids\n",
    "    msgs = build_messages(system, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(msgs, add_generation_prompt=False)  # [1, L_sys+doc]\n",
    "    ids_hdr    = tokens_from_messages(msgs, add_generation_prompt=True)   # [1, L_sys+doc + header]\n",
    "    sys_only   = tokens_from_messages([{\"role\":\"system\",\"content\":system}], add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Phase-1 must include document tokens.\"\n",
    "\n",
    "    # 1) System-only prefill (base model)\n",
    "    out_sys = model.model(\n",
    "        input_ids=sys_only,\n",
    "        attention_mask=build_mask(L_sys),\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    n_layers = pkv_len(pkv_sys)\n",
    "\n",
    "    # 2) Lower-K doc pass (base model) ‚Äî no upper continue\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "    full_layers: nn.ModuleList = model.model.layers\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "\n",
    "    model.model.layers = lower_layers\n",
    "    dc_low_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    attn_doc_full = torch.cat([build_mask(L_sys), build_mask(L_doc)], dim=1)\n",
    "\n",
    "    out_low = model.model(\n",
    "        input_ids=ids_phase1[:, L_sys:],  # doc only\n",
    "        past_key_values=dc_low_in,\n",
    "        attention_mask=attn_doc_full,     # sys+doc Í∏∏Ïù¥\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # Î≥µÏõê\n",
    "    model.model.layers = full_layers\n",
    "\n",
    "    # 3) Combine caches\n",
    "    # lower(0..K-1): sys + doc\n",
    "    # upper(K..):    sys only  (doc ÎØ∏Ìè¨Ìï®; generation ÏãúÏûë ÏãúÏ†êÎ∂ÄÌÑ∞ ÏåìÏûÑ)\n",
    "    combined = DynamicCache()\n",
    "    # Ìó§Îìú/Ï∞®Ïõê shape ÌôïÎ≥¥\n",
    "    k0_sys, v0_sys = pkv_get(pkv_sys, 0)\n",
    "    for li in range(n_layers):\n",
    "        k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "        k_sys = k_sys[:, :, :L_sys, :]\n",
    "        v_sys = v_sys[:, :, :L_sys, :]\n",
    "        if li < K_eff:\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_doc, v_doc = cache_slice_doc_only(k_low, v_low, L_doc)\n",
    "            k_cat = torch.cat([k_sys, k_doc], dim=2).contiguous()\n",
    "            v_cat = torch.cat([v_sys, v_doc], dim=2).contiguous()\n",
    "            combined.update(k_cat, v_cat, li)\n",
    "        else:\n",
    "            # upper: sys only\n",
    "            combined.update(k_sys.contiguous(), v_sys.contiguous(), li)\n",
    "\n",
    "    # 4) Phase-2: Generation\n",
    "    # seed: assistant headerÎ•º Ìïú ÌÜ†ÌÅ∞Ïî© Î∞ÄÏñ¥ ÎÑ£Ïñ¥ upper pastÍ∞Ä L_sys ‚Üí L_sys+HÎ°ú ÏûêÎùºÎèÑÎ°ù Ìï®\n",
    "    if start_with_assistant_header:\n",
    "        hdr_tail = ids_hdr[:, L_all:]  # header-only tokens (len H)\n",
    "        H = int(hdr_tail.size(1))\n",
    "        for j in range(H):\n",
    "            past_len = pkv_get(combined, 0)[0].shape[2]  # lower Í∏∞Ï§Ä: L_sys+L_doc+grown\n",
    "            attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "            step_tok = hdr_tail[:, j:j+1]\n",
    "            out_seed = model(\n",
    "                input_ids=step_tok,\n",
    "                past_key_values=combined,\n",
    "                attention_mask=attn_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            combined = out_seed.past_key_values\n",
    "\n",
    "    # 5) Decoding (CausalLM) with safe processors\n",
    "    from transformers.generation import LogitsProcessorList\n",
    "    from transformers.generation.logits_process import (\n",
    "        TemperatureLogitsWarper, TopPLogitsWarper, TopKLogitsWarper,\n",
    "        RepetitionPenaltyLogitsProcessor,\n",
    "    )\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    generated_ids = torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    # Ï≤´ ÏÉùÏÑ± Ïä§ÌÖùÏùò ÏûÖÎ†•ÏùÄ \"ÏßÅÏ†Ñ ÌÜ†ÌÅ∞\" (header ÎßàÏßÄÎßâ ÎòêÎäî ÎßàÏßÄÎßâ doc)\n",
    "    last = (ids_hdr[:, -1:] if start_with_assistant_header else ids_phase1[:, -1:])\n",
    "\n",
    "    processors = LogitsProcessorList()\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        processors.append(RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty)))\n",
    "    if temperature and temperature != 1.0:\n",
    "        processors.append(TemperatureLogitsWarper(temperature=float(temperature)))\n",
    "    if top_k is not None and top_k > 0:\n",
    "        processors.append(TopKLogitsWarper(top_k=int(top_k), filter_value=-float(\"inf\")))\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        processors.append(TopPLogitsWarper(top_p=float(top_p), min_tokens_to_keep=1))\n",
    "\n",
    "    cur = 0\n",
    "    while cur < max_new_tokens:\n",
    "        # lower past_lenÏùÄ L_sys+L_doc + grown; upper past_lenÏùÄ L_sys + grown\n",
    "        past_len = pkv_get(combined, 0)[0].shape[2]\n",
    "        attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=last,\n",
    "            past_key_values=combined,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        combined = out.past_key_values\n",
    "        logits = out.logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "        # Ï¶âÏãú EOS Î∞©ÏßÄ (min_length ÏàòÎèô Ï†ÅÏö©)\n",
    "        if eos_id is not None and cur < min_length:\n",
    "            logits[:, eos_id] = -float(\"inf\")\n",
    "\n",
    "        if not torch.isfinite(logits).all():\n",
    "            logits = torch.nan_to_num(logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        inp = generated_ids if generated_ids.numel() > 0 else last.new_zeros((1, 0), dtype=torch.long, device=device)\n",
    "        inp = inp.to(logits.device)\n",
    "        logits = processors(inp, logits)\n",
    "\n",
    "        # Fallback-safe sampling\n",
    "        if not torch.isfinite(logits).any():\n",
    "            next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if do_sample:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                if (not torch.isfinite(probs).all()) or (probs.sum(dim=-1) <= 0).any():\n",
    "                    next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if eos_id is not None and int(next_tok.item()) == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_tok], dim=1)\n",
    "        last = next_tok\n",
    "        cur += 1\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "answer = lopa_generate(\n",
    "    system, document, question,\n",
    "    K=K,\n",
    "    max_new_tokens=256,\n",
    "    min_length=16,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    start_with_assistant_header=True,  # Phase2 Ï≤òÏùåÏóê headerÎ•º Îã®Í≥ÑÏ†ÅÏúºÎ°ú Ìà¨ÏûÖ\n",
    "    # force_attn_impl=\"eager\",         # ÌïÑÏöîÏãú ÎπÑÍµê\n",
    ")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ff3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcead6b3d56471c87aadf12086bbd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lens] L_sys=40, L_doc=35, L_all=75, header_len=4\n",
      "[Layers] n_layers=32, K_eff=4\n",
      "[sys] n_layers=32 | L0:K(1, 8, 40, 128) | L1:K(1, 8, 40, 128) | L31:K(1, 8, 40, 128)\n",
      "[OK]   system_prefill: all present layers seq_len == 40\n",
      "[low] h_low=(1, 35, 4096)\n",
      "[low] n_layers=4 | L0:K(1, 8, 75, 128) | L1:K(1, 8, 75, 128) | L3:K(1, 8, 75, 128)\n",
      "[up] n_layers=32 | L4:K(1, 8, 75, 128) | L5:K(1, 8, 75, 128) | L31:K(1, 8, 75, 128)\n",
      "[combined(before_hdr)] n_layers=32 | L0:K(1, 8, 75, 128) | L1:K(1, 8, 75, 128) | L31:K(1, 8, 75, 128)\n",
      "[OK]   combined_prefill: all present layers seq_len == 75\n",
      "[hdr] past_len before=75, after=79, delta=4\n",
      "[combined(after_hdr)] n_layers=32 | L0:K(1, 8, 79, 128) | L1:K(1, 8, 79, 128) | L31:K(1, 8, 79, 128)\n",
      "[decode step 0] past_len 79 -> 80 | next_id=40\n",
      "[decode step 1] past_len 80 -> 81 | next_id=2846\n",
      "[decode step 2] past_len 81 -> 82 | next_id=539\n",
      "[debug answer] I'm not able to answer your question as I don't have any information about a document. Can you please provide the document or more context about the question you are trying to ask?\n",
      "I'm not able to answer your question since you didn't provide a document. Please provide the document, and I'll be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "# LoPA (low-only prefill) scratch pipeline without trust_remote_code\n",
    "# Torch 2.5.1, Transformers 4.56.1, GPU+SDPA\n",
    "# - Phase1: lower-K only prefill (no assistant header)\n",
    "# - Phase2: generation (first feed assistant header tokens step-by-step)\n",
    "# - Positions unchanged; no remapping\n",
    "# - LoRA: best/lora attach ‚Üí merge\n",
    "# - MinLength ÏàòÎèô Ï†ÅÏö©(4.56.1 ÎîîÎ∞îÏù¥Ïä§ Î∂àÏùºÏπò ÌöåÌîº)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "K = 4  # prefill layers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (device == \"cuda\" and torch.cuda.is_bf16_supported())\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "cache_dir_model = \"/data2/jeongseokoh/hub/model\"\n",
    "cache_dir_tok   = \"/data2/jeongseokoh/hub/tokenizer\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (best_dir Ïö∞ÏÑ†)\n",
    "# -----------------------------\n",
    "tok_src = str(best_dir) if (best_dir / \"tokenizer.json\").is_file() else backbone_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir_tok, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone + LoRA (merge)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir=cache_dir_model,\n",
    ")\n",
    "base_model.to(device).eval()\n",
    "\n",
    "from peft import PeftModel\n",
    "assert lora_dir.is_dir(), f\"LoRA folder not found: {lora_dir}\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "model = peft_model.merge_and_unload().to(device).eval()\n",
    "del peft_model\n",
    "\n",
    "# SDPA Í≥†Ï†ï (ÌïÑÏöîÏãú eagerÎ°ú ÎπÑÍµê)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try: setattr(model.config, k, \"sdpa\")\n",
    "    except: pass\n",
    "try:\n",
    "    for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "        setattr(model.model.config, k, \"sdpa\")\n",
    "except: pass\n",
    "\n",
    "# -----------------------------\n",
    "# Cache helpers\n",
    "# -----------------------------\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    try: return len(pkv)\n",
    "    except: return 0\n",
    "\n",
    "def pkv_get(pkv, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)  # layer index Ïú†ÏßÄ\n",
    "    return dc\n",
    "\n",
    "def cache_slice_doc_only(k: torch.Tensor, v: torch.Tensor, doc_len: int):\n",
    "    return k[:, :, -doc_len:, :], v[:, :, -doc_len:, :]\n",
    "\n",
    "def build_mask(length: int, batch: int = 1):\n",
    "    return torch.ones(batch, length, device=device, dtype=torch.long)\n",
    "\n",
    "def build_pos_ids(start: int, length: int, batch: int = 1):\n",
    "    return torch.arange(start, start + length, device=device, dtype=torch.long).unsqueeze(0).expand(batch, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt builders (Gen3)\n",
    "# -----------------------------\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    s = apply_chat_template(messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# LoPA generate (low-only prefill; no upper continue)\n",
    "# -----------------------------\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    system: str,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    K: int = 4,\n",
    "    max_new_tokens: int = 256,\n",
    "    min_length: int = 16,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: Optional[int] = None,\n",
    "    do_sample: bool = True,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    start_with_assistant_header: bool = True, # Phase2 Ï≤´ ÌÜ†ÌÅ∞: headerÎ•º Îã®Í≥ÑÏ†ÅÏúºÎ°ú Ìà¨ÏûÖ\n",
    "    force_attn_impl: Optional[str] = None,   # \"sdpa\" | \"eager\"\n",
    "):\n",
    "    if force_attn_impl:\n",
    "        for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "            try:\n",
    "                setattr(model.config, k, force_attn_impl)\n",
    "                setattr(model.model.config, k, force_attn_impl)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Phase-1 ids\n",
    "    msgs = build_messages(system, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(msgs, add_generation_prompt=False)  # [1, L_sys+doc]\n",
    "    ids_hdr    = tokens_from_messages(msgs, add_generation_prompt=True)   # [1, L_sys+doc + header]\n",
    "    sys_only   = tokens_from_messages([{\"role\":\"system\",\"content\":system}], add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Phase-1 must include document tokens.\"\n",
    "\n",
    "    # 1) System-only prefill (base model)\n",
    "    out_sys = model.model(\n",
    "        input_ids=sys_only,\n",
    "        attention_mask=build_mask(L_sys),\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    n_layers = pkv_len(pkv_sys)\n",
    "\n",
    "    # 2) Lower-K doc pass (base model) ‚Äî no upper continue\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "    full_layers: nn.ModuleList = model.model.layers\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "\n",
    "    model.model.layers = lower_layers\n",
    "    dc_low_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    attn_doc_full = torch.cat([build_mask(L_sys), build_mask(L_doc)], dim=1)\n",
    "\n",
    "    out_low = model.model(\n",
    "        input_ids=ids_phase1[:, L_sys:],  # doc only\n",
    "        past_key_values=dc_low_in,\n",
    "        attention_mask=attn_doc_full,     # sys+doc Í∏∏Ïù¥\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # Î≥µÏõê\n",
    "    model.model.layers = full_layers\n",
    "\n",
    "    # 3) Combine caches\n",
    "    # lower(0..K-1): sys + doc\n",
    "    # upper(K..):    sys only  (doc ÎØ∏Ìè¨Ìï®; generation ÏãúÏûë ÏãúÏ†êÎ∂ÄÌÑ∞ ÏåìÏûÑ)\n",
    "    combined = DynamicCache()\n",
    "    # Ìó§Îìú/Ï∞®Ïõê shape ÌôïÎ≥¥\n",
    "    k0_sys, v0_sys = pkv_get(pkv_sys, 0)\n",
    "    for li in range(n_layers):\n",
    "        k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "        k_sys = k_sys[:, :, :L_sys, :]\n",
    "        v_sys = v_sys[:, :, :L_sys, :]\n",
    "        if li < K_eff:\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_doc, v_doc = cache_slice_doc_only(k_low, v_low, L_doc)\n",
    "            k_cat = torch.cat([k_sys, k_doc], dim=2).contiguous()\n",
    "            v_cat = torch.cat([v_sys, v_doc], dim=2).contiguous()\n",
    "            combined.update(k_cat, v_cat, li)\n",
    "        else:\n",
    "            # upper: sys only\n",
    "            combined.update(k_sys.contiguous(), v_sys.contiguous(), li)\n",
    "\n",
    "    # 4) Phase-2: Generation\n",
    "    # seed: assistant headerÎ•º Ìïú ÌÜ†ÌÅ∞Ïî© Î∞ÄÏñ¥ ÎÑ£Ïñ¥ upper pastÍ∞Ä L_sys ‚Üí L_sys+HÎ°ú ÏûêÎùºÎèÑÎ°ù Ìï®\n",
    "    if start_with_assistant_header:\n",
    "        hdr_tail = ids_hdr[:, L_all:]  # header-only tokens (len H)\n",
    "        H = int(hdr_tail.size(1))\n",
    "        for j in range(H):\n",
    "            past_len = pkv_get(combined, 0)[0].shape[2]  # lower Í∏∞Ï§Ä: L_sys+L_doc+grown\n",
    "            attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "            step_tok = hdr_tail[:, j:j+1]\n",
    "            out_seed = model(\n",
    "                input_ids=step_tok,\n",
    "                past_key_values=combined,\n",
    "                attention_mask=attn_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            combined = out_seed.past_key_values\n",
    "\n",
    "    # 5) Decoding (CausalLM) with safe processors\n",
    "    from transformers.generation import LogitsProcessorList\n",
    "    from transformers.generation.logits_process import (\n",
    "        TemperatureLogitsWarper, TopPLogitsWarper, TopKLogitsWarper,\n",
    "        RepetitionPenaltyLogitsProcessor,\n",
    "    )\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    generated_ids = torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    # Ï≤´ ÏÉùÏÑ± Ïä§ÌÖùÏùò ÏûÖÎ†•ÏùÄ \"ÏßÅÏ†Ñ ÌÜ†ÌÅ∞\" (header ÎßàÏßÄÎßâ ÎòêÎäî ÎßàÏßÄÎßâ doc)\n",
    "    last = (ids_hdr[:, -1:] if start_with_assistant_header else ids_phase1[:, -1:])\n",
    "\n",
    "    processors = LogitsProcessorList()\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        processors.append(RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty)))\n",
    "    if temperature and temperature != 1.0:\n",
    "        processors.append(TemperatureLogitsWarper(temperature=float(temperature)))\n",
    "    if top_k is not None and top_k > 0:\n",
    "        processors.append(TopKLogitsWarper(top_k=int(top_k), filter_value=-float(\"inf\")))\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        processors.append(TopPLogitsWarper(top_p=float(top_p), min_tokens_to_keep=1))\n",
    "\n",
    "    cur = 0\n",
    "    while cur < max_new_tokens:\n",
    "        # lower past_lenÏùÄ L_sys+L_doc + grown; upper past_lenÏùÄ L_sys + grown\n",
    "        past_len = pkv_get(combined, 0)[0].shape[2]\n",
    "        attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=last,\n",
    "            past_key_values=combined,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        combined = out.past_key_values\n",
    "        logits = out.logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "        # Ï¶âÏãú EOS Î∞©ÏßÄ (min_length ÏàòÎèô Ï†ÅÏö©)\n",
    "        if eos_id is not None and cur < min_length:\n",
    "            logits[:, eos_id] = -float(\"inf\")\n",
    "\n",
    "        if not torch.isfinite(logits).all():\n",
    "            logits = torch.nan_to_num(logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        inp = generated_ids if generated_ids.numel() > 0 else last.new_zeros((1, 0), dtype=torch.long, device=device)\n",
    "        inp = inp.to(logits.device)\n",
    "        logits = processors(inp, logits)\n",
    "\n",
    "        # Fallback-safe sampling\n",
    "        if not torch.isfinite(logits).any():\n",
    "            next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if do_sample:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                if (not torch.isfinite(probs).all()) or (probs.sum(dim=-1) <= 0).any():\n",
    "                    next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if eos_id is not None and int(next_tok.item()) == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_tok], dim=1)\n",
    "        last = next_tok\n",
    "        cur += 1\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "answer = lopa_generate(\n",
    "    system, document, question,\n",
    "    K=K,\n",
    "    max_new_tokens=256,\n",
    "    min_length=16,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    start_with_assistant_header=True,  # Phase2 Ï≤òÏùåÏóê headerÎ•º Îã®Í≥ÑÏ†ÅÏúºÎ°ú Ìà¨ÏûÖ\n",
    "    # force_attn_impl=\"eager\",         # ÌïÑÏöîÏãú ÎπÑÍµê\n",
    ")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7acf95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1346d627656d4138b818cc49145abfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant that answers questions based on the given document.user\\n\\nDocument:\\nThe Nile is the longest river in Africa...\\n\\nQuestion: Which continent is the Nile the longest river in?assistant\\n\\nThe Nile is the longest river in Africa.']\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_dir = \"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\"\n",
    "prefill_layers = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sys.path.insert(0, model_dir)  # best/ Î•º import Í≤ΩÎ°úÏóê Ï∂îÍ∞Ä\n",
    "from modeling_partial_layer import LlamaForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir, device_map=\"cuda:0\", dtype=torch.bfloat16, trust_remote_code=True).to(device).eval()\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    setattr(model.config, k, \"sdpa\")\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "out = model.generate(\n",
    "    system=system, document=document, query=question,\n",
    "    compress=False, tokenizer=tok, prefill_layers=prefill_layers,\n",
    "    max_new_tokens=256, do_sample=False, temperature=0.7, top_p=0.9\n",
    ")\n",
    "print(tok.batch_decode(out, skip_special_tokens=True))  # Î¨∏ÏûêÏó¥ ÎòêÎäî Î¨∏ÏûêÏó¥ Î¶¨Ïä§Ìä∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786d4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_available=True | attached=True | merged=False | is_sharded=True | source=/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best/lora | error=None | last_used_lora=True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LATENTRAG_DEBUG\"]=\"1\"\n",
    "print(model.lora_debug_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89bed7b41e444fab4a660167f1660c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da2aecd742744a68944483b961a633b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant<|end_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Africa.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï (ÌòºÎèô Î∞©ÏßÄÎ•º ÏúÑÌï¥ Ï†àÎåÄÍ≤ΩÎ°ú Í∂åÏû•)\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "prefill_layers = 4  # ÌïôÏäµ Ïãú ÏÇ¨Ïö©Ìïú Í∞íÍ≥º ÎèôÏùºÌïòÍ≤å\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# best/Ïùò remote-code(LOPA Íµ¨ÌòÑ)Î•º importÌï† Ïàò ÏûàÍ≤å Í≤ΩÎ°ú Ï∂îÍ∞Ä\n",
    "sys.path.insert(0, str(best_dir))\n",
    "from modeling_partial_layer import LlamaForCausalLM  # Llama 3.1 Í≥ÑÏó¥Ïö©\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä: chat template Ìè¨Ìï®\n",
    "tok = AutoTokenizer.from_pretrained(str(best_dir))\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# 1) Î∞±Î≥∏ÏùÑ ÌóàÍπÖÌéòÏù¥Ïä§ÏóêÏÑú ‚ÄòÍπ®ÎÅóÌïòÍ≤å‚Äô Î°úÎìú (Î∞±Î≥∏=llama3.1 8B instruct)\n",
    "#    Ï£ºÏùò: trust_remote_code=False ‚Üí ÌëúÏ§Ä HF ÌÅ¥ÎûòÏä§ Í≤ΩÎ°úÎ•º ÌÜµÌï¥ Í∞ÄÏ§ëÏπòÎßå Î°úÎìú\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=False,  # Í∞ÄÏ§ëÏπòÎßå Í∞ÄÏ†∏Ïò§Í≥† ÎèôÏûëÏùÄ our remote-code ÌÅ¥ÎûòÏä§Í∞Ä Îã¥Îãπ\n",
    "    cache_dir=\"/data2/jeongseokoh/hub/model\",\n",
    ").to(device).eval()\n",
    "\n",
    "# 2) LoRA Ïñ¥ÎåëÌÑ∞ attach ÌõÑ merge (Îã®Ïùº Î™®Îç∏Î°ú)\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "merged = peft_model.merge_and_unload().to(device).eval()\n",
    "\n",
    "# 3) LOPA Ï∂îÎ°† (compress=True ‚Üí partial prefill)\n",
    "#    Ïù¥ÎØ∏ mergeÎêú Î™®Îç∏Ïù¥ÎØÄÎ°ú Ï∂îÍ∞Ä attachÎ•º ÎßâÍ∏∞ ÏúÑÌï¥ use_lora=FalseÎ°ú Í≥†Ï†ï\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "out = merged.generate(\n",
    "    system=system,\n",
    "    document=document,\n",
    "    query=question,\n",
    "    compress=True,                 # LOPA Í≤ΩÎ°ú\n",
    "    tokenizer=tok,                 # ÌïÑÏàò\n",
    "    prefill_layers=prefill_layers, # ÌïôÏäµÍ≥º ÎèôÏùº\n",
    "    use_lora=False,                # Ïù¥ÎØ∏ mergeÎêêÏúºÎØÄÎ°ú Ïû¨-Î∂ÄÏ∞© Î∞©ÏßÄ\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(out)  # Î¨∏ÏûêÏó¥ ÎòêÎäî Î¨∏ÏûêÏó¥ Î¶¨Ïä§Ìä∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913061c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
