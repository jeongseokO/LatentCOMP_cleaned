{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e606cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda | dtype: torch.bfloat16\n",
      "[DEBUG] TRI patch loaded: ./lopa_llama_modeling.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e35e8f140b24a829e9b4088e7b46a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] LoRA adapters loaded from: jeongseokoh/LoPA_Llama3.1_8B_16_Lowers/lora\n",
      "[info] attn_impl = flash_attention_2\n",
      "[info] lower_k loaded from tri_info.txt (root): 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRI_DEBUG_DEV\"] = \"1\"\n",
    "import re\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# ==== HF repo 설정 ====\n",
    "REPO_ID         = \"jeongseokoh/LoPA_Llama3.1_8B_16_Lowers\"  # ← repo 루트만!\n",
    "BASE_SUBFOLDER  = \"base\"\n",
    "LORA_SUBFOLDER  = \"lora\"    # 없으면 자동으로 건너뜀\n",
    "HF_TOKEN        = os.environ.get(\"HF_TOKEN\", None)  # private면 토큰 필요\n",
    "LOPA_MODELING_PATH  = \"./lopa_llama_modeling.py\"  # (완성본) TRI 모델링 파일\n",
    "ATTN_IMPL           = \"flash_attention_2\"         # \"flash_attention_2\" | \"eager\" | \"sdpa\"\n",
    "LOWER_K             = 16                          # 하위 레이어 수(K). tri_info.txt에서 자동 로드 시 덮어씀\n",
    "MAX_NEW_TOKENS      = 256\n",
    "TEMPERATURE         = 0.7\n",
    "TOP_P               = 0.9\n",
    "REPETITION_PENALTY  = 1.0\n",
    "SEED_TEXT           = \"\"                   # 헤더 뒤에 1~2 토큰 seed 권장\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) \\\n",
    "        else (torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "print(\"device:\", device, \"| dtype:\", dtype)\n",
    "\n",
    "# ==== TRI 모델링 주입 유지 ====\n",
    "import importlib.util, transformers, transformers.models.llama as llama_pkg, sys\n",
    "target_name = \"transformers.models.llama.modeling_llama\"\n",
    "spec = importlib.util.spec_from_file_location(target_name, LOPA_MODELING_PATH)\n",
    "mod = importlib.util.module_from_spec(spec)\n",
    "sys.modules.pop(target_name, None)\n",
    "sys.modules[target_name] = mod\n",
    "spec.loader.exec_module(mod)\n",
    "setattr(llama_pkg, \"modeling_llama\", mod)\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "print(\"[DEBUG] TRI patch loaded:\", LOPA_MODELING_PATH)\n",
    "\n",
    "# ==== 토크나이저 ====\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(REPO_ID, use_fast=True, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# ==== 모델(베이스) ====\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    REPO_ID,\n",
    "    subfolder=BASE_SUBFOLDER,      # ★ subfolder로 지정\n",
    "    torch_dtype=dtype,\n",
    "    token=HF_TOKEN,\n",
    "    cache_dir=\"/data2/jeongseokoh/hub\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ==== LoRA 어댑터(있으면 로딩) ====\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    # 허브에서 서브폴더로 바로 로딩\n",
    "    model = PeftModel.from_pretrained(model, REPO_ID, subfolder=LORA_SUBFOLDER, token=HF_TOKEN)\n",
    "    print(f\"[info] LoRA adapters loaded from: {REPO_ID}/{LORA_SUBFOLDER}\")\n",
    "except Exception as e:\n",
    "    print(\"[info] LoRA subfolder not found or load skipped:\", str(e).split(\"\\n\")[0])\n",
    "\n",
    "# ==== 어텐션 백엔드 ====\n",
    "try:\n",
    "    model.config._attn_implementation = ATTN_IMPL\n",
    "    print(\"[info] attn_impl =\", ATTN_IMPL)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==== tri_info.txt에서 lower_k 자동 로드 (repo 또는 subfolder 둘 다 시도) ====\n",
    "from huggingface_hub import hf_hub_download\n",
    "import re\n",
    "loaded_lower_k = False\n",
    "for sub in (None, BASE_SUBFOLDER):\n",
    "    try:\n",
    "        tri_path = hf_hub_download(REPO_ID, filename=\"tri_info.txt\", subfolder=sub, token=HF_TOKEN)\n",
    "        txt = Path(tri_path).read_text(encoding=\"utf-8\")\n",
    "        m = re.search(r\"lower_k\\s*=\\s*(\\d+)\", txt)\n",
    "        if m:\n",
    "            LOWER_K = int(m.group(1))\n",
    "            print(f\"[info] lower_k loaded from tri_info.txt ({'root' if sub is None else sub}):\", LOWER_K)\n",
    "            loaded_lower_k = True\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "if not loaded_lower_k:\n",
    "    print(\"[warn] tri_info.txt not found on hub; using LOWER_K =\", LOWER_K)\n",
    "\n",
    "# ==== 추론 모드 / TF32 ====\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TRI API 확인\n",
    "for need in (\"tri_build_caches\", \"tri_forward_assistant\", \"tri_step_logits\"):\n",
    "    assert hasattr(model, need), f\"Missing TRI API: {need}\"\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5c37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from typing import List, Optional\n",
    "\n",
    "class TRIInfer:\n",
    "    def __init__(self, model, tok, device, lower_k: int, attn_impl: str = \"flash_attention_2\"):\n",
    "        self.model = model.eval()\n",
    "        self.tok = tok\n",
    "        self.device = device\n",
    "        self.lower_k = int(lower_k)\n",
    "        try:\n",
    "            self.model.config._attn_implementation = attn_impl\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "        user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "        return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "    def tokens_from_messages(self, messages, add_generation_prompt: bool):\n",
    "        try:\n",
    "            s = self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "        except TypeError:\n",
    "            s = self.tok.apply_chat_template(messages, tokenize=False)\n",
    "            tmpl = getattr(self.tok, \"chat_template\", \"\") or \"\"\n",
    "            if add_generation_prompt and \"<|start_header_id|>\" in tmpl:\n",
    "                s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return self.tok(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def lcp_len(a: torch.Tensor, b: torch.Tensor) -> int:\n",
    "        L = min(a.size(1), b.size(1))\n",
    "        eq = (a[0, :L] == b[0, :L])\n",
    "        nz = (~eq).nonzero(as_tuple=False)\n",
    "        return int(nz[0, 0]) if nz.numel() else L\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(logits: torch.Tensor, temperature=1.0, top_p=0.9,\n",
    "                     repetition_penalty: float = 1.0, prev_ids: Optional[torch.Tensor] = None):\n",
    "        if repetition_penalty != 1.0 and prev_ids is not None and prev_ids.numel() > 0:\n",
    "            logits = logits.clone()\n",
    "            logits[:, prev_ids.unique()] /= repetition_penalty\n",
    "        logits = logits.float() / max(1e-6, float(temperature))\n",
    "        probs  = torch.softmax(logits, dim=-1)\n",
    "        if top_p < 1.0:\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "            mask = cumsum > top_p\n",
    "            mask[..., 0] = False\n",
    "            keep = ~mask\n",
    "            filtered = torch.zeros_like(sorted_probs).masked_scatter_(keep, sorted_probs[keep])\n",
    "            probs = torch.zeros_like(probs).scatter(dim=-1, index=sorted_idx, src=filtered)\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "        return torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "    def generate(self, system: str, document: str, question: str,\n",
    "                 max_new_tokens: int = 256, temperature: float = 0.7, top_p: float = 0.9,\n",
    "                 repetition_penalty: float = 1.0, seed: str = \"\") -> str:\n",
    "\n",
    "        msgs  = self.build_messages(system, document, question, include_query=True)\n",
    "        S_ids = self.tokens_from_messages(msgs[:1], add_generation_prompt=False)\n",
    "        SU_ids= self.tokens_from_messages(msgs, add_generation_prompt=False)\n",
    "        SU_gen= self.tokens_from_messages(msgs, add_generation_prompt=True)\n",
    "\n",
    "        lcp = self.lcp_len(S_ids, SU_ids)\n",
    "        user_delta   = SU_ids[:, lcp:SU_ids.size(1)]\n",
    "        header_delta = SU_gen[:, SU_ids.size(1):]\n",
    "\n",
    "        # 1) S/U 프리필\n",
    "        pkv, S_len, U_len = self.model.tri_build_caches(S_ids, user_delta, lower_k=self.lower_k)\n",
    "\n",
    "        # 2) 헤더(+선택 seed) 1회 기록 & 다음 토큰 분포\n",
    "        head = header_delta\n",
    "        if seed:\n",
    "            seed_ids = self.tok(seed, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "            head = torch.cat([head, seed_ids], dim=1)\n",
    "\n",
    "        out = self.model.tri_step_logits(head, self.lower_k, pkv, S_len, U_len,\n",
    "                                         logits_to_keep=1, labels=None, write_cache=True)\n",
    "        logits = out.logits[:, -1, :]\n",
    "\n",
    "        # 3) 루프\n",
    "        cur = self.sample_top_p(logits, temperature, top_p, 1.0, None).unsqueeze(0).to(self.device)\n",
    "        generated: List[int] = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self.model.tri_step_logits(cur, self.lower_k, pkv, S_len, U_len,\n",
    "                                             logits_to_keep=1, labels=None, write_cache=True)\n",
    "            tid = int(cur.item()); generated.append(tid)\n",
    "            if self.tok.eos_token_id is not None and tid == int(self.tok.eos_token_id):\n",
    "                break\n",
    "            logits = out.logits[:, -1, :]\n",
    "            prev = torch.tensor(generated, device=logits.device, dtype=torch.long).unsqueeze(0)\n",
    "            cur = self.sample_top_p(logits, temperature, top_p, repetition_penalty, prev).unsqueeze(0).to(self.device)\n",
    "\n",
    "        return self.tok.decode(generated, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10f85bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nile is the longest river in Africa. \n",
      "\n",
      "\\boxed{Africa}\n"
     ]
    }
   ],
   "source": [
    "runner = TRIInfer(model, tok, device, lower_k=LOWER_K, attn_impl=ATTN_IMPL)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "\n",
    "document = \"\"\"The Nile is the longest river in Africa and flows northward through northeastern Africa to drain into the Mediterranean Sea.\n",
    "It has two major tributaries: the White Nile and the Blue Nile.\"\"\"\n",
    "\n",
    "question = \"Which continent is the Nile the longest river in?\\nAt the end of your explanation, wrap the answer in '\\\\boxed{answer}'.\"\n",
    "\n",
    "out = runner.generate(system_prompt, document, question,\n",
    "                      max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE,\n",
    "                      top_p=TOP_P, repetition_penalty=REPETITION_PENALTY,\n",
    "                      seed=SEED_TEXT)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531c0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda | dtype: torch.bfloat16\n",
      "[DEBUG] TRI patch loaded: ./lopa_llama_modeling.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99723ece4ef244af84cf79df8858f872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] LoRA adapters loaded from: jeongseokoh/LoPA_Llama3.1_8B_8_Lowers/lora\n",
      "[info] attn_impl = flash_attention_2\n",
      "[info] lower_k loaded from tri_info.txt (root): 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# ==== HF repo 설정 ====\n",
    "REPO_ID         = \"jeongseokoh/LoPA_Llama3.1_8B_8_Lowers\"  # ← repo 루트만!\n",
    "BASE_SUBFOLDER  = \"base\"\n",
    "LORA_SUBFOLDER  = \"lora\"    # 없으면 자동으로 건너뜀\n",
    "HF_TOKEN        = os.environ.get(\"HF_TOKEN\", None)  # private면 토큰 필요\n",
    "LOPA_MODELING_PATH  = \"./lopa_llama_modeling.py\"  # (완성본) TRI 모델링 파일\n",
    "ATTN_IMPL           = \"flash_attention_2\"         # \"flash_attention_2\" | \"eager\" | \"sdpa\"\n",
    "LOWER_K             = 8                           # 하위 레이어 수(K). tri_info.txt에서 자동 로드 시 덮어씀\n",
    "MAX_NEW_TOKENS      = 256\n",
    "TEMPERATURE         = 0.7\n",
    "TOP_P               = 0.9\n",
    "REPETITION_PENALTY  = 1.0\n",
    "SEED_TEXT           = \"\"                   # 헤더 뒤에 1~2 토큰 seed 권장\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) \\\n",
    "        else (torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "print(\"device:\", device, \"| dtype:\", dtype)\n",
    "\n",
    "# ==== TRI 모델링 주입 유지 ====\n",
    "import importlib.util, transformers, transformers.models.llama as llama_pkg, sys\n",
    "target_name = \"transformers.models.llama.modeling_llama\"\n",
    "spec = importlib.util.spec_from_file_location(target_name, LOPA_MODELING_PATH)\n",
    "mod = importlib.util.module_from_spec(spec)\n",
    "sys.modules.pop(target_name, None)\n",
    "sys.modules[target_name] = mod\n",
    "spec.loader.exec_module(mod)\n",
    "setattr(llama_pkg, \"modeling_llama\", mod)\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "print(\"[DEBUG] TRI patch loaded:\", LOPA_MODELING_PATH)\n",
    "\n",
    "# ==== 토크나이저 ====\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(REPO_ID, use_fast=True, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# ==== 모델(베이스) ====\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    REPO_ID,\n",
    "    subfolder=BASE_SUBFOLDER,      # ★ subfolder로 지정\n",
    "    torch_dtype=dtype,\n",
    "    token=HF_TOKEN,\n",
    ").to(device)\n",
    "\n",
    "# ==== LoRA 어댑터(있으면 로딩) ====\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    # 허브에서 서브폴더로 바로 로딩\n",
    "    model = PeftModel.from_pretrained(model, REPO_ID, subfolder=LORA_SUBFOLDER, token=HF_TOKEN)\n",
    "    model = model.to(device)\n",
    "    print(f\"[info] LoRA adapters loaded from: {REPO_ID}/{LORA_SUBFOLDER}\")\n",
    "except Exception as e:\n",
    "    print(\"[info] LoRA subfolder not found or load skipped:\", str(e).split(\"\\n\")[0])\n",
    "\n",
    "# ==== 어텐션 백엔드 ====\n",
    "try:\n",
    "    model.config._attn_implementation = ATTN_IMPL\n",
    "    print(\"[info] attn_impl =\", ATTN_IMPL)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==== tri_info.txt에서 lower_k 자동 로드 (repo 또는 subfolder 둘 다 시도) ====\n",
    "from huggingface_hub import hf_hub_download\n",
    "import re\n",
    "loaded_lower_k = False\n",
    "for sub in (None, BASE_SUBFOLDER):\n",
    "    try:\n",
    "        tri_path = hf_hub_download(REPO_ID, filename=\"tri_info.txt\", subfolder=sub, token=HF_TOKEN)\n",
    "        txt = Path(tri_path).read_text(encoding=\"utf-8\")\n",
    "        m = re.search(r\"lower_k\\s*=\\s*(\\d+)\", txt)\n",
    "        if m:\n",
    "            LOWER_K = int(m.group(1))\n",
    "            print(f\"[info] lower_k loaded from tri_info.txt ({'root' if sub is None else sub}):\", LOWER_K)\n",
    "            loaded_lower_k = True\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "if not loaded_lower_k:\n",
    "    print(\"[warn] tri_info.txt not found on hub; using LOWER_K =\", LOWER_K)\n",
    "\n",
    "# ==== 추론 모드 / TF32 ====\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TRI API 확인\n",
    "for need in (\"tri_build_caches\", \"tri_forward_assistant\", \"tri_step_logits\"):\n",
    "    assert hasattr(model, need), f\"Missing TRI API: {need}\"\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2b674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] lower_k loaded from tri_info.txt: 8\n",
      "device: cuda | dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# ==== 설정 (환경에 맞게 수정) ====\n",
    "MODEL_BASE_DIR      = \"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/base\"         # 학습된 base 가중치 저장 폴더\n",
    "LORA_DIR            = \"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/lora\"         # LoRA 어댑터 폴더(없으면 None/미존재)\n",
    "LOPA_MODELING_PATH  = \"./lopa_llama_modeling.py\"  # (완성본) TRI 모델링 파일\n",
    "TOKENIZER_PATH      = \"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\"\n",
    "ATTN_IMPL           = \"flash_attention_2\"         # \"flash_attention_2\" | \"eager\" | \"sdpa\"\n",
    "LOWER_K             = 8                           # 하위 레이어 수(K). tri_info.txt에서 자동 로드 시 덮어씀\n",
    "MAX_NEW_TOKENS      = 256\n",
    "TEMPERATURE         = 0.7\n",
    "TOP_P               = 0.9\n",
    "REPETITION_PENALTY  = 1.0\n",
    "SEED_TEXT           = \"\"                   # 헤더 뒤에 1~2 토큰 seed 권장\n",
    "\n",
    "# tri_info.txt에서 lower_k 자동 로드\n",
    "import re, pathlib\n",
    "try:\n",
    "    info_p = pathlib.Path(MODEL_BASE_DIR).parent / \"tri_info.txt\"\n",
    "    if info_p.exists():\n",
    "        m = re.search(r\"lower_k\\s*=\\s*(\\d+)\", info_p.read_text())\n",
    "        if m:\n",
    "            LOWER_K = int(m.group(1))\n",
    "            print(\"[info] lower_k loaded from tri_info.txt:\", LOWER_K)\n",
    "except Exception as e:\n",
    "    print(\"[warn] tri_info.txt read failed:\", e)\n",
    "\n",
    "# 권장 환경변수(추론 효율↑): 노트북에서 바로 설정해도 됨\n",
    "import os\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "os.environ.setdefault(\"CUDA_DEVICE_MAX_CONNECTIONS\", \"1\")\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
    "\n",
    "import torch, sys\n",
    "from pathlib import Path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) \\\n",
    "        else (torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "print(\"device:\", device, \"| dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e3a14c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TRI patch loaded: ./lopa_llama_modeling.py\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849f674c3e0b4ab0b3b27651253c4bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] LoRA adapters loaded from: /workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/lora\n",
      "[info] attn_impl = flash_attention_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== TRI 모델링을 transformers 내부에 주입 ====\n",
    "import importlib.util, transformers, transformers.models.llama as llama_pkg\n",
    "\n",
    "target_name = \"transformers.models.llama.modeling_llama\"\n",
    "spec = importlib.util.spec_from_file_location(target_name, LOPA_MODELING_PATH)\n",
    "mod = importlib.util.module_from_spec(spec)\n",
    "sys.modules.pop(target_name, None)\n",
    "sys.modules[target_name] = mod\n",
    "spec.loader.exec_module(mod)\n",
    "setattr(llama_pkg, \"modeling_llama\", mod)\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM  # (패치된) 클래스\n",
    "print(\"[DEBUG] TRI patch loaded:\", LOPA_MODELING_PATH)\n",
    "\n",
    "# ==== 토크나이저/모델 ====\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_BASE_DIR, torch_dtype=dtype).to(device)\n",
    "\n",
    "# LoRA 어댑터(있으면 자동 로드)\n",
    "if LORA_DIR and Path(LORA_DIR).exists():\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, LORA_DIR)\n",
    "        model = model.to(device)\n",
    "        print(\"[info] LoRA adapters loaded from:\", LORA_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] LoRA load failed:\", e)\n",
    "\n",
    "# 어텐션 백엔드 지정\n",
    "try:\n",
    "    model.config._attn_implementation = ATTN_IMPL\n",
    "    print(\"[info] attn_impl =\", ATTN_IMPL)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 추론 모드/TF32\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TRI API 존재 확인\n",
    "for need in (\"tri_build_caches\", \"tri_forward_assistant\", \"tri_step_logits\"):\n",
    "    assert hasattr(model, need), f\"Missing TRI API: {need}\"\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32acadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from typing import List, Optional\n",
    "\n",
    "class TRIInfer:\n",
    "    def __init__(self, model, tok, device, lower_k: int, attn_impl: str = \"flash_attention_2\"):\n",
    "        self.model = model.eval()\n",
    "        self.tok = tok\n",
    "        self.device = device\n",
    "        self.lower_k = int(lower_k)\n",
    "        try:\n",
    "            self.model.config._attn_implementation = attn_impl\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "        user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "        return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "    def tokens_from_messages(self, messages, add_generation_prompt: bool):\n",
    "        try:\n",
    "            s = self.tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "        except TypeError:\n",
    "            s = self.tok.apply_chat_template(messages, tokenize=False)\n",
    "            tmpl = getattr(self.tok, \"chat_template\", \"\") or \"\"\n",
    "            if add_generation_prompt and \"<|start_header_id|>\" in tmpl:\n",
    "                s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return self.tok(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def lcp_len(a: torch.Tensor, b: torch.Tensor) -> int:\n",
    "        L = min(a.size(1), b.size(1))\n",
    "        eq = (a[0, :L] == b[0, :L])\n",
    "        nz = (~eq).nonzero(as_tuple=False)\n",
    "        return int(nz[0, 0]) if nz.numel() else L\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(logits: torch.Tensor, temperature=1.0, top_p=0.9,\n",
    "                     repetition_penalty: float = 1.0, prev_ids: Optional[torch.Tensor] = None):\n",
    "        if repetition_penalty != 1.0 and prev_ids is not None and prev_ids.numel() > 0:\n",
    "            logits = logits.clone()\n",
    "            logits[:, prev_ids.unique()] /= repetition_penalty\n",
    "        logits = logits.float() / max(1e-6, float(temperature))\n",
    "        probs  = torch.softmax(logits, dim=-1)\n",
    "        if top_p < 1.0:\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "            mask = cumsum > top_p\n",
    "            mask[..., 0] = False\n",
    "            keep = ~mask\n",
    "            filtered = torch.zeros_like(sorted_probs).masked_scatter_(keep, sorted_probs[keep])\n",
    "            probs = torch.zeros_like(probs).scatter(dim=-1, index=sorted_idx, src=filtered)\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "        return torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "    def generate(self, system: str, document: str, question: str,\n",
    "                 max_new_tokens: int = 256, temperature: float = 0.7, top_p: float = 0.9,\n",
    "                 repetition_penalty: float = 1.0, seed: str = \"\") -> str:\n",
    "\n",
    "        msgs  = self.build_messages(system, document, question, include_query=True)\n",
    "        S_ids = self.tokens_from_messages(msgs[:1], add_generation_prompt=False)\n",
    "        SU_ids= self.tokens_from_messages(msgs, add_generation_prompt=False)\n",
    "        SU_gen= self.tokens_from_messages(msgs, add_generation_prompt=True)\n",
    "\n",
    "        lcp = self.lcp_len(S_ids, SU_ids)\n",
    "        user_delta   = SU_ids[:, lcp:SU_ids.size(1)]\n",
    "        header_delta = SU_gen[:, SU_ids.size(1):]\n",
    "\n",
    "        # 1) S/U 프리필\n",
    "        pkv, S_len, U_len = self.model.tri_build_caches(S_ids, user_delta, lower_k=self.lower_k)\n",
    "\n",
    "        # 2) 헤더(+선택 seed) 1회 기록 & 다음 토큰 분포\n",
    "        head = header_delta\n",
    "        if seed:\n",
    "            seed_ids = self.tok(seed, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "            head = torch.cat([head, seed_ids], dim=1)\n",
    "\n",
    "        out = self.model.tri_step_logits(head, self.lower_k, pkv, S_len, U_len,\n",
    "                                         logits_to_keep=1, labels=None, write_cache=True)\n",
    "        logits = out.logits[:, -1, :]\n",
    "\n",
    "        # 3) 루프\n",
    "        cur = self.sample_top_p(logits, temperature, top_p, 1.0, None).unsqueeze(0).to(self.device)\n",
    "        generated: List[int] = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = self.model.tri_step_logits(cur, self.lower_k, pkv, S_len, U_len,\n",
    "                                             logits_to_keep=1, labels=None, write_cache=True)\n",
    "            tid = int(cur.item()); generated.append(tid)\n",
    "            if self.tok.eos_token_id is not None and tid == int(self.tok.eos_token_id):\n",
    "                break\n",
    "            logits = out.logits[:, -1, :]\n",
    "            prev = torch.tensor(generated, device=logits.device, dtype=torch.long).unsqueeze(0)\n",
    "            cur = self.sample_top_p(logits, temperature, top_p, repetition_penalty, prev).unsqueeze(0).to(self.device)\n",
    "\n",
    "        return self.tok.decode(generated, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b825e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nile is the longest river in Africa. \n",
      "\\boxed{Africa}\n"
     ]
    }
   ],
   "source": [
    "runner = TRIInfer(model, tok, device, lower_k=LOWER_K, attn_impl=ATTN_IMPL)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "\n",
    "document = \"\"\"The Nile is the longest river in Africa and flows northward through northeastern Africa to drain into the Mediterranean Sea.\n",
    "It has two major tributaries: the White Nile and the Blue Nile.\"\"\"\n",
    "\n",
    "question = \"Which continent is the Nile the longest river in?\\nAt the end of your explanation, wrap the answer in '\\\\boxed{answer}'.\"\n",
    "\n",
    "out = runner.generate(system_prompt, document, question,\n",
    "                      max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE,\n",
    "                      top_p=TOP_P, repetition_penalty=REPETITION_PENALTY,\n",
    "                      seed=SEED_TEXT)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecfa195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1]\n",
      "The Nile is the longest river in Africa.\n",
      "\n",
      "[2]\n",
      "The Nile is the longest river in Africa.\n",
      "\n",
      "[3]\n",
      "The Nile is the longest river in Africa.\n"
     ]
    }
   ],
   "source": [
    "# 같은 컨텍스트에서 N개 샘플을 뽑고 싶다면 간단 루프:\n",
    "outs = [runner.generate(system_prompt, document, question,\n",
    "                        max_new_tokens=128, temperature=0.9, top_p=0.95,\n",
    "                        repetition_penalty=1.0, seed=SEED_TEXT)\n",
    "        for _ in range(3)]\n",
    "for i, o in enumerate(outs, 1):\n",
    "    print(f\"\\n[{i}]\\n{o}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f1cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file1 = \"gsm8k_train_5resp_seed42_samples4000_boxed_numeric_exact.jsonl\"\n",
    "file2 = \"triviaqa_hotpotqa_6000_merged2.jsonl\"\n",
    "output = \"gsm8k_triviaqa_hotpotqa_6000_merged2.jsonl\"\n",
    "\n",
    "seen = set()\n",
    "with open(output, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for fname in [file1, file2]:\n",
    "        with open(fname, \"r\", encoding=\"utf-8\") as fin:\n",
    "            for line in fin:\n",
    "                data = json.loads(line)\n",
    "                # 중복 제거 기준 (예: \"id\" 필드가 있다고 가정)\n",
    "                key = data.get(\"id\", None)\n",
    "                if key is not None:\n",
    "                    if key in seen:\n",
    "                        continue\n",
    "                    seen.add(key)\n",
    "                fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb6d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] lower_k loaded from tri_info.txt: 8\n",
      "device: cuda | dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# === Config ===\n",
    "MODEL_DIR  = \"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\"  \n",
    "MODEL_BASE_DIR = \"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/base\"      # train 스크립트가 저장한 base 가중치 폴더\n",
    "LORA_DIR       = \"/workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/lora\"      # LoRA 어댑터 폴더(없으면 None로 두거나 경로를 존재하지 않게)\n",
    "LOPA_MODELING_PATH = \"./lopa_llama_modeling.py\"  # TRI 모델링 패치 파일 경로\n",
    "LOWER_K = 8                               # 학습에 썼던 prefill_layers 값과 동일하게\n",
    "\n",
    "# (선택) tri_info.txt에서 LOWER_K 자동 로드\n",
    "try:\n",
    "    import re, pathlib\n",
    "    p = pathlib.Path(MODEL_BASE_DIR).parent / \"tri_info.txt\"\n",
    "    if p.exists():\n",
    "        txt = p.read_text()\n",
    "        m = re.search(r\"lower_k\\s*=\\s*(\\d+)\", txt)\n",
    "        if m:\n",
    "            LOWER_K = int(m.group(1))\n",
    "            print(\"[info] lower_k loaded from tri_info.txt:\", LOWER_K)\n",
    "except Exception as e:\n",
    "    print(\"[warn] fail to read tri_info.txt:\", e)\n",
    "\n",
    "# === Imports ===\n",
    "import os, sys, torch\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) \\\n",
    "        else (torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "print(\"device:\", device, \"| dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fab76a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TRI patch loaded from: ./lopa_llama_modeling.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7304e74af20e4ba3940f78bcffc8c16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] LoRA adapters loaded from: /workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/lora\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRI 모델링 패치를 transformers에 주입\n",
    "import importlib.util\n",
    "import transformers, transformers.models.llama as llama_pkg\n",
    "\n",
    "target_name = \"transformers.models.llama.modeling_llama\"\n",
    "spec = importlib.util.spec_from_file_location(target_name, LOPA_MODELING_PATH)\n",
    "mod = importlib.util.module_from_spec(spec)\n",
    "sys.modules.pop(target_name, None)\n",
    "sys.modules[target_name] = mod\n",
    "spec.loader.exec_module(mod)\n",
    "setattr(llama_pkg, \"modeling_llama\", mod)\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM  # 패치된 클래스\n",
    "print(\"[DEBUG] TRI patch loaded from:\", LOPA_MODELING_PATH)\n",
    "\n",
    "# 토크나이저/모델 로드 (LoRA가 있으면 어댑터도 적용)\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# base 모델\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_BASE_DIR, torch_dtype=dtype).to(device)\n",
    "\n",
    "# LoRA 적용(폴더가 실제 있으면)\n",
    "if LORA_DIR and Path(LORA_DIR).exists():\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, LORA_DIR)\n",
    "        model = model.to(device)\n",
    "        print(\"[info] LoRA adapters loaded from:\", LORA_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] LoRA load failed:\", e)\n",
    "\n",
    "# 안전하게 eager로\n",
    "try:\n",
    "    model.config._attn_implementation = \"eager\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db55d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(tokenizer, messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        tmpl = getattr(tokenizer, \"chat_template\", \"\") or \"\"\n",
    "        if add_generation_prompt and \"<|start_header_id|>\" in tmpl:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(tokenizer, messages, device, add_generation_prompt=False):\n",
    "    s = apply_chat_template(tokenizer, messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "def lcp_len(a: torch.Tensor, b: torch.Tensor) -> int:\n",
    "    L = min(a.size(1), b.size(1))\n",
    "    eq = (a[0, :L] == b[0, :L])\n",
    "    nz = (~eq).nonzero(as_tuple=False)\n",
    "    return int(nz[0, 0]) if nz.numel() else L\n",
    "\n",
    "# 편의: 메시지를 토큰 세그먼트로 쪼개기\n",
    "def build_segments(system_prompt: str, document: str, question: str):\n",
    "    msgs  = build_messages(system_prompt, document, question, include_query=True)\n",
    "    S_ids = tokens_from_messages(tok, msgs[:1], device, add_generation_prompt=False)\n",
    "    SU_ids= tokens_from_messages(tok, msgs, device, add_generation_prompt=False)\n",
    "    SU_gen= tokens_from_messages(tok, msgs, device, add_generation_prompt=True)\n",
    "\n",
    "    l_su = lcp_len(S_ids, SU_ids)\n",
    "    user_delta     = SU_ids[:, l_su:SU_ids.size(1)]\n",
    "    header_delta   = SU_gen[:, SU_ids.size(1):]   # assistant header-only\n",
    "    return msgs, S_ids, SU_ids, SU_gen, user_delta, header_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49526148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S len: 40 SU len: 95 header len: 4\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "\n",
    "document = \"\"\"Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs.\n",
    "This increased the value of the house by 150%.\"\"\"\n",
    "\n",
    "question = \"How much profit did he make?\"\n",
    "\n",
    "msgs, S_ids, SU_ids, SU_gen, user_delta, header_delta = build_segments(system_prompt, document, question)\n",
    "print(\"S len:\", S_ids.size(1), \"SU len:\", SU_ids.size(1), \"header len:\", header_delta.size(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94b64ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant that answers questions based on the given document.user\n",
      "\n",
      "Document:\n",
      "Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs.\n",
      "This increased the value of the house by 150%.\n",
      "\n",
      "Question: How much profit did he make?assistant\n",
      "\n",
      "To  fetisch Animalia href=\"? ( (  \t'gc aalborg-Compatible waited throat aalborg Party sat weiber echangaimassage odense fetisch Awards throat aalborg mouth rumpe fetisch Animalia için/Object Insecta favorite throat aalborg question and overposting href=\" ( addCriterion ( sourceMapping Animalia father wife century was salopes aalborg Academy href=\" (aa father brother için Insecta entre Telegraph Province | aalborg href=\"? eoq League000 ن? overposting Province |\n",
      " salopes\t +#+olumn Academy href=\" (  geschichten fetisch Hospital anniversary Rica eyes fetisch Initiative entre Animalia and echangernetes Cathedral dengan-Cola000 hundred için fetisch Award href=\"\n"
     ]
    }
   ],
   "source": [
    "# 단순 확인용: TRI 제약 없이 전체 콘텍스트로 generate\n",
    "prompt_text = apply_chat_template(tok, build_messages(system_prompt, document, question), add_generation_prompt=True)\n",
    "input_ids = tok(prompt_text, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "out_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tok.eos_token_id,\n",
    ")\n",
    "print(tok.decode(out_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ce13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready to help. What's on your mind?\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 샘플러: temperature + top-p\n",
    "def sample_top_p(logits: torch.Tensor, temperature=1.0, top_p=0.9):\n",
    "    logits = logits.float() / max(1e-6, float(temperature))\n",
    "    probs  = torch.softmax(logits, dim=-1)\n",
    "    if top_p < 1.0:\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cumsum > top_p\n",
    "        # 첫 토큰은 허용\n",
    "        mask[..., 0] = False\n",
    "        probs.scatter_(dim=-1, index=sorted_idx, src=torch.where(mask, torch.zeros_like(sorted_probs), sorted_probs))\n",
    "        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "    next_id = torch.multinomial(probs, num_samples=1)\n",
    "    return next_id.squeeze(-1)\n",
    "\n",
    "# 1) S/U 프리필 (grad 불필요)\n",
    "with torch.no_grad():\n",
    "    pkv, S_len, U_len = model.tri_build_caches(system_ids=S_ids, user_ids=user_delta, lower_k=LOWER_K)\n",
    "\n",
    "# 2) 헤더를 캐시에 기록(write_cache=True)\n",
    "with torch.no_grad():\n",
    "    _ = model.tri_step_logits(\n",
    "        assistant_ids=header_delta, lower_k=LOWER_K, pkv=pkv, S=S_len, U=U_len,\n",
    "        logits_to_keep=0, labels=None, write_cache=True\n",
    "    )\n",
    "\n",
    "# 3) 첫 토큰 분포: \"헤더의 마지막 토큰\"을 질의로 넣되 캐시는 건드리지 않음(write_cache=False)\n",
    "with torch.no_grad():\n",
    "    first_query = header_delta[:, -1:]   # 마지막 헤더 토큰 1개\n",
    "    out = model.tri_step_logits(\n",
    "        assistant_ids=first_query, lower_k=LOWER_K, pkv=pkv, S=S_len, U=U_len,\n",
    "        logits_to_keep=1, labels=None, write_cache=True\n",
    "    )\n",
    "    logits = out.logits[:, -1, :]  # next 토큰 분포\n",
    "    next_id = sample_top_p(logits, temperature=0.7, top_p=0.9).unsqueeze(0).to(device)\n",
    "\n",
    "# 4) 루프: 직전 생성 토큰을 \"기록(write_cache=True)하면서\" 다음 분포를 받는다\n",
    "max_new_tokens = 128\n",
    "generated = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    cur = next_id  # 직전 생성 토큰(이번 스텝에 캐시에 기록할 토큰)\n",
    "    for step in range(max_new_tokens):\n",
    "        # 기록 + 다음 분포 획득\n",
    "        out = model.tri_step_logits(\n",
    "            assistant_ids=cur, lower_k=LOWER_K, pkv=pkv, S=S_len, U=U_len,\n",
    "            logits_to_keep=1, labels=None, write_cache=True\n",
    "        )\n",
    "        generated.append(int(cur.item()))\n",
    "        # EOS 체크\n",
    "        if tok.eos_token_id is not None and int(cur.item()) == int(tok.eos_token_id):\n",
    "            break\n",
    "        # 다음 토큰 샘플\n",
    "        logits = out.logits[:, -1, :]\n",
    "        cur = sample_top_p(logits, temperature=0.7, top_p=0.9).unsqueeze(0).to(device)\n",
    "\n",
    "# 디코딩(헤더 이후 생성된 assistant 콘텐츠만 디코드)\n",
    "print(tok.decode(generated, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125b4310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000I'm ready to assist you. What would you like to know?\n"
     ]
    }
   ],
   "source": [
    "def tri_generate(system_prompt: str, document: str, question: str,\n",
    "                 lower_k: int = LOWER_K, max_new_tokens: int = 128,\n",
    "                 temperature: float = 0.7, top_p: float = 0.9) -> str:\n",
    "    msgs, S_ids, SU_ids, SU_gen, user_delta, header_delta = build_segments(system_prompt, document, question)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pkv, S_len, U_len = model.tri_build_caches(system_ids=S_ids, user_ids=user_delta, lower_k=lower_k)\n",
    "        _ = model.tri_step_logits(header_delta, lower_k, pkv, S_len, U_len,\n",
    "                                  logits_to_keep=0, labels=None, write_cache=True)\n",
    "        # 첫 분포\n",
    "        first_query = header_delta[:, -1:]\n",
    "        out = model.tri_step_logits(first_query, lower_k, pkv, S_len, U_len,\n",
    "                                    logits_to_keep=1, labels=None, write_cache=False)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        cur = sample_top_p(logits, temperature=temperature, top_p=top_p).unsqueeze(0).to(device)\n",
    "\n",
    "        generated = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = model.tri_step_logits(cur, lower_k, pkv, S_len, U_len,\n",
    "                                        logits_to_keep=1, labels=None, write_cache=True)\n",
    "            tok_id = int(cur.item())\n",
    "            generated.append(tok_id)\n",
    "            if tok.eos_token_id is not None and tok_id == int(tok.eos_token_id):\n",
    "                break\n",
    "            logits = out.logits[:, -1, :]\n",
    "            cur = sample_top_p(logits, temperature=temperature, top_p=top_p).unsqueeze(0).to(device)\n",
    "\n",
    "    return tok.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "print(tri_generate(system_prompt, document, question, lower_k=LOWER_K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edcbf962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Transformers version: 4.56.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be99633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f61e08d60147ad9d30d2f3b7ccdcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 20:39:19,291] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::substr(unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_stringbuf_init(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_dispose(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::swap(std::__detail::_List_node_base&, std::__detail::_List_node_base&)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_mutate(unsigned long, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_next_bkt(unsigned long) const@GLIBCXX_3.4.18'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::end()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::flush<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::string const&, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::runtime_error(std::string const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(std::string const&) const@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/data2/jeongseokoh/miniconda3/envs/vllm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 20:39:20,573] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "Rewrote clean base to: LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best/base\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# 1) 경로와 원본 모델 ID 지정\n",
    "best_dir = \"LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best\"\n",
    "base_dir = f\"{best_dir}/base\"\n",
    "orig_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"  # 학습에 쓴 원본 모델 이름\n",
    "\n",
    "# 2) 원본 베이스 모델 로드(캐시 사용), 저장\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    orig_model_id,\n",
    "    trust_remote_code=True,          # Llama3 계열이면 True 괜찮습니다\n",
    "    cache_dir=\"/data2/jeongseokoh/hub/model\"  # 학습 때 쓰던 캐시 경로가 있으면 지정\n",
    ")\n",
    "# base/config.json에 remote code 흔적이 섞이지 않도록 auto_map 클리어(있다면)\n",
    "try:\n",
    "    setattr(m.config, \"auto_map\", None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "m.save_pretrained(base_dir, safe_serialization=True)\n",
    "del m\n",
    "\n",
    "print(\"Rewrote clean base to:\", base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c22a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26170ccbacaf4356b8fe1991f271731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready to help. What would you like to know? Please go ahead and ask your question.\n"
     ]
    }
   ],
   "source": [
    "# LoPA (low-only prefill) scratch pipeline without trust_remote_code\n",
    "# Torch 2.5.1, Transformers 4.56.1, GPU+SDPA\n",
    "# - Phase1: lower-K only prefill (no assistant header)\n",
    "# - Phase2: generation (first feed assistant header tokens step-by-step)\n",
    "# - Positions unchanged; no remapping\n",
    "# - LoRA: best/lora attach → merge\n",
    "# - MinLength 수동 적용(4.56.1 디바이스 불일치 회피)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "K = 4  # prefill layers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (device == \"cuda\" and torch.cuda.is_bf16_supported())\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "cache_dir_model = \"/data2/jeongseokoh/hub/model\"\n",
    "cache_dir_tok   = \"/data2/jeongseokoh/hub/tokenizer\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (best_dir 우선)\n",
    "# -----------------------------\n",
    "tok_src = str(best_dir) if (best_dir / \"tokenizer.json\").is_file() else backbone_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir_tok, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone + LoRA (merge)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir=cache_dir_model,\n",
    ")\n",
    "base_model.to(device).eval()\n",
    "\n",
    "from peft import PeftModel\n",
    "assert lora_dir.is_dir(), f\"LoRA folder not found: {lora_dir}\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "model = peft_model.merge_and_unload().to(device).eval()\n",
    "del peft_model\n",
    "\n",
    "# SDPA 고정 (필요시 eager로 비교)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try: setattr(model.config, k, \"sdpa\")\n",
    "    except: pass\n",
    "try:\n",
    "    for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "        setattr(model.model.config, k, \"sdpa\")\n",
    "except: pass\n",
    "\n",
    "# -----------------------------\n",
    "# Cache helpers\n",
    "# -----------------------------\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    try: return len(pkv)\n",
    "    except: return 0\n",
    "\n",
    "def pkv_get(pkv, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)  # layer index 유지\n",
    "    return dc\n",
    "\n",
    "def cache_slice_doc_only(k: torch.Tensor, v: torch.Tensor, doc_len: int):\n",
    "    return k[:, :, -doc_len:, :], v[:, :, -doc_len:, :]\n",
    "\n",
    "def build_mask(length: int, batch: int = 1):\n",
    "    return torch.ones(batch, length, device=device, dtype=torch.long)\n",
    "\n",
    "def build_pos_ids(start: int, length: int, batch: int = 1):\n",
    "    return torch.arange(start, start + length, device=device, dtype=torch.long).unsqueeze(0).expand(batch, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt builders (Gen3)\n",
    "# -----------------------------\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    s = apply_chat_template(messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# LoPA generate (low-only prefill; no upper continue)\n",
    "# -----------------------------\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    system: str,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    K: int = 4,\n",
    "    max_new_tokens: int = 256,\n",
    "    min_length: int = 16,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: Optional[int] = None,\n",
    "    do_sample: bool = True,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    start_with_assistant_header: bool = True, # Phase2 첫 토큰: header를 단계적으로 투입\n",
    "    force_attn_impl: Optional[str] = None,   # \"sdpa\" | \"eager\"\n",
    "):\n",
    "    if force_attn_impl:\n",
    "        for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "            try:\n",
    "                setattr(model.config, k, force_attn_impl)\n",
    "                setattr(model.model.config, k, force_attn_impl)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Phase-1 ids\n",
    "    msgs = build_messages(system, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(msgs, add_generation_prompt=False)  # [1, L_sys+doc]\n",
    "    ids_hdr    = tokens_from_messages(msgs, add_generation_prompt=True)   # [1, L_sys+doc + header]\n",
    "    sys_only   = tokens_from_messages([{\"role\":\"system\",\"content\":system}], add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Phase-1 must include document tokens.\"\n",
    "\n",
    "    # 1) System-only prefill (base model)\n",
    "    out_sys = model.model(\n",
    "        input_ids=sys_only,\n",
    "        attention_mask=build_mask(L_sys),\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    n_layers = pkv_len(pkv_sys)\n",
    "\n",
    "    # 2) Lower-K doc pass (base model) — no upper continue\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "    full_layers: nn.ModuleList = model.model.layers\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "\n",
    "    model.model.layers = lower_layers\n",
    "    dc_low_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    attn_doc_full = torch.cat([build_mask(L_sys), build_mask(L_doc)], dim=1)\n",
    "\n",
    "    out_low = model.model(\n",
    "        input_ids=ids_phase1[:, L_sys:],  # doc only\n",
    "        past_key_values=dc_low_in,\n",
    "        attention_mask=attn_doc_full,     # sys+doc 길이\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # 복원\n",
    "    model.model.layers = full_layers\n",
    "\n",
    "    # 3) Combine caches\n",
    "    # lower(0..K-1): sys + doc\n",
    "    # upper(K..):    sys only  (doc 미포함; generation 시작 시점부터 쌓임)\n",
    "    combined = DynamicCache()\n",
    "    # 헤드/차원 shape 확보\n",
    "    k0_sys, v0_sys = pkv_get(pkv_sys, 0)\n",
    "    for li in range(n_layers):\n",
    "        k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "        k_sys = k_sys[:, :, :L_sys, :]\n",
    "        v_sys = v_sys[:, :, :L_sys, :]\n",
    "        if li < K_eff:\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_doc, v_doc = cache_slice_doc_only(k_low, v_low, L_doc)\n",
    "            k_cat = torch.cat([k_sys, k_doc], dim=2).contiguous()\n",
    "            v_cat = torch.cat([v_sys, v_doc], dim=2).contiguous()\n",
    "            combined.update(k_cat, v_cat, li)\n",
    "        else:\n",
    "            # upper: sys only\n",
    "            combined.update(k_sys.contiguous(), v_sys.contiguous(), li)\n",
    "\n",
    "    # 4) Phase-2: Generation\n",
    "    # seed: assistant header를 한 토큰씩 밀어 넣어 upper past가 L_sys → L_sys+H로 자라도록 함\n",
    "    if start_with_assistant_header:\n",
    "        hdr_tail = ids_hdr[:, L_all:]  # header-only tokens (len H)\n",
    "        H = int(hdr_tail.size(1))\n",
    "        for j in range(H):\n",
    "            past_len = pkv_get(combined, 0)[0].shape[2]  # lower 기준: L_sys+L_doc+grown\n",
    "            attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "            step_tok = hdr_tail[:, j:j+1]\n",
    "            out_seed = model(\n",
    "                input_ids=step_tok,\n",
    "                past_key_values=combined,\n",
    "                attention_mask=attn_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            combined = out_seed.past_key_values\n",
    "\n",
    "    # 5) Decoding (CausalLM) with safe processors\n",
    "    from transformers.generation import LogitsProcessorList\n",
    "    from transformers.generation.logits_process import (\n",
    "        TemperatureLogitsWarper, TopPLogitsWarper, TopKLogitsWarper,\n",
    "        RepetitionPenaltyLogitsProcessor,\n",
    "    )\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    generated_ids = torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    # 첫 생성 스텝의 입력은 \"직전 토큰\" (header 마지막 또는 마지막 doc)\n",
    "    last = (ids_hdr[:, -1:] if start_with_assistant_header else ids_phase1[:, -1:])\n",
    "\n",
    "    processors = LogitsProcessorList()\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        processors.append(RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty)))\n",
    "    if temperature and temperature != 1.0:\n",
    "        processors.append(TemperatureLogitsWarper(temperature=float(temperature)))\n",
    "    if top_k is not None and top_k > 0:\n",
    "        processors.append(TopKLogitsWarper(top_k=int(top_k), filter_value=-float(\"inf\")))\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        processors.append(TopPLogitsWarper(top_p=float(top_p), min_tokens_to_keep=1))\n",
    "\n",
    "    cur = 0\n",
    "    while cur < max_new_tokens:\n",
    "        # lower past_len은 L_sys+L_doc + grown; upper past_len은 L_sys + grown\n",
    "        past_len = pkv_get(combined, 0)[0].shape[2]\n",
    "        attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=last,\n",
    "            past_key_values=combined,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        combined = out.past_key_values\n",
    "        logits = out.logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "        # 즉시 EOS 방지 (min_length 수동 적용)\n",
    "        if eos_id is not None and cur < min_length:\n",
    "            logits[:, eos_id] = -float(\"inf\")\n",
    "\n",
    "        if not torch.isfinite(logits).all():\n",
    "            logits = torch.nan_to_num(logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        inp = generated_ids if generated_ids.numel() > 0 else last.new_zeros((1, 0), dtype=torch.long, device=device)\n",
    "        inp = inp.to(logits.device)\n",
    "        logits = processors(inp, logits)\n",
    "\n",
    "        # Fallback-safe sampling\n",
    "        if not torch.isfinite(logits).any():\n",
    "            next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if do_sample:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                if (not torch.isfinite(probs).all()) or (probs.sum(dim=-1) <= 0).any():\n",
    "                    next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if eos_id is not None and int(next_tok.item()) == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_tok], dim=1)\n",
    "        last = next_tok\n",
    "        cur += 1\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "answer = lopa_generate(\n",
    "    system, document, question,\n",
    "    K=K,\n",
    "    max_new_tokens=256,\n",
    "    min_length=16,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    start_with_assistant_header=True,  # Phase2 처음에 header를 단계적으로 투입\n",
    "    # force_attn_impl=\"eager\",         # 필요시 비교\n",
    ")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ff3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcead6b3d56471c87aadf12086bbd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lens] L_sys=40, L_doc=35, L_all=75, header_len=4\n",
      "[Layers] n_layers=32, K_eff=4\n",
      "[sys] n_layers=32 | L0:K(1, 8, 40, 128) | L1:K(1, 8, 40, 128) | L31:K(1, 8, 40, 128)\n",
      "[OK]   system_prefill: all present layers seq_len == 40\n",
      "[low] h_low=(1, 35, 4096)\n",
      "[low] n_layers=4 | L0:K(1, 8, 75, 128) | L1:K(1, 8, 75, 128) | L3:K(1, 8, 75, 128)\n",
      "[up] n_layers=32 | L4:K(1, 8, 75, 128) | L5:K(1, 8, 75, 128) | L31:K(1, 8, 75, 128)\n",
      "[combined(before_hdr)] n_layers=32 | L0:K(1, 8, 75, 128) | L1:K(1, 8, 75, 128) | L31:K(1, 8, 75, 128)\n",
      "[OK]   combined_prefill: all present layers seq_len == 75\n",
      "[hdr] past_len before=75, after=79, delta=4\n",
      "[combined(after_hdr)] n_layers=32 | L0:K(1, 8, 79, 128) | L1:K(1, 8, 79, 128) | L31:K(1, 8, 79, 128)\n",
      "[decode step 0] past_len 79 -> 80 | next_id=40\n",
      "[decode step 1] past_len 80 -> 81 | next_id=2846\n",
      "[decode step 2] past_len 81 -> 82 | next_id=539\n",
      "[debug answer] I'm not able to answer your question as I don't have any information about a document. Can you please provide the document or more context about the question you are trying to ask?\n",
      "I'm not able to answer your question since you didn't provide a document. Please provide the document, and I'll be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "# LoPA (low-only prefill) scratch pipeline without trust_remote_code\n",
    "# Torch 2.5.1, Transformers 4.56.1, GPU+SDPA\n",
    "# - Phase1: lower-K only prefill (no assistant header)\n",
    "# - Phase2: generation (first feed assistant header tokens step-by-step)\n",
    "# - Positions unchanged; no remapping\n",
    "# - LoRA: best/lora attach → merge\n",
    "# - MinLength 수동 적용(4.56.1 디바이스 불일치 회피)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "K = 4  # prefill layers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (device == \"cuda\" and torch.cuda.is_bf16_supported())\n",
    "    else (torch.float16 if device == \"cuda\" else torch.float32)\n",
    ")\n",
    "\n",
    "cache_dir_model = \"/data2/jeongseokoh/hub/model\"\n",
    "cache_dir_tok   = \"/data2/jeongseokoh/hub/tokenizer\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer (best_dir 우선)\n",
    "# -----------------------------\n",
    "tok_src = str(best_dir) if (best_dir / \"tokenizer.json\").is_file() else backbone_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_src, cache_dir=cache_dir_tok, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Backbone + LoRA (merge)\n",
    "# -----------------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=dtype,\n",
    "    cache_dir=cache_dir_model,\n",
    ")\n",
    "base_model.to(device).eval()\n",
    "\n",
    "from peft import PeftModel\n",
    "assert lora_dir.is_dir(), f\"LoRA folder not found: {lora_dir}\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "model = peft_model.merge_and_unload().to(device).eval()\n",
    "del peft_model\n",
    "\n",
    "# SDPA 고정 (필요시 eager로 비교)\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    try: setattr(model.config, k, \"sdpa\")\n",
    "    except: pass\n",
    "try:\n",
    "    for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "        setattr(model.model.config, k, \"sdpa\")\n",
    "except: pass\n",
    "\n",
    "# -----------------------------\n",
    "# Cache helpers\n",
    "# -----------------------------\n",
    "def pkv_len(pkv) -> int:\n",
    "    if hasattr(pkv, \"key_cache\"): return len(pkv.key_cache)\n",
    "    if hasattr(pkv, \"layers\"):    return len(pkv.layers)\n",
    "    try: return len(pkv)\n",
    "    except: return 0\n",
    "\n",
    "def pkv_get(pkv, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if hasattr(pkv, \"key_cache\") and hasattr(pkv, \"value_cache\"):\n",
    "        return pkv.key_cache[idx], pkv.value_cache[idx]\n",
    "    if hasattr(pkv, \"layers\"):\n",
    "        layer = pkv.layers[idx]\n",
    "        return layer.keys, layer.values\n",
    "    return pkv[idx]\n",
    "\n",
    "def dc_from_subset(pkv_src, layer_indices: List[int]) -> DynamicCache:\n",
    "    dc = DynamicCache()\n",
    "    for li in layer_indices:\n",
    "        k, v = pkv_get(pkv_src, li)\n",
    "        dc.update(k, v, li)  # layer index 유지\n",
    "    return dc\n",
    "\n",
    "def cache_slice_doc_only(k: torch.Tensor, v: torch.Tensor, doc_len: int):\n",
    "    return k[:, :, -doc_len:, :], v[:, :, -doc_len:, :]\n",
    "\n",
    "def build_mask(length: int, batch: int = 1):\n",
    "    return torch.ones(batch, length, device=device, dtype=torch.long)\n",
    "\n",
    "def build_pos_ids(start: int, length: int, batch: int = 1):\n",
    "    return torch.arange(start, start + length, device=device, dtype=torch.long).unsqueeze(0).expand(batch, -1)\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt builders (Gen3)\n",
    "# -----------------------------\n",
    "def build_messages(system: str, document: str, question: str, include_query: bool = True):\n",
    "    user = f\"Document:\\n{document}\\n\\nQuestion: {question}\" if include_query else f\"Document:\\n{document}\\n\\n\"\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt: bool):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        if add_generation_prompt:\n",
    "            s += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        return s\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    s = apply_chat_template(messages, add_generation_prompt)\n",
    "    return tokenizer(s, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# LoPA generate (low-only prefill; no upper continue)\n",
    "# -----------------------------\n",
    "@torch.inference_mode()\n",
    "def lopa_generate(\n",
    "    system: str,\n",
    "    document: str,\n",
    "    question: str,\n",
    "    *,\n",
    "    K: int = 4,\n",
    "    max_new_tokens: int = 256,\n",
    "    min_length: int = 16,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: Optional[int] = None,\n",
    "    do_sample: bool = True,\n",
    "    repetition_penalty: Optional[float] = None,\n",
    "    start_with_assistant_header: bool = True, # Phase2 첫 토큰: header를 단계적으로 투입\n",
    "    force_attn_impl: Optional[str] = None,   # \"sdpa\" | \"eager\"\n",
    "):\n",
    "    if force_attn_impl:\n",
    "        for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "            try:\n",
    "                setattr(model.config, k, force_attn_impl)\n",
    "                setattr(model.model.config, k, force_attn_impl)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Phase-1 ids\n",
    "    msgs = build_messages(system, document, question, include_query=True)\n",
    "    ids_phase1 = tokens_from_messages(msgs, add_generation_prompt=False)  # [1, L_sys+doc]\n",
    "    ids_hdr    = tokens_from_messages(msgs, add_generation_prompt=True)   # [1, L_sys+doc + header]\n",
    "    sys_only   = tokens_from_messages([{\"role\":\"system\",\"content\":system}], add_generation_prompt=False)\n",
    "\n",
    "    L_sys = sys_only.size(1)\n",
    "    L_all = ids_phase1.size(1)\n",
    "    L_doc = L_all - L_sys\n",
    "    assert L_doc > 0, \"Phase-1 must include document tokens.\"\n",
    "\n",
    "    # 1) System-only prefill (base model)\n",
    "    out_sys = model.model(\n",
    "        input_ids=sys_only,\n",
    "        attention_mask=build_mask(L_sys),\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_sys = out_sys.past_key_values\n",
    "    n_layers = pkv_len(pkv_sys)\n",
    "\n",
    "    # 2) Lower-K doc pass (base model) — no upper continue\n",
    "    K_eff = max(0, min(K, n_layers))\n",
    "    full_layers: nn.ModuleList = model.model.layers\n",
    "    lower_layers = nn.ModuleList([full_layers[i] for i in range(0, K_eff)])\n",
    "\n",
    "    model.model.layers = lower_layers\n",
    "    dc_low_in = dc_from_subset(pkv_sys, list(range(K_eff))) if K_eff > 0 else DynamicCache()\n",
    "    attn_doc_full = torch.cat([build_mask(L_sys), build_mask(L_doc)], dim=1)\n",
    "\n",
    "    out_low = model.model(\n",
    "        input_ids=ids_phase1[:, L_sys:],  # doc only\n",
    "        past_key_values=dc_low_in,\n",
    "        attention_mask=attn_doc_full,     # sys+doc 길이\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    pkv_low = out_low.past_key_values\n",
    "\n",
    "    # 복원\n",
    "    model.model.layers = full_layers\n",
    "\n",
    "    # 3) Combine caches\n",
    "    # lower(0..K-1): sys + doc\n",
    "    # upper(K..):    sys only  (doc 미포함; generation 시작 시점부터 쌓임)\n",
    "    combined = DynamicCache()\n",
    "    # 헤드/차원 shape 확보\n",
    "    k0_sys, v0_sys = pkv_get(pkv_sys, 0)\n",
    "    for li in range(n_layers):\n",
    "        k_sys, v_sys = pkv_get(pkv_sys, li)\n",
    "        k_sys = k_sys[:, :, :L_sys, :]\n",
    "        v_sys = v_sys[:, :, :L_sys, :]\n",
    "        if li < K_eff:\n",
    "            k_low, v_low = pkv_get(pkv_low, li)\n",
    "            k_doc, v_doc = cache_slice_doc_only(k_low, v_low, L_doc)\n",
    "            k_cat = torch.cat([k_sys, k_doc], dim=2).contiguous()\n",
    "            v_cat = torch.cat([v_sys, v_doc], dim=2).contiguous()\n",
    "            combined.update(k_cat, v_cat, li)\n",
    "        else:\n",
    "            # upper: sys only\n",
    "            combined.update(k_sys.contiguous(), v_sys.contiguous(), li)\n",
    "\n",
    "    # 4) Phase-2: Generation\n",
    "    # seed: assistant header를 한 토큰씩 밀어 넣어 upper past가 L_sys → L_sys+H로 자라도록 함\n",
    "    if start_with_assistant_header:\n",
    "        hdr_tail = ids_hdr[:, L_all:]  # header-only tokens (len H)\n",
    "        H = int(hdr_tail.size(1))\n",
    "        for j in range(H):\n",
    "            past_len = pkv_get(combined, 0)[0].shape[2]  # lower 기준: L_sys+L_doc+grown\n",
    "            attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "            step_tok = hdr_tail[:, j:j+1]\n",
    "            out_seed = model(\n",
    "                input_ids=step_tok,\n",
    "                past_key_values=combined,\n",
    "                attention_mask=attn_mask,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            combined = out_seed.past_key_values\n",
    "\n",
    "    # 5) Decoding (CausalLM) with safe processors\n",
    "    from transformers.generation import LogitsProcessorList\n",
    "    from transformers.generation.logits_process import (\n",
    "        TemperatureLogitsWarper, TopPLogitsWarper, TopKLogitsWarper,\n",
    "        RepetitionPenaltyLogitsProcessor,\n",
    "    )\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    generated_ids = torch.empty((1, 0), dtype=torch.long, device=device)\n",
    "    # 첫 생성 스텝의 입력은 \"직전 토큰\" (header 마지막 또는 마지막 doc)\n",
    "    last = (ids_hdr[:, -1:] if start_with_assistant_header else ids_phase1[:, -1:])\n",
    "\n",
    "    processors = LogitsProcessorList()\n",
    "    if repetition_penalty and repetition_penalty != 1.0:\n",
    "        processors.append(RepetitionPenaltyLogitsProcessor(penalty=float(repetition_penalty)))\n",
    "    if temperature and temperature != 1.0:\n",
    "        processors.append(TemperatureLogitsWarper(temperature=float(temperature)))\n",
    "    if top_k is not None and top_k > 0:\n",
    "        processors.append(TopKLogitsWarper(top_k=int(top_k), filter_value=-float(\"inf\")))\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        processors.append(TopPLogitsWarper(top_p=float(top_p), min_tokens_to_keep=1))\n",
    "\n",
    "    cur = 0\n",
    "    while cur < max_new_tokens:\n",
    "        # lower past_len은 L_sys+L_doc + grown; upper past_len은 L_sys + grown\n",
    "        past_len = pkv_get(combined, 0)[0].shape[2]\n",
    "        attn_mask = torch.cat([build_mask(past_len), build_mask(1)], dim=1)\n",
    "\n",
    "        out = model(\n",
    "            input_ids=last,\n",
    "            past_key_values=combined,\n",
    "            attention_mask=attn_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        combined = out.past_key_values\n",
    "        logits = out.logits[:, -1, :].to(torch.float32)\n",
    "\n",
    "        # 즉시 EOS 방지 (min_length 수동 적용)\n",
    "        if eos_id is not None and cur < min_length:\n",
    "            logits[:, eos_id] = -float(\"inf\")\n",
    "\n",
    "        if not torch.isfinite(logits).all():\n",
    "            logits = torch.nan_to_num(logits, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "\n",
    "        inp = generated_ids if generated_ids.numel() > 0 else last.new_zeros((1, 0), dtype=torch.long, device=device)\n",
    "        inp = inp.to(logits.device)\n",
    "        logits = processors(inp, logits)\n",
    "\n",
    "        # Fallback-safe sampling\n",
    "        if not torch.isfinite(logits).any():\n",
    "            next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            if do_sample:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                if (not torch.isfinite(probs).all()) or (probs.sum(dim=-1) <= 0).any():\n",
    "                    next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_tok = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if eos_id is not None and int(next_tok.item()) == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_tok], dim=1)\n",
    "        last = next_tok\n",
    "        cur += 1\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "answer = lopa_generate(\n",
    "    system, document, question,\n",
    "    K=K,\n",
    "    max_new_tokens=256,\n",
    "    min_length=16,\n",
    "    temperature=0.7, top_p=0.9,\n",
    "    start_with_assistant_header=True,  # Phase2 처음에 header를 단계적으로 투입\n",
    "    # force_attn_impl=\"eager\",         # 필요시 비교\n",
    ")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7acf95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1346d627656d4138b818cc49145abfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a helpful assistant that answers questions based on the given document.user\\n\\nDocument:\\nThe Nile is the longest river in Africa...\\n\\nQuestion: Which continent is the Nile the longest river in?assistant\\n\\nThe Nile is the longest river in Africa.']\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_dir = \"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\"\n",
    "prefill_layers = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sys.path.insert(0, model_dir)  # best/ 를 import 경로에 추가\n",
    "from modeling_partial_layer import LlamaForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir, device_map=\"cuda:0\", dtype=torch.bfloat16, trust_remote_code=True).to(device).eval()\n",
    "for k in (\"attn_implementation\", \"_attn_implementation\"):\n",
    "    setattr(model.config, k, \"sdpa\")\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "out = model.generate(\n",
    "    system=system, document=document, query=question,\n",
    "    compress=False, tokenizer=tok, prefill_layers=prefill_layers,\n",
    "    max_new_tokens=256, do_sample=False, temperature=0.7, top_p=0.9\n",
    ")\n",
    "print(tok.batch_decode(out, skip_special_tokens=True))  # 문자열 또는 문자열 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786d4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_available=True | attached=True | merged=False | is_sharded=True | source=/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best/lora | error=None | last_used_lora=True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LATENTRAG_DEBUG\"]=\"1\"\n",
    "print(model.lora_debug_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89bed7b41e444fab4a660167f1660c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da2aecd742744a68944483b961a633b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant<|end_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Africa.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 경로 설정 (혼동 방지를 위해 절대경로 권장)\n",
    "best_dir = Path(\"/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial4-0specials/best\")\n",
    "lora_dir = best_dir / \"lora\"\n",
    "prefill_layers = 4  # 학습 시 사용한 값과 동일하게\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# best/의 remote-code(LOPA 구현)를 import할 수 있게 경로 추가\n",
    "sys.path.insert(0, str(best_dir))\n",
    "from modeling_partial_layer import LlamaForCausalLM  # Llama 3.1 계열용\n",
    "\n",
    "# 토크나이저: chat template 포함\n",
    "tok = AutoTokenizer.from_pretrained(str(best_dir))\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# 1) 백본을 허깅페이스에서 ‘깨끗하게’ 로드 (백본=llama3.1 8B instruct)\n",
    "#    주의: trust_remote_code=False → 표준 HF 클래스 경로를 통해 가중치만 로드\n",
    "backbone_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    backbone_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=False,  # 가중치만 가져오고 동작은 our remote-code 클래스가 담당\n",
    "    cache_dir=\"/data2/jeongseokoh/hub/model\",\n",
    ").to(device).eval()\n",
    "\n",
    "# 2) LoRA 어댑터 attach 후 merge (단일 모델로)\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, str(lora_dir))\n",
    "merged = peft_model.merge_and_unload().to(device).eval()\n",
    "\n",
    "# 3) LOPA 추론 (compress=True → partial prefill)\n",
    "#    이미 merge된 모델이므로 추가 attach를 막기 위해 use_lora=False로 고정\n",
    "system = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "document = \"The Nile is the longest river in Africa and flows northward through several countries...\"\n",
    "question = \"Which continent is the Nile the longest river in?\"\n",
    "\n",
    "out = merged.generate(\n",
    "    system=system,\n",
    "    document=document,\n",
    "    query=question,\n",
    "    compress=True,                 # LOPA 경로\n",
    "    tokenizer=tok,                 # 필수\n",
    "    prefill_layers=prefill_layers, # 학습과 동일\n",
    "    use_lora=False,                # 이미 merge됐으므로 재-부착 방지\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(out)  # 문자열 또는 문자열 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913061c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
