{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb85d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda | dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# ==== 설정 (환경에 맞게 수정) ====\n",
    "VANILLA_REPO_ID     = \"meta-llama/Llama-3.1-8B-Instruct\"       # 바닐라 모델\n",
    "TRI_REPO_ID         = \"jeongseokoh/LoPA_Llama3.1_8B_8_Lowers_assistant_10Specials\"       # 너의 LoPA TRI 레포\n",
    "BASE_SUBFOLDER      = \"base\"                                        # TRI 레포의 base 가중치 폴더\n",
    "LORA_SUBFOLDER      = \"lora\"                                        # LoRA 폴더(없으면 자동 건너뜀)\n",
    "LOPA_MODELING_PATH  = \"./lopa_llama_modeling.py\"                    # (완성본) TRI 모델링 파일 경로\n",
    "TOKENIZER_PATH      = TRI_REPO_ID                                   # 동일 토크나이저 권장\n",
    "ATTN_IMPL           = \"flash_attention_2\"                           # \"flash_attention_2\" | \"eager\" | \"sdpa\"\n",
    "HF_TOKEN            = None  # private면 토큰 넣어줘\n",
    "LOWER_K = 8\n",
    "# 길이 (요청 조건)\n",
    "LEN_S   = 256   # system\n",
    "LEN_U   = 256   # user(질의)\n",
    "LEN_D   = 10240  # document\n",
    "LEN_H   = 4     # assistant header 길이(고정 더미)\n",
    "LEN_GEN = 512   # 생성 토큰 수\n",
    "NUM_SPECIALS = 10   # Latent special tokens count for profiling\n",
    "SPECIAL_ADD_TO = \"assistant\"  # 'none' | 'user' | 'assistant'\n",
    "SPECIAL_ADD_TO = (SPECIAL_ADD_TO or \"none\").lower()\n",
    "if SPECIAL_ADD_TO not in {\"user\", \"assistant\"}:\n",
    "    SPECIAL_ADD_TO = \"none\"\n",
    "\n",
    "\n",
    "# 실행 환경 권장 변수\n",
    "import os, torch, sys\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "os.environ.setdefault(\"CUDA_DEVICE_MAX_CONNECTIONS\", \"1\")\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128,expandable_segments:True\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype  = (torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
    "          else (torch.float16 if torch.cuda.is_available() else torch.float32))\n",
    "print(\"device:\", device, \"| dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340e30e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca74c9974f614f52874ffa8f5b6686b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TRI patch loaded: ./lopa_llama_modeling.py\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79d45f3428e4704b539e5762a00c744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/868 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae183340cd1c4291802fc446828e5922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2290a552b34f46991ef7bfb5b295a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639d63da25d84931bf0f0b448f194401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "base/model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04301d5f658845dc94510516b30c06ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "base/model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f24edcbd8624bdf9effebd12ca14339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "base/model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769f2c84361641b3b40b6d4fe9ac9ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "base/model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c524f4170545c9b7ce15a235c911ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd1bdeebcaf4ead8cc2380f744474e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c2706c7d9c44878f7974a42a543877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecca6614e1fc43118102309abba7ac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lora/adapter_model.safetensors:   0%|          | 0.00/2.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] LoRA adapters loaded: jeongseokoh/LoPA_Llama3.1_8B_8_Lowers_assistant_10Specials/lora\n",
      "[OK] models ready\n"
     ]
    }
   ],
   "source": [
    "# ==== 토크나이저 ====\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True, token=HF_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# ==== 바닐라 모델 ====\n",
    "from transformers import AutoModelForCausalLM\n",
    "van_model = AutoModelForCausalLM.from_pretrained(\n",
    "    VANILLA_REPO_ID, torch_dtype=dtype, token=HF_TOKEN, device_map=\"auto\", cache_dir=\"/data2/jeongseokoh/hub\"\n",
    ")\n",
    "van_model.eval()\n",
    "\n",
    "# ==== TRI 모델링을 transformers 내부에 주입 ====\n",
    "import importlib.util, transformers, transformers.models.llama as llama_pkg\n",
    "target_name = \"transformers.models.llama.modeling_llama\"\n",
    "spec = importlib.util.spec_from_file_location(target_name, LOPA_MODELING_PATH)\n",
    "mod = importlib.util.module_from_spec(spec)\n",
    "sys.modules.pop(target_name, None)\n",
    "sys.modules[target_name] = mod\n",
    "spec.loader.exec_module(mod)\n",
    "setattr(llama_pkg, \"modeling_llama\", mod)\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM  # (패치된) 클래스\n",
    "print(\"[DEBUG] TRI patch loaded:\", LOPA_MODELING_PATH)\n",
    "\n",
    "# ==== TRI 모델 ====\n",
    "tri_model = LlamaForCausalLM.from_pretrained(\n",
    "    TRI_REPO_ID, subfolder=BASE_SUBFOLDER, torch_dtype=dtype, token=HF_TOKEN, device_map=\"auto\", cache_dir=\"/data2/jeongseokoh/hub\"\n",
    ")\n",
    "\n",
    "# LoRA 어댑터(있으면 자동 로드)\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    tri_model = PeftModel.from_pretrained(tri_model, TRI_REPO_ID, subfolder=LORA_SUBFOLDER, token=HF_TOKEN)\n",
    "    tri_model = tri_model\n",
    "    print(f\"[info] LoRA adapters loaded: {TRI_REPO_ID}/{LORA_SUBFOLDER}\")\n",
    "except Exception as e:\n",
    "    print(\"[info] LoRA not found or skipped:\", str(e).split(\"\\n\")[0])\n",
    "\n",
    "# 어텐션 백엔드 지정(둘 다)\n",
    "\n",
    "# Special tokens configuration shared with profiling pipeline\n",
    "USE_SPECIALS = int(NUM_SPECIALS) > 0 and SPECIAL_ADD_TO in {\"user\", \"assistant\"}\n",
    "SPECIAL_TOKENS = [f\"<|Latent{i}|>\" for i in range(1, int(NUM_SPECIALS) + 1)] if USE_SPECIALS else []\n",
    "\n",
    "for m in (van_model, tri_model):\n",
    "    try:\n",
    "        m.config._attn_implementation = ATTN_IMPL\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 추론모드 & TF32\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TRI API 존재 확인\n",
    "for need in (\"tri_build_caches\", \"tri_forward_assistant\", \"tri_step_logits\"):\n",
    "    assert hasattr(tri_model, need), f\"Missing TRI API: {need}\"\n",
    "\n",
    "van_model.eval(); tri_model.eval()\n",
    "print(\"[OK] models ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229c8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lens: 256 10496 14\n",
      "[info] profiling specials inserted at assistant: 10 tokens\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "SPECIAL_IDS_TENSOR = None\n",
    "if 'USE_SPECIALS' in globals() and USE_SPECIALS:\n",
    "    _special_ids = []\n",
    "    for _token_str in SPECIAL_TOKENS:\n",
    "        _tid = tok.convert_tokens_to_ids(_token_str)\n",
    "        if _tid is None or (tok.unk_token_id is not None and _tid == tok.unk_token_id):\n",
    "            _enc = tok.encode(_token_str, add_special_tokens=False)\n",
    "            if len(_enc) != 1:\n",
    "                raise ValueError(f\"Tokenizer cannot represent special token {_token_str} as single id\")\n",
    "            _tid = _enc[0]\n",
    "        _special_ids.append(int(_tid))\n",
    "    if _special_ids:\n",
    "        SPECIAL_IDS_TENSOR = torch.tensor(_special_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "# \"한 글자 → 1토큰\" 되는 후보를 찾아서 filler 토큰으로 사용\n",
    "def pick_single_token_id(candidates=(\"A\", \"B\", \"C\", \"x\", \"y\", \"z\")):\n",
    "    for s in candidates:\n",
    "        ids = tok.encode(s, add_special_tokens=False)\n",
    "        if len(ids) == 1 and ids[0] != tok.eos_token_id:\n",
    "            return ids[0]\n",
    "    # fallback: vocab 중앙값\n",
    "    return min(len(tok), 32_000) - 10\n",
    "\n",
    "FILL_ID = pick_single_token_id()\n",
    "HEAD_ID = pick_single_token_id((\"?\", \":\", \".\", \"!\"))\n",
    "\n",
    "def make_ids(length: int, fill_id: int = FILL_ID):\n",
    "    return torch.full((1, length), fill_id, dtype=torch.long, device=device)\n",
    "\n",
    "def build_segments_lengths():\n",
    "    S_ids  = make_ids(LEN_S,  FILL_ID)\n",
    "    U_ids  = make_ids(LEN_U,  FILL_ID)\n",
    "    D_ids  = make_ids(LEN_D,  FILL_ID)\n",
    "    H_ids  = make_ids(LEN_H,  HEAD_ID)\n",
    "    # LoPA user 프리필 = Document + User\n",
    "    U_total = torch.cat([D_ids, U_ids], dim=1)  # Document + user query\n",
    "    if SPECIAL_IDS_TENSOR is not None:\n",
    "        if SPECIAL_ADD_TO == \"user\":\n",
    "            U_total = torch.cat([U_total, SPECIAL_IDS_TENSOR], dim=1)\n",
    "        elif SPECIAL_ADD_TO == \"assistant\":\n",
    "            H_ids = torch.cat([SPECIAL_IDS_TENSOR, H_ids], dim=1)\n",
    "    return S_ids, U_total, H_ids\n",
    "\n",
    "S_ids, Utotal_ids, H_ids = build_segments_lengths()\n",
    "print(\"lens:\", S_ids.size(1), Utotal_ids.size(1), H_ids.size(1))\n",
    "if SPECIAL_IDS_TENSOR is not None:\n",
    "    print(f\"[info] profiling specials inserted at {SPECIAL_ADD_TO}: {SPECIAL_IDS_TENSOR.size(1)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FLOPs 계산 (Attention-only) ====\n",
    "# 가정:\n",
    "#  - 어텐션 FLOPs ≈ 4 * (#heads) * head_dim * (Tq * Tk)     (QKᵀ + AV 두 matmul)\n",
    "#  - Prefill에서 한 번에 블록 길이 N을 넣으면 per-layer FLOPs ≈ 4 * H * Dh * N^2\n",
    "#  - Generation(단일 토큰 루프) per-layer FLOPs ≈ 4 * H * Dh * sum_i Tk_i\n",
    "#  - TRI: Prefill은 Upper: (S+H)^2, Lower: (S+U+H)^2 (Header 포함)\n",
    "#         Generation에서 Upper: Tk_i = S+H+(i-1), Lower: Tk_i = S+U+H+(i-1)\n",
    "\n",
    "def _attn_cfg(model):\n",
    "    cfg = model.config\n",
    "    H = int(cfg.num_attention_heads)\n",
    "    Dh = getattr(cfg, \"head_dim\", cfg.hidden_size // H)\n",
    "    L = int(cfg.num_hidden_layers)\n",
    "    return L, H, Dh\n",
    "\n",
    "def flops_vanilla_attention(model, S:int, U:int, Htok:int, A:int):\n",
    "    L, Hh, Dh = _attn_cfg(model)\n",
    "    N = S + U + Htok\n",
    "    # Prefill: per-layer 4*H*Dh*N^2\n",
    "    pre_per_layer = 4.0 * Hh * Dh * (N**2)\n",
    "    # Gen: sum Tk = A*N + A*(A-1)/2\n",
    "    sum_Tk = A * N + (A * (A - 1)) / 2.0\n",
    "    gen_per_layer = 4.0 * Hh * Dh * sum_Tk\n",
    "    pre = pre_per_layer * L\n",
    "    gen = gen_per_layer * L\n",
    "    tot = pre + gen\n",
    "    return pre, gen, tot\n",
    "\n",
    "def flops_tri_attention(model, S:int, U:int, Htok:int, A:int, K:int):\n",
    "    L, Hh, Dh = _attn_cfg(model)\n",
    "    K = int(K)\n",
    "    # Prefill:\n",
    "    #  Lower K layers: (S+U+H)^2, Upper L-K layers: (S+H)^2\n",
    "    N_lower = S + U + Htok\n",
    "    N_upper = S + Htok\n",
    "    pre_lower = 4.0 * Hh * Dh * (N_lower**2) * K\n",
    "    pre_upper = 4.0 * Hh * Dh * (N_upper**2) * (L - K)\n",
    "    pre = pre_lower + pre_upper\n",
    "    # Generation:\n",
    "    #  sum Tk lower = A*(S+U+H) + A(A-1)/2\n",
    "    #  sum Tk upper = A*(S+H)   + A(A-1)/2\n",
    "    sumTk_lower = A * (S + U + Htok) + (A * (A - 1)) / 2.0\n",
    "    sumTk_upper = A * (S + Htok)     + (A * (A - 1)) / 2.0\n",
    "    gen_lower = 4.0 * Hh * Dh * sumTk_lower * K\n",
    "    gen_upper = 4.0 * Hh * Dh * sumTk_upper * (L - K)\n",
    "    gen = gen_lower + gen_upper\n",
    "    tot = pre + gen\n",
    "    return pre, gen, tot\n",
    "\n",
    "def to_gflops(x):   # FLOPs → GFLOPs\n",
    "    return x / 1e9\n",
    "\n",
    "def throughput_gflops_per_s(gflops, seconds):\n",
    "    return (gflops / max(1e-9, seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def sync():\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def reset_peak():\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def get_mem_mib():\n",
    "    if device.type != \"cuda\": \n",
    "        return {\"alloc_MiB\": 0.0, \"reserved_MiB\": 0.0}\n",
    "    alloc = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    reserv= torch.cuda.max_memory_reserved() / (1024**2)\n",
    "    return {\"alloc_MiB\": round(alloc,2), \"reserved_MiB\": round(reserv,2)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "@torch.inference_mode()\n",
    "def profile_vanilla(model, S_ids, Utotal_ids, H_ids, gen_len, greedily=True):\n",
    "    S = int(S_ids.size(1)); U = int(Utotal_ids.size(1)); Htok = int(H_ids.size(1)); A = int(gen_len)\n",
    "    prompt = torch.cat([S_ids, Utotal_ids, H_ids], dim=1)\n",
    "    L_prompt = prompt.size(1)\n",
    "\n",
    "    # FLOPs 이론값(어텐션 기준)\n",
    "    FLOP_pre, FLOP_gen, FLOP_tot = flops_vanilla_attention(model, S, U, Htok, A)\n",
    "\n",
    "    reset_peak(); sync(); t0 = time.perf_counter()\n",
    "    out = model(input_ids=prompt, use_cache=True)\n",
    "    sync(); t_prefill = time.perf_counter() - t0\n",
    "\n",
    "    logits = out.logits[:, -1, :]\n",
    "    if greedily:\n",
    "        next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    else:\n",
    "        probs = F.softmax(logits, dim=-1); next_id = torch.multinomial(probs, num_samples=1)\n",
    "    pkv = out.past_key_values\n",
    "\n",
    "    sync(); t1 = time.perf_counter()\n",
    "    out = model(input_ids=next_id, past_key_values=pkv, use_cache=True)\n",
    "    sync(); t_first = time.perf_counter() - t1\n",
    "\n",
    "    cur = next_id; pkv = out.past_key_values\n",
    "    sync(); tg0 = time.perf_counter()\n",
    "    for _ in range(gen_len - 1):\n",
    "        out = model(input_ids=cur, past_key_values=pkv, use_cache=True)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        if greedily:\n",
    "            cur = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1); cur = torch.multinomial(probs, num_samples=1)\n",
    "        pkv = out.past_key_values\n",
    "    sync(); t_gen = time.perf_counter() - tg0\n",
    "\n",
    "    mem = get_mem_mib()\n",
    "    ttft = t_prefill + t_first\n",
    "    ttot = ttft + t_gen\n",
    "\n",
    "    # Throughput (GFLOPs/s)\n",
    "    pre_gflops   = to_gflops(FLOP_pre)\n",
    "    gen_gflops   = to_gflops(FLOP_gen)\n",
    "    tot_gflops   = to_gflops(FLOP_tot)\n",
    "    pre_gflops_s = throughput_gflops_per_s(pre_gflops, t_prefill)\n",
    "    gen_gflops_s = throughput_gflops_per_s(gen_gflops, t_gen)\n",
    "    tot_gflops_s = throughput_gflops_per_s(tot_gflops, ttot)\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": \"vanilla\",\n",
    "        \"lens\": {\"S\": S, \"U_total\": U, \"H\": Htok, \"gen\": A, \"prompt\": L_prompt},\n",
    "        \"prefill_ms\": round(t_prefill*1000, 3),\n",
    "        \"first_token_ms\": round(t_first*1000, 3),\n",
    "        \"ttft_ms\": round(ttft*1000, 3),\n",
    "        \"gen_ms\": round(t_gen*1000, 3),\n",
    "        \"prefill_ms_per_tok\": round(1000 * t_prefill / L_prompt, 5),\n",
    "        \"gen_ms_per_tok\": round(1000 * t_gen / A, 5),\n",
    "        \"total_ms_per_tok\": round(1000 * ttot / (L_prompt + A), 5),\n",
    "        \"total_ms\": round(ttot*1000, 3),\n",
    "        \"peak_mem_alloc_MiB\": mem[\"alloc_MiB\"],\n",
    "        \"peak_mem_reserved_MiB\": mem[\"reserved_MiB\"],\n",
    "        # FLOPs/GFLOPs\n",
    "        \"FLOPs_prefill\": int(FLOP_pre), \"FLOPs_gen\": int(FLOP_gen), \"FLOPs_total\": int(FLOP_tot),\n",
    "        \"GFLOPs_prefill\": round(pre_gflops, 3), \"GFLOPs_gen\": round(gen_gflops, 3), \"GFLOPs_total\": round(tot_gflops, 3),\n",
    "        \"GFLOPs/s_prefill\": round(pre_gflops_s, 2), \"GFLOPs/s_gen\": round(gen_gflops_s, 2), \"GFLOPs/s_total\": round(tot_gflops_s, 2),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "@torch.inference_mode()\n",
    "def profile_tri(model, S_ids, Utotal_ids, H_ids, lower_k, gen_len, greedily=True):\n",
    "    S = int(S_ids.size(1)); U = int(Utotal_ids.size(1)); Htok = int(H_ids.size(1)); A = int(gen_len)\n",
    "    L_prompt = S + U + Htok\n",
    "\n",
    "    # FLOPs 이론값(어텐션 기준)\n",
    "    FLOP_pre, FLOP_gen, FLOP_tot = flops_tri_attention(model, S, U, Htok, A, K=lower_k)\n",
    "\n",
    "    reset_peak(); sync(); t0 = time.perf_counter()\n",
    "    pkv, S_len, U_len = model.tri_build_caches(system_ids=S_ids, user_ids=Utotal_ids, lower_k=lower_k)\n",
    "    out = model.tri_step_logits(H_ids, lower_k, pkv, S_len, U_len, logits_to_keep=1, labels=None, write_cache=True)\n",
    "    sync(); t_prefill = time.perf_counter() - t0\n",
    "\n",
    "    logits = out.logits[:, -1, :]\n",
    "    if greedily:\n",
    "        cur = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    else:\n",
    "        probs = F.softmax(logits, dim=-1); cur = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    sync(); t1 = time.perf_counter()\n",
    "    out = model.tri_step_logits(cur, lower_k, pkv, S_len, U_len, logits_to_keep=1, labels=None, write_cache=True)\n",
    "    sync(); t_first = time.perf_counter() - t1\n",
    "\n",
    "    sync(); tg0 = time.perf_counter()\n",
    "    for _ in range(gen_len - 1):\n",
    "        out = model.tri_step_logits(cur, lower_k, pkv, S_len, U_len, logits_to_keep=1, labels=None, write_cache=True)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        if greedily:\n",
    "            cur = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1); cur = torch.multinomial(probs, num_samples=1)\n",
    "    sync(); t_gen = time.perf_counter() - tg0\n",
    "\n",
    "    mem = get_mem_mib()\n",
    "    ttft = t_prefill + t_first\n",
    "    ttot = ttft + t_gen\n",
    "\n",
    "    pre_gflops   = to_gflops(FLOP_pre)\n",
    "    gen_gflops   = to_gflops(FLOP_gen)\n",
    "    tot_gflops   = to_gflops(FLOP_tot)\n",
    "    pre_gflops_s = throughput_gflops_per_s(pre_gflops, t_prefill)\n",
    "    gen_gflops_s = throughput_gflops_per_s(gen_gflops, t_gen)\n",
    "    tot_gflops_s = throughput_gflops_per_s(tot_gflops, ttot)\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": f\"LoPA-TRI(K={lower_k})\",\n",
    "        \"lens\": {\"S\": S, \"U_total\": U, \"H\": Htok, \"gen\": A, \"prompt\": L_prompt},\n",
    "        \"prefill_ms\": round(t_prefill*1000, 3),\n",
    "        \"first_token_ms\": round(t_first*1000, 3),\n",
    "        \"ttft_ms\": round(ttft*1000, 3),\n",
    "        \"gen_ms\": round(t_gen*1000, 3),\n",
    "        \"prefill_ms_per_tok\": round(1000 * t_prefill / L_prompt, 5),\n",
    "        \"gen_ms_per_tok\": round(1000 * t_gen / A, 5),\n",
    "        \"total_ms_per_tok\": round(1000 * ttot / (L_prompt + A), 5),\n",
    "        \"total_ms\": round(ttot*1000, 3),\n",
    "        \"peak_mem_alloc_MiB\": mem[\"alloc_MiB\"],\n",
    "        \"peak_mem_reserved_MiB\": mem[\"reserved_MiB\"],\n",
    "        # FLOPs/GFLOPs\n",
    "        \"FLOPs_prefill\": int(FLOP_pre), \"FLOPs_gen\": int(FLOP_gen), \"FLOPs_total\": int(FLOP_tot),\n",
    "        \"GFLOPs_prefill\": round(pre_gflops, 3), \"GFLOPs_gen\": round(gen_gflops, 3), \"GFLOPs_total\": round(tot_gflops, 3),\n",
    "        \"GFLOPs/s_prefill\": round(pre_gflops_s, 2), \"GFLOPs/s_gen\": round(gen_gflops_s, 2), \"GFLOPs/s_total\": round(tot_gflops_s, 2),\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c068e31",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'FLOPs/s_prefill'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_row\u001b[39m(m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt(S/U/H)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlens\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlens\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU_total\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlens\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs Total (theory)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs_total\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs/s Total\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs/s_total\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     40\u001b[0m     }\n\u001b[0;32m---> 42\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\u001b[43m_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_van\u001b[49m\u001b[43m)\u001b[49m, _row(m_tri)])\n\u001b[1;32m     43\u001b[0m display(df)\n",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m, in \u001b[0;36m_row\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_row\u001b[39m(m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt(S/U/H)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlens\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlens\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU_total\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlens\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenToks\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlens\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrefill (ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefill_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirstTok (ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_token_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTFT (ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mttft_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGen (ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrefill (ms/tok)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefill_ms_per_tok\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGen (ms/tok)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_ms_per_tok\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal (ms/tok)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_ms_per_tok\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal (ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeak alloc (MiB)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeak_mem_alloc_MiB\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeak reserved (MiB)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeak_mem_reserved_MiB\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# FLOPs\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs Prefill (theory)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs_prefill\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs/s Prefill\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLOPs/s_prefill\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs Gen (theory)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs/s Gen\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs/s_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs Total (theory)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs_total\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs/s Total\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs/s_total\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs Prefill (theory)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs_prefill\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs/s Prefill\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs/s_prefill\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs Gen (theory)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs/s Gen\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs/s_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs Total (theory)\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs_total\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs/s Total\u001b[39m\u001b[38;5;124m\"\u001b[39m: m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGFLOPs/s_total\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     40\u001b[0m     }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'FLOPs/s_prefill'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from peft import PeftModel\n",
    "    if isinstance(tri_model, PeftModel):\n",
    "        tri_model = tri_model.merge_and_unload()\n",
    "        tri_model = tri_model.eval()\n",
    "        print(\"[info] merged LoRA into base (unloaded PEFT)\")\n",
    "except Exception as e:\n",
    "    print(\"[info] LoRA merge skipped:\", e)\n",
    "\n",
    "# 동일 백엔드 재지정\n",
    "try:\n",
    "    tri_model.config._attn_implementation = ATTN_IMPL\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 워밍업\n",
    "_ = profile_vanilla(van_model, S_ids[:, :64], Utotal_ids[:, :128], H_ids[:, :2], gen_len=32, greedily=True)\n",
    "_ = profile_tri(tri_model, S_ids[:, :64], Utotal_ids[:, :128], H_ids[:, :2], lower_k=LOWER_K, gen_len=32, greedily=True)\n",
    "\n",
    "# 본측정\n",
    "m_van = profile_vanilla(van_model, S_ids, Utotal_ids, H_ids, gen_len=LEN_GEN, greedily=True)\n",
    "m_tri = profile_tri(tri_model, S_ids, Utotal_ids, H_ids, lower_k=LOWER_K, gen_len=LEN_GEN, greedily=True)\n",
    "\n",
    "import pandas as pd\n",
    "def _row(m):\n",
    "    return {\n",
    "        \"Model\": m[\"model\"],\n",
    "        \"Prompt(S/U/H)\": f'{m[\"lens\"][\"S\"]}/{m[\"lens\"][\"U_total\"]}/{m[\"lens\"][\"H\"]}',\n",
    "        \"GenToks\": m[\"lens\"][\"gen\"],\n",
    "        \"Prefill (ms)\": m[\"prefill_ms\"], \"FirstTok (ms)\": m[\"first_token_ms\"], \"TTFT (ms)\": m[\"ttft_ms\"], \"Gen (ms)\": m[\"gen_ms\"],\n",
    "        \"Prefill (ms/tok)\": m[\"prefill_ms_per_tok\"], \"Gen (ms/tok)\": m[\"gen_ms_per_tok\"], \"Total (ms/tok)\": m[\"total_ms_per_tok\"], \"Total (ms)\": m[\"total_ms\"],\n",
    "        \"Peak alloc (MiB)\": m[\"peak_mem_alloc_MiB\"], \"Peak reserved (MiB)\": m[\"peak_mem_reserved_MiB\"],\n",
    "        # FLOPs\n",
    "        \"FLOPs Prefill\": m[\"FLOPs_prefill\"], \"FLOPs_gen\": m[\"FLOPs_gen\"], \"FLOPs_total\": m[\"FLOPs_total\"],\n",
    "        \"GFLOPs Prefill (theory)\": m[\"GFLOPs_prefill\"], \"GFLOPs/s Prefill\": m[\"GFLOPs/s_prefill\"],\n",
    "        \"GFLOPs Gen (theory)\": m[\"GFLOPs_gen\"], \"GFLOPs/s Gen\": m[\"GFLOPs/s_gen\"],\n",
    "        \"GFLOPs Total (theory)\": m[\"GFLOPs_total\"], \"GFLOPs/s Total\": m[\"GFLOPs/s_total\"],\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame([_row(m_van), _row(m_tri)])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 간단한 TRI 생성 예제 (정답 확인) ====\\n\n",
    "example_document = \"Paris is the capital city of France.\"\n",
    "example_question = \"What is the capital of France?\"\n",
    "system_prompt_demo = \"You are a helpful assistant that answers questions based on the given document.\"\n",
    "\n",
    "def build_demo_messages(system_text: str, document_text: str, question_text: str):\n",
    "    user = f\"Document:\\\\n{document_text}\\\\n\\\\nQuestion: {question_text}\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_text},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "\n",
    "def apply_chat_template(messages, add_generation_prompt=False):\n",
    "    try:\n",
    "        rendered = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    except TypeError:\n",
    "        rendered = tok.apply_chat_template(messages, tokenize=False)\n",
    "        template = getattr(tok, \"chat_template\", \"\") or \"\"\n",
    "        if add_generation_prompt and \"<|start_header_id|>\" in template:\n",
    "            rendered += \"<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\"\n",
    "    return rendered\n",
    "\n",
    "def tokens_from_messages(messages, add_generation_prompt=False):\n",
    "    rendered = apply_chat_template(messages, add_generation_prompt=add_generation_prompt)\n",
    "    return tok(rendered, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "def lcp_len(a: torch.Tensor, b: torch.Tensor) -> int:\n",
    "    max_len = min(a.size(1), b.size(1))\n",
    "    for idx in range(max_len):\n",
    "        if int(a[0, idx]) != int(b[0, idx]):\n",
    "            return idx\n",
    "    return max_len\n",
    "\n",
    "demo_messages = build_demo_messages(system_prompt_demo, example_document, example_question)\n",
    "if SPECIAL_IDS_TENSOR is not None and SPECIAL_ADD_TO == \"user\":\n",
    "    demo_messages[-1][\"content\"] += f\"\\n\\n{' '.join(SPECIAL_TOKENS)}\"\n",
    "\n",
    "S_ids_demo = tokens_from_messages(demo_messages[:1], add_generation_prompt=False)\n",
    "SU_ids_demo = tokens_from_messages(demo_messages, add_generation_prompt=False)\n",
    "SU_gen_demo = tokens_from_messages(demo_messages, add_generation_prompt=True)\n",
    "l_su_demo = lcp_len(S_ids_demo, SU_ids_demo)\n",
    "user_delta_demo = SU_ids_demo[:, l_su_demo:SU_ids_demo.size(1)]\n",
    "header_delta_demo = SU_gen_demo[:, SU_ids_demo.size(1):]\n",
    "assistant_prefix_ids = []\n",
    "if SPECIAL_IDS_TENSOR is not None and SPECIAL_ADD_TO == \"assistant\":\n",
    "    header_delta_demo = torch.cat([header_delta_demo, SPECIAL_IDS_TENSOR], dim=1)\n",
    "    assistant_prefix_ids = SPECIAL_IDS_TENSOR[0].tolist()\n",
    "\n",
    "max_demo_tokens = 64\n",
    "generated_ids = []\n",
    "with torch.no_grad():\n",
    "    pkv_demo, S_len_demo, U_len_demo = tri_model.tri_build_caches(\n",
    "        system_ids=S_ids_demo, user_ids=user_delta_demo, lower_k=LOWER_K\n",
    "    )\n",
    "    step_out = tri_model.tri_step_logits(\n",
    "        assistant_ids=header_delta_demo, lower_k=LOWER_K, pkv=pkv_demo,\n",
    "        S=S_len_demo, U=U_len_demo, logits_to_keep=1, labels=None, write_cache=True\n",
    "    )\n",
    "    logits = step_out.logits[:, -1, :]\n",
    "    cur = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    for _ in range(max_demo_tokens):\n",
    "        token_id = int(cur.item())\n",
    "        generated_ids.append(token_id)\n",
    "        if tok.eos_token_id is not None and token_id == int(tok.eos_token_id):\n",
    "            break\n",
    "        step_out = tri_model.tri_step_logits(\n",
    "            assistant_ids=cur, lower_k=LOWER_K, pkv=pkv_demo, S=S_len_demo, U=U_len_demo,\n",
    "            logits_to_keep=1, labels=None, write_cache=True\n",
    "        )\n",
    "        logits = step_out.logits[:, -1, :]\n",
    "        cur = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "decoded_answer = tok.decode(assistant_prefix_ids + generated_ids, skip_special_tokens=True).strip()\n",
    "print(\"[Demo Question]\", example_question)\n",
    "print(\"[Demo Answer]\", decoded_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def profile_tri(model, S_ids, Utotal_ids, H_ids, lower_k=8, gen_len=LEN_GEN, greedily=True):\n",
    "    # Prefill: S/U_total\n",
    "    reset_peak(); sync(); t0 = time.perf_counter()\n",
    "    pkv, S_len, U_len = model.tri_build_caches(system_ids=S_ids, user_ids=Utotal_ids, lower_k=lower_k)\n",
    "    # Header 기록(+ start logits 1개만)\n",
    "    out = model.tri_step_logits(H_ids, lower_k, pkv, S_len, U_len, logits_to_keep=1, labels=None, write_cache=True)\n",
    "    sync(); t_prefill = time.perf_counter() - t0\n",
    "\n",
    "    # 첫 토큰\n",
    "    logits = out.logits[:, -1, :]\n",
    "    if greedily:\n",
    "        cur = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    else:\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        cur = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    sync(); t1 = time.perf_counter()\n",
    "    out = model.tri_step_logits(cur, lower_k, pkv, S_len, U_len, logits_to_keep=1, labels=None, write_cache=True)\n",
    "    sync(); t_first = time.perf_counter() - t1\n",
    "\n",
    "    # Generation loop\n",
    "    sync(); tg0 = time.perf_counter()\n",
    "    for _ in range(gen_len - 1):\n",
    "        out = model.tri_step_logits(cur, lower_k, pkv, S_len, U_len, logits_to_keep=1, labels=None, write_cache=True)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        if greedily:\n",
    "            cur = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            cur = torch.multinomial(probs, num_samples=1)\n",
    "    sync(); t_gen = time.perf_counter() - tg0\n",
    "\n",
    "    mem = get_mem_mib()\n",
    "    L_prompt = S_ids.size(1) + Utotal_ids.size(1) + H_ids.size(1)\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": f\"LoPA-TRI(K={lower_k})\",\n",
    "        \"lens\": {\"S\": S_ids.size(1), \"U_total\": Utotal_ids.size(1), \"H\": H_ids.size(1), \"gen\": gen_len, \"prompt\": L_prompt},\n",
    "        \"prefill_ms\": round(t_prefill*1000, 3),\n",
    "        \"first_token_ms\": round(t_first*1000, 3),\n",
    "        \"ttft_ms\": round((t_prefill + t_first)*1000, 3),\n",
    "        \"gen_ms\": round(t_gen*1000, 3),\n",
    "        \"prefill_ms_per_tok\": round(1000 * t_prefill / L_prompt, 5),\n",
    "        \"gen_ms_per_tok\": round(1000 * t_gen / gen_len, 5),\n",
    "        \"total_ms_per_tok\": round(1000 * (t_prefill + t_first + t_gen) / (L_prompt + gen_len), 5),\n",
    "        \"total_ms\": round((t_prefill + t_first + t_gen)*1000, 3),\n",
    "        \"peak_mem_alloc_MiB\": mem[\"alloc_MiB\"],\n",
    "        \"peak_mem_reserved_MiB\": mem[\"reserved_MiB\"],\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1139e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'vanilla', 'lens': {'S': 256, 'U_total': 10496, 'H': 4, 'gen': 512, 'prompt': 10756}, 'prefill_ms': 830.267, 'first_token_ms': 18.328, 'ttft_ms': 848.595, 'gen_ms': 9041.353, 'prefill_ms_per_tok': 0.07719, 'gen_ms_per_tok': 17.65889, 'total_ms_per_tok': 0.8777, 'total_ms': 9889.948, 'peak_mem_alloc_MiB': 34741.07, 'peak_mem_reserved_MiB': 96564.0}\n",
      "{'model': 'LoPA-TRI(K=8)', 'lens': {'S': 256, 'U_total': 10496, 'H': 4, 'gen': 512, 'prompt': 10756}, 'prefill_ms': 279.043, 'first_token_ms': 22.784, 'ttft_ms': 301.827, 'gen_ms': 11811.404, 'prefill_ms_per_tok': 0.02594, 'gen_ms_per_tok': 23.06915, 'total_ms_per_tok': 1.07501, 'total_ms': 12113.231, 'peak_mem_alloc_MiB': 33262.51, 'peak_mem_reserved_MiB': 96562.0}\n"
     ]
    }
   ],
   "source": [
    "# 워밍업(커널/캐시 안정화): 각 1회 짧게\n",
    "_ = profile_vanilla(van_model, S_ids[:, :64], Utotal_ids[:, :128], H_ids[:, :2], gen_len=32)\n",
    "_ = profile_tri(tri_model, S_ids[:, :64], Utotal_ids[:, :128], H_ids[:, :2], lower_k=LOWER_K, gen_len=32)\n",
    "\n",
    "# 본측정 (정확히 512토큰 생성)\n",
    "m_van = profile_vanilla(van_model, S_ids, Utotal_ids, H_ids, gen_len=LEN_GEN, greedily=True)\n",
    "m_tri = profile_tri(tri_model, S_ids, Utotal_ids, H_ids, lower_k=LOWER_K, gen_len=LEN_GEN, greedily=True)\n",
    "\n",
    "print(m_van)\n",
    "print(m_tri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] merged LoRA into base (unloaded PEFT)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>lens</th>\n",
       "      <th>prefill_ms</th>\n",
       "      <th>first_token_ms</th>\n",
       "      <th>ttft_ms</th>\n",
       "      <th>gen_ms</th>\n",
       "      <th>prefill_ms_per_tok</th>\n",
       "      <th>gen_ms_per_tok</th>\n",
       "      <th>total_ms_per_tok</th>\n",
       "      <th>total_ms</th>\n",
       "      <th>peak_mem_alloc_MiB</th>\n",
       "      <th>peak_mem_reserved_MiB</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>{'S': 256, 'U_total': 10496, 'H': 4, 'gen': 51...</td>\n",
       "      <td>830.267</td>\n",
       "      <td>18.328</td>\n",
       "      <td>848.595</td>\n",
       "      <td>9041.353</td>\n",
       "      <td>0.07719</td>\n",
       "      <td>17.65889</td>\n",
       "      <td>0.87770</td>\n",
       "      <td>9889.948</td>\n",
       "      <td>34741.07</td>\n",
       "      <td>96564.0</td>\n",
       "      <td>vanilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LoPA-TRI(K=8)</td>\n",
       "      <td>{'S': 256, 'U_total': 10496, 'H': 4, 'gen': 51...</td>\n",
       "      <td>279.043</td>\n",
       "      <td>22.784</td>\n",
       "      <td>301.827</td>\n",
       "      <td>11811.404</td>\n",
       "      <td>0.02594</td>\n",
       "      <td>23.06915</td>\n",
       "      <td>1.07501</td>\n",
       "      <td>12113.231</td>\n",
       "      <td>33262.51</td>\n",
       "      <td>96562.0</td>\n",
       "      <td>LoPA(before)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LoPA-TRI(K=8)</td>\n",
       "      <td>{'S': 256, 'U_total': 10496, 'H': 4, 'gen': 51...</td>\n",
       "      <td>182.177</td>\n",
       "      <td>15.669</td>\n",
       "      <td>197.846</td>\n",
       "      <td>8192.882</td>\n",
       "      <td>0.01694</td>\n",
       "      <td>16.00172</td>\n",
       "      <td>0.74465</td>\n",
       "      <td>8390.728</td>\n",
       "      <td>32197.51</td>\n",
       "      <td>96552.0</td>\n",
       "      <td>LoPA(merged)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model                                               lens  \\\n",
       "0        vanilla  {'S': 256, 'U_total': 10496, 'H': 4, 'gen': 51...   \n",
       "1  LoPA-TRI(K=8)  {'S': 256, 'U_total': 10496, 'H': 4, 'gen': 51...   \n",
       "2  LoPA-TRI(K=8)  {'S': 256, 'U_total': 10496, 'H': 4, 'gen': 51...   \n",
       "\n",
       "   prefill_ms  first_token_ms  ttft_ms     gen_ms  prefill_ms_per_tok  \\\n",
       "0     830.267          18.328  848.595   9041.353             0.07719   \n",
       "1     279.043          22.784  301.827  11811.404             0.02594   \n",
       "2     182.177          15.669  197.846   8192.882             0.01694   \n",
       "\n",
       "   gen_ms_per_tok  total_ms_per_tok   total_ms  peak_mem_alloc_MiB  \\\n",
       "0        17.65889           0.87770   9889.948            34741.07   \n",
       "1        23.06915           1.07501  12113.231            33262.51   \n",
       "2        16.00172           0.74465   8390.728            32197.51   \n",
       "\n",
       "   peak_mem_reserved_MiB          note  \n",
       "0                96564.0       vanilla  \n",
       "1                96562.0  LoPA(before)  \n",
       "2                96552.0  LoPA(merged)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# LoRA가 붙어 있으면 병합해서 언로드 (추론 속도 개선)\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    if isinstance(tri_model, PeftModel):\n",
    "        tri_model = tri_model.merge_and_unload()\n",
    "        tri_model = tri_model.eval()\n",
    "        print(\"[info] merged LoRA into base (unloaded PEFT)\")\n",
    "except Exception as e:\n",
    "    print(\"[info] LoRA merge skipped:\", e)\n",
    "\n",
    "# 동일 백엔드 재지정\n",
    "try:\n",
    "    tri_model.config._attn_implementation = ATTN_IMPL\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 재측정\n",
    "m_tri2 = profile_tri(tri_model, S_ids, Utotal_ids, H_ids, lower_k=LOWER_K, gen_len=LEN_GEN, greedily=True)\n",
    "display(pd.DataFrame([m_van, m_tri, m_tri2]).assign(note=[\"vanilla\",\"LoPA(before)\",\"LoPA(merged)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294be00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Prompt(S/U/H)</th>\n",
       "      <th>GenToks</th>\n",
       "      <th>Prefill (ms)</th>\n",
       "      <th>FirstTok (ms)</th>\n",
       "      <th>TTFT (ms)</th>\n",
       "      <th>Gen (ms)</th>\n",
       "      <th>Prefill (ms/tok)</th>\n",
       "      <th>Gen (ms/tok)</th>\n",
       "      <th>Total (ms/tok)</th>\n",
       "      <th>Total (ms)</th>\n",
       "      <th>Peak alloc (MiB)</th>\n",
       "      <th>Peak reserved (MiB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vanilla</td>\n",
       "      <td>256/10496/4</td>\n",
       "      <td>512</td>\n",
       "      <td>830.267</td>\n",
       "      <td>18.328</td>\n",
       "      <td>848.595</td>\n",
       "      <td>9041.353</td>\n",
       "      <td>0.07719</td>\n",
       "      <td>17.65889</td>\n",
       "      <td>0.87770</td>\n",
       "      <td>9889.948</td>\n",
       "      <td>34741.07</td>\n",
       "      <td>96564.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LoPA-TRI(K=8)</td>\n",
       "      <td>256/10496/4</td>\n",
       "      <td>512</td>\n",
       "      <td>279.043</td>\n",
       "      <td>22.784</td>\n",
       "      <td>301.827</td>\n",
       "      <td>11811.404</td>\n",
       "      <td>0.02594</td>\n",
       "      <td>23.06915</td>\n",
       "      <td>1.07501</td>\n",
       "      <td>12113.231</td>\n",
       "      <td>33262.51</td>\n",
       "      <td>96562.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model Prompt(S/U/H)  GenToks  Prefill (ms)  FirstTok (ms)  \\\n",
       "0        vanilla   256/10496/4      512       830.267         18.328   \n",
       "1  LoPA-TRI(K=8)   256/10496/4      512       279.043         22.784   \n",
       "\n",
       "   TTFT (ms)   Gen (ms)  Prefill (ms/tok)  Gen (ms/tok)  Total (ms/tok)  \\\n",
       "0    848.595   9041.353           0.07719      17.65889         0.87770   \n",
       "1    301.827  11811.404           0.02594      23.06915         1.07501   \n",
       "\n",
       "   Total (ms)  Peak alloc (MiB)  Peak reserved (MiB)  \n",
       "0    9889.948          34741.07              96564.0  \n",
       "1   12113.231          33262.51              96562.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def to_row(m):\n",
    "    return {\n",
    "        \"Model\": m[\"model\"],\n",
    "        \"Prompt(S/U/H)\": f'{m[\"lens\"][\"S\"]}/{m[\"lens\"][\"U_total\"]}/{m[\"lens\"][\"H\"]}',\n",
    "        \"GenToks\": m[\"lens\"][\"gen\"],\n",
    "        \"Prefill (ms)\": m[\"prefill_ms\"],\n",
    "        \"FirstTok (ms)\": m[\"first_token_ms\"],\n",
    "        \"TTFT (ms)\": m[\"ttft_ms\"],\n",
    "        \"Gen (ms)\": m[\"gen_ms\"],\n",
    "        \"Prefill (ms/tok)\": m[\"prefill_ms_per_tok\"],\n",
    "        \"Gen (ms/tok)\": m[\"gen_ms_per_tok\"],\n",
    "        \"Total (ms/tok)\": m[\"total_ms_per_tok\"],\n",
    "        \"Total (ms)\": m[\"total_ms\"],\n",
    "        \"Peak alloc (MiB)\": m[\"peak_mem_alloc_MiB\"],\n",
    "        \"Peak reserved (MiB)\": m[\"peak_mem_reserved_MiB\"],\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame([to_row(m_van), to_row(m_tri)])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a318a44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATTN HOOK] calls: {'fa2_calls': 328, 'mask_none': 328, 'mask_additive': 0}\n"
     ]
    }
   ],
   "source": [
    "# TRI 어텐션 fwd를 훅킹해서 어떤 경로/마스크가 쓰이는지 카운트\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention\n",
    "\n",
    "_attn_calls = {\"fa2_calls\":0, \"mask_none\":0, \"mask_additive\":0}\n",
    "_orig_fwd = LlamaAttention.forward\n",
    "\n",
    "def _hook_fwd(self, hidden_states, position_embeddings, attention_mask,\n",
    "              past_key_values=None, cache_position=None, **kwargs):\n",
    "    impl = getattr(self.config, \"_attn_implementation\", \"eager\")\n",
    "    if impl == \"flash_attention_2\":\n",
    "        _attn_calls[\"fa2_calls\"] += 1\n",
    "    if attention_mask is None:\n",
    "        _attn_calls[\"mask_none\"] += 1\n",
    "    else:\n",
    "        _attn_calls[\"mask_additive\"] += 1\n",
    "    return _orig_fwd(self, hidden_states, position_embeddings, attention_mask,\n",
    "                     past_key_values=past_key_values, cache_position=cache_position, **kwargs)\n",
    "\n",
    "# 훅 설치\n",
    "LlamaAttention.forward = _hook_fwd\n",
    "\n",
    "# LoPA 한 번 짧게 돌려서 카운트\n",
    "_ = profile_tri(tri_model, S_ids[:, :64], Utotal_ids[:, :128], H_ids[:, :2], lower_k=LOWER_K, gen_len=8, greedily=True)\n",
    "print(\"[ATTN HOOK] calls:\", _attn_calls)\n",
    "\n",
    "# 사용 후 원복\n",
    "LlamaAttention.forward = _orig_fwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47be230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
