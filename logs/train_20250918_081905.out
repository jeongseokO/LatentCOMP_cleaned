[Unified] Training method: lopa
CutGen: False
[DEBUG] modeling_llama path: /workspace/LatentCOMP_cleaned/lopa_llama_modeling.py
[DEBUG] TRI bound: {'tri_prefill_system_all': True, 'tri_prefill_user_lower': True, 'tri_build_caches': True, 'tri_forward_assistant': True} {'tri_build_caches': True, 'tri_forward_assistant': True, 'tri_step_logits': True}
[Info] attn_implementation = flash_attention_2
[LoRA] target_modules = ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']
[Env] torch: 2.7.1+cu128 cuda: 12.8
[Env] device: NVIDIA RTX PRO 6000 Blackwell Workstation Edition | param dtype: torch.bfloat16
[Epoch 1] train_avg=0.789470 | valid_avg=0.627059 | n_train=8940 | n_valid=993
[Best] Saved to /workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best (val=0.627059)
[Epoch 2] train_avg=0.553548 | valid_avg=0.584069 | n_train=8940 | n_valid=993
[Best] Saved to /workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best (val=0.584069)
[Epoch 3] train_avg=0.472144 | valid_avg=0.591912 | n_train=8940 | n_valid=993
âœ… Uploaded to Hub: jeongseokoh/best
[Unified] Packaged best artifact at: /workspace/LatentCOMP_cleaned/LatentCOMP_cleaned/outputs/Llama-3.1-8B-Instruct-LOPA-partial8-0specials/best
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mlopa_tri_balanced_K8_R1024[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250918_081930-m2bp0sh8/logs[0m
