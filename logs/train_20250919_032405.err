W0919 03:24:07.535000 3016970 site-packages/torch/distributed/run.py:793] 
W0919 03:24:07.535000 3016970 site-packages/torch/distributed/run.py:793] *****************************************
W0919 03:24:07.535000 3016970 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0919 03:24:07.535000 3016970 site-packages/torch/distributed/run.py:793] *****************************************
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.17it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:02,  1.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
wandb: Currently logged in as: luke0112. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.2
wandb: Run data is saved locally in /data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/wandb/run-20250919_032510-rf1sp2m4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run LoPA_Llama3.1_8B_8_Lowers_user_10Specials
wandb: ⭐️ View project at https://wandb.ai/luke0112/latentcomp-cleaned
wandb: 🚀 View run at https://wandb.ai/luke0112/latentcomp-cleaned/runs/rf1sp2m4
Epoch 1 [train]:   0%|          | 0/4470 [00:00<?, ?it/s][rank1]: Traceback (most recent call last):
[rank1]:   File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train.py", line 282, in <module>
[rank1]:     main()
[rank1]:   File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train.py", line 177, in main
[rank1]:     lopa_mod.train(args)
[rank1]:   File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train_lopa_pure.py", line 640, in train
[rank1]:     
[rank1]:   File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train_lopa_pure.py", line 598, in compute_loss_on_group_seq
[rank1]:     
[rank1]:   File "/data2/jeongseokoh/miniconda3/envs/vllm/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 630, in __getattr__
[rank1]:     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
[rank1]: AttributeError: 'DeepSpeedEngine' object has no attribute 'tri_build_caches'
                                                         Traceback (most recent call last):
  File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train.py", line 282, in <module>
    main()
  File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train.py", line 177, in main
    lopa_mod.train(args)
  File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train_lopa_pure.py", line 640, in train
  File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train_lopa_pure.py", line 598, in compute_loss_on_group_seq
  File "/data2/jeongseokoh/miniconda3/envs/vllm/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 630, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'DeepSpeedEngine' object has no attribute 'tri_build_caches'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train.py", line 282, in <module>
[rank0]:     main()
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train.py", line 177, in main
[rank0]:     lopa_mod.train(args)
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train_lopa_pure.py", line 640, in train
[rank0]:     
[rank0]:   File "/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train_lopa_pure.py", line 598, in compute_loss_on_group_seq
[rank0]:     
[rank0]:   File "/data2/jeongseokoh/miniconda3/envs/vllm/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 630, in __getattr__
[rank0]:     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
[rank0]: AttributeError: 'DeepSpeedEngine' object has no attribute 'tri_build_caches'
W0919 03:25:33.310000 3016970 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3017036 closing signal SIGTERM
E0919 03:25:33.628000 3016970 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 3017037) of binary: /data2/jeongseokoh/miniconda3/envs/vllm/bin/python
Traceback (most recent call last):
  File "/data2/jeongseokoh/miniconda3/envs/vllm/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.5.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data2/jeongseokoh/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/data2/jeongseokoh/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/data2/jeongseokoh/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/data2/jeongseokoh/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data2/jeongseokoh/miniconda3/envs/vllm/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-19_03:25:33
  host      : n01
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3017037)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: n01: task 0: Exited with exit code 1
