{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick LoPA Inference (HF model)\n",
        "\n",
        "This notebook loads the HF repo `jeongseokoh/Mistral-7B-Instruct-v0.2-LOPA-partial4-0specials` and runs inference using `infer_lopa_pure.py`'s `lopa_generate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies if needed (uncomment)\n",
        "# %pip install -q transformers accelerate peft\n",
        "\n",
        "%cd /data2/jeongseokoh/jeongseokoh/LatentCOMP_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84efe4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from huggingface_hub import snapshot_download\n",
        "import os, torch, pathlib\n",
        "\n",
        "# Import LoPA generator from the local script\n",
        "from infer_lopa_pure import lopa_generate\n",
        "\n",
        "# HF repo that holds best/ (tokenizer at root, base/ and lora/ as subfolders)\n",
        "repo_id = 'jeongseokoh/Mistral-7B-Instruct-v0.2-LOPA-partial4-0specials'\n",
        "# Base backbone to merge LoRA into\n",
        "base_model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = (torch.bfloat16 if (device=='cuda' and torch.cuda.is_bf16_supported()) else\n",
        "         (torch.float16 if device=='cuda' else torch.float32))\n",
        "\n",
        "# Download the HF repo snapshot locally so we can access subfolders\n",
        "best_dir = snapshot_download(repo_id=repo_id)\n",
        "print('Snapshot path:', best_dir)\n",
        "\n",
        "# Load tokenizer from the snapshot root (to pick up chat template)\n",
        "tokenizer = AutoTokenizer.from_pretrained(best_dir, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "# Load base model, then merge LoRA if present\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=False, torch_dtype=dtype)\n",
        "lora_dir = os.path.join(best_dir, 'lora')\n",
        "if os.path.isdir(lora_dir) and any(pathlib.Path(lora_dir).glob('adapter*')):\n",
        "    from peft import PeftModel\n",
        "    peft = PeftModel.from_pretrained(model, lora_dir)\n",
        "    model = peft.merge_and_unload()\n",
        "else:\n",
        "    # Fallback: if base weights are inside the snapshot (best_dir/base), prefer them\n",
        "    base_dir = os.path.join(best_dir, 'base')\n",
        "    if os.path.isdir(base_dir):\n",
        "        model = AutoModelForCausalLM.from_pretrained(base_dir, trust_remote_code=False, torch_dtype=dtype)\n",
        "\n",
        "model = model.to(device).eval()\n",
        "for k in ('attn_implementation', '_attn_implementation'):\n",
        "    try:\n",
        "        setattr(model.config, k, 'eager')\n",
        "        setattr(model.model.config, k, 'eager')\n",
        "    except Exception:\n",
        "        pass\n",
        "print('Loaded tokenizer and model. Device:', device, '| dtype:', dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f242ded0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example inputs\n",
        "system = 'You are a helpful assistant that answers questions based on the given document. '\n",
        "document = 'The Eiffel Tower is a wrought-iron lattice tower in Paris, France. It is named after the engineer Gustave Eiffel.'\n",
        "question = 'Which city is the Eiffel Tower located in?'\n",
        "\n",
        "# LoPA prefill layers (K). Use 4 to match the partial4 model.\n",
        "K = 4\n",
        "\n",
        "with torch.inference_mode():\n",
        "    answer = lopa_generate(\n",
        "        model, tokenizer,\n",
        "        system=system, document=document, question=question,\n",
        "        K=K, device=device,\n",
        "        max_new_tokens=128, min_length=8,\n",
        "        temperature=0.7, top_p=0.95, top_k=None,\n",
        "        do_sample=True, debug=False,\n",
        "    )\n",
        "print('Answer:', answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "676d2e08",
      "metadata": {},
      "source": [
        "---\n",
        "### Alternative: CLI (inside notebook)\n",
        "Download the repo with `snapshot_download` and pass that path as `--best_dir`, and set `--model_name` to the original base backbone (Mistral)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be4b7d55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example CLI usage (uncomment to run)\n",
        "# from huggingface_hub import snapshot_download\n",
        "# best_dir = snapshot_download(repo_id='jeongseokoh/Mistral-7B-Instruct-v0.2-LOPA-partial4-0specials')\n",
        "# !python infer_lopa_pure.py \\\n",
        "#   --best_dir {best_dir} \\\n",
        "#   --model_name mistralai/Mistral-7B-Instruct-v0.2 \\\n",
        "#   --prefill_layers 4 \\\n",
        "#   --document 'The Eiffel Tower is a wrought-iron lattice tower in Paris, France.' \\\n",
        "#   --question 'Which city is the Eiffel Tower located in?' \\\n",
        "#   --max_new_tokens 128 --do_sample --top_p 0.95 --temperature 0.7"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
