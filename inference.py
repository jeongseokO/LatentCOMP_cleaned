#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GSM8K(train) â†’ ìˆ˜í•™ ì¶”ë¡  ë°ì´í„° ìƒì„±ê¸°
- ë¬¸ì„œ ì—†ì´ ë¬¸ì œë§Œ ì£¼ê³ , ë‹µë³€ì€ ë°˜ë“œì‹œ ë§ˆì§€ë§‰ì— \boxed{...} ë¡œ ìˆ«ìë§Œ ì¶œë ¥
- ê° ë¬¸ì œë‹¹ 5ê°œ ì‘ë‹µ ìƒì„±(ë§¤ì¹­ëœ ì‘ë‹µë§Œ ì €ì¥)
- question_id: gsm8k_{index}
- ì¶œë ¥ í¬ë§·(jsonl):
  {
    "sample_idx": int,
    "question_id": "gsm8k_{i}",
    "document": "",               # ë¹„ì–´ ìˆìŒ
    "question": <raw question>,   # ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì“¸ ìˆœìˆ˜ ì§ˆë¬¸
    "responses": [matched_response, ...],
    "matched_boxed": ["24", ...], # \boxed{...} ì•ˆì˜ ìˆ«ì
    "ground_truth_answer_raw": "<gold_str>",
    "step_by_step": bool,
    "num_responses": int
  }
"""

import os
import json
import random
import gc
import re
import string
from typing import List, Tuple, Dict, Any

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
from datasets import load_dataset
from tqdm import tqdm
from decimal import Decimal, InvalidOperation
from fractions import Fraction

# --------------------------
# 1) ì„¤ì •
# --------------------------
SEED = 42
NUM_SAMPLES = 4000            # GSM8K train ì „ì²´ê°€ 7k+ ì´ë¯€ë¡œ, í•„ìš” ê°œìˆ˜ë§Œ
NUM_RETURN_SEQUENCES = 5
MAX_TOKENS = 512
TEMPERATURE = 0.9

CACHE_DIR_DATA = "/data/jeongseokoh/data"
CACHE_DIR_TOKENIZER = "/data/jeongseokoh/hub/tokenizer"
CACHE_DIR_MODEL = "/data/jeongseokoh/hub/model"
MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"

# ë©”ëª¨ë¦¬/ë°°ì¹˜
BATCH_SIZE = 50
MEMORY_CLEANUP_INTERVAL = 100

# \boxed ê°•ì œ í…œí”Œë¦¿(ìˆ«ìë§Œ)
FORCED_CONCLUDE_TEMPLATE = (
    "Ensure your final answer is presented within the format '#### {numeric answer}'."
)

# --------------------------
# 2) ì‹œë“œ/ë””ë°”ì´ìŠ¤
# --------------------------
set_seed(SEED)
random.seed(SEED)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    torch.cuda.empty_cache()

# --------------------------
# 3) ë°ì´í„° ë¡œë“œ(GSM8K)
# --------------------------
print("GSM8K(train) ë¡œë“œ ì¤‘...")
ds = load_dataset("openai/gsm8k", "main", cache_dir=CACHE_DIR_DATA)
train_data = list(ds["train"])
random.shuffle(train_data)
selected_samples = train_data[:NUM_SAMPLES]
print(f"ì´ {len(train_data)}ê°œ ì¤‘ {len(selected_samples)}ê°œ ìƒ˜í”Œ ì‚¬ìš© (seed={SEED})")

# --------------------------
# 4) ëª¨ë¸/í† í¬ë‚˜ì´ì €
# --------------------------
print("ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
n_gpu = torch.cuda.device_count()
assert n_gpu >= 1, "ìµœì†Œ 1ê°œì˜ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤."
# ê° GPU ë©”ëª¨ë¦¬ì˜ 90%ë¥¼ max_memoryë¡œ ì„¤ì •
def _mem_str(i):
    cap = torch.cuda.get_device_properties(i).total_memory / (1024**3)
    return f"{int(cap * 0.9)}GiB"
max_mem = {i: _mem_str(i) for i in range(n_gpu)}

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR_TOKENIZER)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    cache_dir=CACHE_DIR_MODEL,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    max_memory=max_mem,
    low_cpu_mem_usage=True,
)
model.eval()
print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}")

# --------------------------
# 5) ë©”ëª¨ë¦¬ ìœ í‹¸
# --------------------------
def cleanup_memory():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def check_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        return f"í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB"
    return "CUDA ë¶ˆê°€"

# --------------------------
# 6) ì •ê·œí™”/ì¶”ì¶œ/ì •ë‹µ ë¹„êµ
# --------------------------
_PUNCT_TABLE = str.maketrans({p: " " for p in string.punctuation})
_BOXED_LAST_RE = re.compile(r"\\(?:boxed|fbox)\{([^{}]*)\}")

# #### ìˆ«ì ì¶”ì¶œìš© ì •ê·œì‹
_GSM_FINAL_RE = re.compile(r"####\s*([^\n]+)")

def last_hash_answer(text: str) -> str:
    if not isinstance(text, str) or not text:
        return "No Match"
    matches = list(_GSM_FINAL_RE.finditer(text))
    if not matches:
        return "No Match"
    content = matches[-1].group(1).strip()
    return content if content else "No Match"

def normalize_simple(s: str) -> str:
    """ê°„ë‹¨ í…ìŠ¤íŠ¸ ì •ê·œí™”(ë¹„ìˆ˜í•™ì )."""
    if s is None:
        return ""
    s = s.strip().lower()
    s = s.translate(_PUNCT_TABLE)
    s = re.sub(r"\b(a|an|the)\b", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

_GSM_FINAL_RE = re.compile(r"####\s*(.+)")

def extract_gsm8k_gold(answer_field: str) -> str:
    """
    GSM8Kì˜ gold answerëŠ” '#### 24' í˜•íƒœê°€ ë³´í†µ ë§ˆì§€ë§‰ ì¤„ì— ì¡´ì¬.
    í•´ë‹¹ ë¶€ë¶„ë§Œ ì¶”ì¶œ(ì¢Œìš° ê³µë°±/ë§ˆì¹¨í‘œ ì œê±°, ì‰¼í‘œ ì œê±°).
    """
    if not isinstance(answer_field, str): 
        return ""
    m = _GSM_FINAL_RE.search(answer_field)
    if not m:
        return ""
    s = m.group(1).strip()
    s = s.rstrip(".")
    s = s.replace(",", "")
    return s

def parse_number_like(s: str):
    """
    '24', '-3.5', '1/2', '50%' â†’ ìˆ˜ì¹˜ë¡œ íŒŒì‹± ì‹œë„
    - ì„±ê³µ: (True, Fraction or Decimal)
    - ì‹¤íŒ¨: (False, None)
    """
    if not isinstance(s, str):
        return (False, None)
    t = s.strip()
    if not t:
        return (False, None)
    # í¼ì„¼íŠ¸
    if t.endswith("%"):
        try:
            val = Decimal(t[:-1].strip())
            return (True, Decimal(val) / Decimal(100))
        except InvalidOperation:
            return (False, None)
    # ë¶„ìˆ˜
    if "/" in t:
        num, *rest = t.split("/")
        if len(rest) == 1:
            den = rest[0]
            if num.strip().lstrip("-").isdigit() and den.strip().isdigit():
                try:
                    return (True, Fraction(int(num), int(den)))
                except ZeroDivisionError:
                    return (False, None)
    # ì¼ë°˜ ìˆ«ì
    try:
        return (True, Decimal(t))
    except InvalidOperation:
        return (False, None)

def numeric_equiv(a: str, b: str, tol: Decimal = Decimal("1e-9")) -> bool:
    """
    ìˆ«ì/ë¶„ìˆ˜/í¼ì„¼íŠ¸ ë“± ìˆ˜ì¹˜ ë™ë“±ì„± í—ˆìš© ë¹„êµ (í—ˆìš©ì˜¤ì°¨ tol).
    """
    ok_a, va = parse_number_like(a.replace(",", ""))
    ok_b, vb = parse_number_like(b.replace(",", ""))
    if ok_a and ok_b:
        # Fraction vs Decimal ë¹„êµ â†’ Decimalë¡œ í†µì¼
        if isinstance(va, Fraction):
            va = Decimal(va.numerator) / Decimal(va.denominator)
        if isinstance(vb, Fraction):
            vb = Decimal(vb.numerator) / Decimal(vb.denominator)
        diff = (va - vb).copy_abs()
        return diff <= tol
    # ìˆ«ìë¡œ í•´ì„ ì•ˆë˜ë©´ ë¬¸ìì—´ ë™ì¹˜(ì •ê·œí™”)ë¡œ fallback
    return normalize_simple(a) == normalize_simple(b)

def exact_or_numeric_match(pred_extracted: str, gold_str: str) -> bool:
    if pred_extracted == "No Match" or not gold_str:
        return False
    # 1) ìˆ«ì ë™ë“±ì„± ìš°ì„ 
    if numeric_equiv(pred_extracted, gold_str):
        return True
    # 2) ë³´ìˆ˜ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì •ê·œí™” ë¹„êµ
    return normalize_simple(pred_extracted) == normalize_simple(gold_str)

# --------------------------
# 7) í”„ë¡¬í”„íŠ¸ ìƒì„±
# --------------------------
def create_prompt_math(question: str, step_by_step: bool = False):
    system_prompt = (
        "You are a methodical mathematician, adept at solving complex mathematical problems. Conclude your explanation with the answer in a '#### {numeric answer}' format, where the answer is solely a number."
    )
    user_prompt = f"Question: {question}"
    if step_by_step:
        user_prompt += "\nThink step by step."
    user_prompt += f"\n{FORCED_CONCLUDE_TEMPLATE}"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    return messages, user_prompt

# --------------------------
# 8) ì•ˆì „ ìƒì„±
# --------------------------
@torch.inference_mode()
def generate_responses_safe(messages, num_return_sequences=NUM_RETURN_SEQUENCES, max_tokens=MAX_TOKENS, temperature=TEMPERATURE):
    try:
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            tokenize=True
        ).to(device)

        outputs = model.generate(
            inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            num_return_sequences=num_return_sequences,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True,
        )

        prompt_len = inputs.shape[-1]
        responses = []
        for output in outputs:
            response = tokenizer.decode(output[prompt_len:], skip_special_tokens=True)
            responses.append(response.strip())

        del inputs, outputs
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return responses, None

    except Exception as e:
        cleanup_memory()
        error_msg = str(e)
        print(f"    âš ï¸ ìƒì„± ì—ëŸ¬: {error_msg}")
        if "CUDA" in error_msg or "out of memory" in error_msg:
            print("    ğŸ”„ CUDA ì—ëŸ¬ ê°ì§€, ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ì¬ì‹œë„(ì¶•ì†Œ)...")
            cleanup_memory()
            try:
                return generate_responses_safe(messages, num_return_sequences=1, max_tokens=max_tokens//2, temperature=temperature)
            except Exception as e2:
                return [], f"ì¬ì‹œë„ ì‹¤íŒ¨: {error_msg} / {e2}"
        return [], error_msg

# --------------------------
# 9) ì²´í¬í¬ì¸íŠ¸
# --------------------------
def save_checkpoint(processed_count, output_file):
    checkpoint = {
        "processed_count": processed_count,
        "output_file": output_file
    }
    with open(f"{output_file}.checkpoint", 'w') as f:
        json.dump(checkpoint, f)

def load_checkpoint(output_file):
    checkpoint_file = f"{output_file}.checkpoint"
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r') as f:
                return json.load(f)
        except:
            return None
    return None

# --------------------------
# 10) ë©”ì¸ ë£¨í”„
# --------------------------
def process_gsm8k_train():
    output_file = f"gsm8k_train_{NUM_RETURN_SEQUENCES}resp_seed{SEED}_samples{NUM_SAMPLES}_boxed_numeric_exact.jsonl"

    # ì²´í¬í¬ì¸íŠ¸
    checkpoint = load_checkpoint(output_file)
    start_idx = checkpoint["processed_count"] if checkpoint else 0
    if start_idx > 0:
        print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘: {start_idx}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
    else:
        if os.path.exists(output_file):
            os.remove(output_file)

    # step-by-step ì ˆë°˜ ìœ ì§€
    midpoint = len(selected_samples) // 2
    normal_samples = [(i, s) for i, s in enumerate(selected_samples[:midpoint])]
    step_samples   = [(midpoint + i, s) for i, s in enumerate(selected_samples[midpoint:])]
    random.shuffle(normal_samples)
    random.shuffle(step_samples)
    tasks = normal_samples + step_samples

    current_batch = []
    batch_num = 0
    processed_samples = 0
    total_samples_with_match = 0
    total_records_saved = 0
    total_matched_responses = 0

    def handle_one(i_local: int, sample: Dict[str, Any], step_by_step: bool):
        nonlocal processed_samples, current_batch, batch_num
        nonlocal total_samples_with_match, total_records_saved, total_matched_responses

        if processed_samples < start_idx:
            processed_samples += 1
            return

        q = sample.get("question", "").strip()
        gold_raw = extract_gsm8k_gold(sample.get("answer", ""))  # "#### ..."ì—ì„œ ì¶”ì¶œ
        qid = f"gsm8k_{i_local}"

        if not q or not gold_raw:
            print(f"âš ï¸ ìƒ˜í”Œ {i_local}: ìœ íš¨í•˜ì§€ ì•Šì€ question/gold â†’ ìŠ¤í‚µ")
            processed_samples += 1
            return

        print(f"\n--- GSM8K {processed_samples+1}/{NUM_SAMPLES} (ë©”ëª¨ë¦¬: {check_gpu_memory()}) ---")
        print(f"    step_by_step={step_by_step} | qid={qid} | gold='{gold_raw}'")

        # ë©”ì‹œì§€ êµ¬ì„±(ë¬¸ì„œ ì—†ìŒ)
        messages, user_prompt = create_prompt_math(q, step_by_step=step_by_step)
        responses, error = generate_responses_safe(messages, num_return_sequences=NUM_RETURN_SEQUENCES)

        if error:
            print(f"  ìƒì„± ì‹¤íŒ¨ - {error}")
            processed_samples += 1
            return

        matched_responses = []
        matched_boxed = []
        for r in responses:
            bx = last_hash_answer(r)
            if exact_or_numeric_match(bx, gold_raw):
                matched_responses.append(r)
                matched_boxed.append(bx)

        if matched_responses:
            total_samples_with_match += 1
            rec = {
                "sample_idx": i_local,
                "question_id": qid,
                "document": "",          # â† ë¬¸ì„œ ì—†ìŒ
                "question": q,           # â† ìˆœìˆ˜ ì§ˆë¬¸ë§Œ ì €ì¥(í›ˆë ¨ì‹œ 'Question: {q}')
                "responses": matched_responses,
                "matched_boxed": matched_boxed,
                "ground_truth_answer_raw": gold_raw,
                "step_by_step": step_by_step,
                "num_responses": len(matched_responses),
            }
            current_batch.append(rec)
            total_records_saved += 1
            total_matched_responses += len(matched_responses)
            print(f"  âœ… ë§¤ì¹­ ì‘ë‹µ {len(matched_responses)}ê°œ ì €ì¥")

            if len(current_batch) >= BATCH_SIZE:
                with open(output_file, 'a', encoding='utf-8') as f:
                    for r in current_batch:
                        f.write(json.dumps(r, ensure_ascii=False) + "\n")
                print(f"âœ… ë°°ì¹˜ {batch_num} ì €ì¥ (ë ˆì½”ë“œ {len(current_batch)}ê°œ, ëˆ„ì ={total_records_saved}, ì‘ë‹µëˆ„ì ={total_matched_responses})")
                current_batch.clear()
                batch_num += 1
        else:
            print(f"  âŒ ë§¤ì¹­ëœ ì‘ë‹µ ì—†ìŒ â†’ ì €ì¥ ì•ˆí•¨")

        processed_samples += 1

        if processed_samples % MEMORY_CLEANUP_INTERVAL == 0:
            cleanup_memory()
            save_checkpoint(processed_samples, output_file)
            print(f"ğŸ”„ ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì§„í–‰ë¥ : {processed_samples}/{NUM_SAMPLES})")

    # ì‹¤í–‰
    try:
        for idx, sample in tqdm(tasks, desc="GSM8K ì²˜ë¦¬", initial=start_idx):
            step_flag = (idx >= midpoint)  # í›„ë°˜ë¶€ëŠ” step-by-step True
            handle_one(idx, sample, step_flag)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì¤‘ë‹¨ - í˜„ì¬ê¹Œì§€ ê²°ê³¼ ì €ì¥")
        save_checkpoint(processed_samples, output_file)
    except Exception as e:
        print(f"\nâŒ ì˜ˆì™¸: {e}")
        save_checkpoint(processed_samples, output_file)
        raise

    # ë‚¨ì€ ë°°ì¹˜ ì €ì¥
    if current_batch:
        with open(output_file, 'a', encoding='utf-8') as f:
            for r in current_batch:
                f.write(json.dumps(r, ensure_ascii=False) + "\n")
        print(f"âœ… ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ (ë ˆì½”ë“œ {len(current_batch)}ê°œ)")

    cleanup_memory()
    print("\nğŸ‰ ì™„ë£Œ!")
    print(f"   - ë§¤ì¹­ëœ ìƒ˜í”Œ ìˆ˜: {total_samples_with_match}")
    print(f"   - ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜: {total_records_saved}")
    print(f"   - ì´ ì €ì¥ ì‘ë‹µ ìˆ˜: {total_matched_responses}")
    print(f"ğŸ“„ ì¶œë ¥ íŒŒì¼: {output_file}")
    return output_file

if __name__ == "__main__":
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
    out = process_gsm8k_train()
    print("\nğŸ“Š íŒŒì¼ ê²€ì¦:")
    try:
        n_lines = 0
        n_resps = 0
        with open(out, 'r', encoding='utf-8') as f:
            for line in f:
                n_lines += 1
                n_resps += len(json.loads(line).get("responses", []))
        print(f"   - íŒŒì¼ ë ˆì½”ë“œ ìˆ˜: {n_lines}")
        print(f"   - íŒŒì¼ ë‚´ ì €ì¥ëœ ì‘ë‹µ ì´ ìˆ˜: {n_resps}")
    except FileNotFoundError:
        print("   - ì¶œë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
